---
title: Ontologie
---
[<img src="/images/accueil.png">](/)

# Ontological thoughts

- [Introduction](#p1)
- [Objective reality](#p2)
	- [Introduction](#p21)
	- [Ancient philosophy](#p22)
	- [Modern philosophy](#p23)
	- [From philosophy to natural science](#p24)
	- [In natural science](#p25)
- [Causality and determinism](#p3)
	- [Introduction](#p31)
	- [Metaphysical considerations](#p32)
	- [In natural science](#p33)
- [Mind and matter](#p4)
	- [Introduction](#p41)
	- [Metaphysical considerations](#p42)
	- [Beauty and truth](#p43)
	- [Quantum physics interpretations](#p44)
	- [Planck’s quantum of action and Heisenberg’s uncertainty](#p45)
	- [Matter in modern physics: waves and particles](#p46)
	- [Wave function and Schrödinger equation](#p47)
	- [Information: Young’s double-slits experiments](#p48)
	- [Observation: von Neumann’s cut and Schrödinger’s cat](#p49)
	- [Hidden variables](#p410)
	- [Nonlocality: EPR, Bell inequalities and Aspect’s experience](#p411)
	- [Time: creation, direction and symmetry](#p412)
	- [Light](#p413)
	- [Limits](#p414)
	- [Beyond: universal field and order](#p415)
- [Life and consciousness](#p5)
	- [Introduction](#p51)
	- [Metaphysical ideas](#p52)
	- [Memory and the ego](#p53)
	- [Single/many/splitting-mind(s)/worlds/awareness](#p54)
	- [Physics and the origin of life](#p55)
	- [Physics and the origin of human consciousness](#p56)
	- [Consciousness at the cell level](#p57)
	- [Beyond: universal consciousness and love](#p58)

## Introduction <a name="p1"></a>

<img src="/images/wordcloud_ontology.png">

Ontology is the philosophical study of being, in particular becoming, existence and reality. One can find numerous ontological arguments at the source of various theologies and philosophies. General improvements in the many fields of natural science allowed for countless evolutions. This word cloud forms a non-exhaustive list of 120 different concepts, and I did not take into account specific theological views. With so many concepts and authors, I find impossible to read and know about them all in the very details my curiosity leads me to. This is frustrating and thus a good exercise. When is the right moment to stop gathering ideas? And above all, what was I really looking for?

In 2005, during higher education, I was introduced to Seneca through De Vita Beata (“On the Happy Life”, 58 BCE) and thus to Stoicism. It had a profound impact on me at that time, as if I had found the noble way to happiness that very much corresponded to my Christian-born values, but in an atheist fashion – or so I thought. As a future scientist, I was convinced throughout my studies that physical laws could explain all there is, and the idea of a God, especially personal, was not necessary. It appears that Stoicism is pantheist, the belief that reality is identical with divinity, or that all-things compose an all-encompassing, transcendent (beyond all known physical laws) God. More precisely, Stoic theology is fatalistic and naturalistic pantheistic; God is never fully transcendent but always immanent (manifested in the material world), and identified with Nature. Would I have known that at that time, I would have certainly dismissed Stoic ideas. Though, a few years later, I came to know that Einstein believed in the pantheistic God, identified with Nature, of Spinoza. Because he was a scientist I admired, I followed his view, again without further analysis. He must have been right and in this way, I could find a satisfying position between my atheist and deist friends.

In 2017, a friend proposed to introduce me to a mindfulness meditation guidance that took place weekly at the heart of one of the world’s best university for life science. I had a mocking attitude with what I thought to be paradoxical but was curious. Indeed, I had lived various intriguing experiences the two previous years. They sparkled my desire to seek beyond the things I know and find some kind of answers or knowledge. Carl Jung coined the concept of synchronicity for “meaningful coincidences” and I feel that, since my first meditations, I let myself go to what I would call synchronicities. I met profoundly meaningful people, now close friends, and when I reflect back, just three years later, these encounters all put me on a coherent search for meaning. So was it for the books and publications that I have read, annotated, transcribed and put in this ontological compendium.

I consider myself a scientist. I am analytic and always use logic, even if nowadays, I also let my inner voice give insights and intuitions. Thus, my readings have concerned both universally accepted and fringe scientific knowledge, as well as philosophy and spirituality. My search for meaning led me naturally to the ontological question about the nature of reality and the epistemological question about the nature of knowledge. Both concepts and a few subsidiary concepts necessary for understanding this quest are scarcely defined in the following chart. Some may appear strange at first sight but are grounded on solid arguments as one will further read throughout the book.

There already are books addressing ontology, summarizing views of various philosophers. However, I wanted to know philosophers and famous scientists personal views. What I mean by “personal” is their own belief and feelings in their own words. I realized that even in quantum physics books, it is possible to find between equations understandable statements about what they mean and imply. But most fascinating, if one studies the work physicists have written in the twilight of their life, one will find intriguing and spiritual statements that are not enough acknowledged. All I wish with this book is to share knowledge that could maybe inspire others with their own thoughts, as much as the authors I quote did for me. I could have just shared the articles and books I read, but who would get such time or interest? I then thought about sharing large extracts, so people could get the context of the idea. I just hope this is not too bulky and messy. 

The book is divided into four chapters. The first one deals with what we can say about an objective reality; things that we can say exist even if we don’t perceive them. This topic is addressed from the point of view of ancient and modern philosophy, before jumping into what scientists say about it. The second chapter is concerned about what we know about determinism and how quantum mechanics brought up new ideas. The third chapter is the largest since many quantum mechanics principles and ideas are explained. The goal is to understand what we can say about what constitutes the world, what “matter” is, and how it is connected to “mind”. Finally, the last chapter directly addresses the question of this “mind” with a few ideas about the origin of life and consciousness, from a human and a cellular perspective.

Throughout the book, it is possible to see that science and philosophy can lead to extreme positions, sometimes completely opposite to one another, sometimes completely unverifiable. Concepts of matter, space and time seem to have limits, and so seems to be the human mind. One can thus close the book and be left with his own belief. The idea that resonated most for me were about the idea of a spread, truly instantaneous field of information that pervades the universe and that makes constant changes, in the vein of Bohmian mechanics. Further implications could be that this field is associated with consciousness, mind or some kind of “universal principle of love”, but as I said, everyone believes what he wishes. This is just food for thought. Ontological thoughts.

>“I neither know nor think that I know” (Plato, Apology of Socrates)

\-	This book was written with the help of the Stanford Encyclopedia of Philosophy, the Wikipedia Encyclopedia and the Internet Archive.

## Objective reality <a name="p2"></a>

### Introduction <a name="p21"></a>

Ontology is concerned with questions regarding what entities exist or may be said to exist and how such entities may be grouped, related within a hierarchy, and subdivided according to similarities and differences. In this chapter, the idea of an objective real physical world is debated, as well as what its constituent could be, if any. Logic and the senses, connected with actual scientific knowledge, lead philosophers and scientists versed in this question to extremely varied ideas and concepts. We will begin with ancient and modern philosophers. We will naturally reach natural science and see what prominent scientists of their time thought about the concept of reality, in a mix of ontology and epistemology. Here are two comments from Pauli and Heisenberg that should be kept in mind before diving into ancient and modern philosophy.

[Wolfgang Pauli](/references/wolfgangpauli.html)
>It is obviously out of the question for modern man to revert to the archaistic point of view that paid the price of its unity and completeness by a naïve ignorance of nature. His strong desire for a greater unification of his world view, however, impels him to recognize the significance of the pre-scientific stage of knowledge for the development of scientific ideas.

[Werner Heisenberg](/references/wernerheisenberg.html)
>Any concepts or words which have been formed in the past through the interplay between the world and ourselves are not really sharply defined with respect to their meaning; that is to say, we do not know exactly how far they will help us in finding our way in the world. We often know that they can be applied to a wide range of inner or outer experience, but we practically never know precisely the limits of their applicability. This is true even of the simplest and most general concepts like “existence” and “space and time”. Therefore, it will never be possible by pure reason to arrive at some absolute truth.

### Ancient philosophy <a name="p22"></a>

ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE 


I will divide this section into two groups. This first batch of thinkers are concerned about the concept of emptiness and vacuity. For Parmenides (VIth BCE, Greek), what can be spoken or thought of must be. On the contrary, Gorgias (Vth BCE, Greek) opposites this view by using an argument to absurdity to prove that nothing exists. In fact, in his ironic refutation, Gorgias shows that true objectivity is impossible since the human mind can never be separated from its possessor. Less radical nihilistic (nihil: indefinite nothing) considerations can be found in other traditions, such as some Buddhists doctrines. Kamalasila (VIIIth CE, Indian) for example talks about emptiness, explaining that all things are empty of intrinsic existence and nature. In the Tao tradition, emptiness has some form of meaning as can be seen in the texts of Laozi (VIth or IVth BCE, Chinese) and Zhuang Zhou (IVth BCE, Chinese). In the newborn Christian world, Basilides (IInd CE, Egyptian) exposes theological ontological thoughts.

Parmenides
>[In his poem On nature introduces Aletheia (the way of truth)] The mares, which carry me as far as my heart desires, were escorting me. They brought and placed me upon the all-speaking path of the Goddess, which carries everywhere unscathed the mortal who knows. […] The Goddess received me kindly, took my right hand in Hers, uttered speech and thus addressed me: […] “Come now, listen, and convey my story. I shall tell you what paths of inquiry alone there are for thinking. The one: that it is and it is impossible for it not to be. This is the path of Persuasion, for it accompanies Truth. […] Whatever can be spoken or thought of necessarily is, since it is possible for it to be, but it is not possible for nothing to be”.
>
>[Explained by [Werner Heisenberg](/references/wernerheisenberg.html)] Greek philosophy returned for some time to the concept of the One in the teaching of Parmenides, who lived in Elea in the south of Italy. His most important contribution to Greek thinking was, perhaps, that he introduced a purely logical argument into metaphysics. “One cannot know what is not – that is impossible – nor utter it; for it is the same thing that can be thought and that can be”. Therefore, only the One is, and there is no becoming or passing away. Parmenides denied the existence of empty space for logical reasons. Since all change requires empty space, as he assumed, he dismissed change as an illusion.

Gorgias, On non-existence, explained by Sextus Empiricus (IIIrd CE, skeptical philosopher who selected texts which refuted Aletheia) in Against the logicians:
>Gorgias develops three sequential arguments: first and foremost, that nothing exists; second, that even if existence exists, it is inapprehensible to humans; and, third, that even if existence is apprehensible, nevertheless it is certainly not able to be communicated or interpreted for one’s neighbors.

Kamalasila, Stages of meditation, explained by the current Dalai Lama (2018).
>One can overcome ignorance which is wrongly believing in a real existence. […] When we say that the ignorant mind is perverse or in error, we are talking about the way it misunderstands about reality. […] The real way of existence of things is different from the way they seem to exist. […] Emptiness … means that things are devoid of their own identity. It does not mean non-existence. Therefore, it is not the cause of a fall into nihilism. […] Emptiness does not mean nothingness; it means that things are empty of intrinsic existence.

Laozi, explained by Jung in Synchronicity – An acausal connecting principle (1952).
>In Chinese philosophy one of the oldest and most central ideas is that of Tao, which the Jesuits translated as “God”. But that is correct only for the Western way of thinking. Richard Wilhelm brilliantly interprets it as “meaning”. […] Lao-tzu gives the following description of Tao in his celebrated Tao Teh Ching:
>
>>There is something formless yet complete that existed before heaven and earth. How still! How empty! Dependent on nothing, unchanging, all pervading, unfailing. One may think of it as the mother of all things under heaven. I do not know its name, but I call it “Meaning”. If I had to give it a name, I should call it “The Great”.
>
>Tao “covers the ten thousand things like a garment but does not claim to be master over them”. Lao-tzu describes it as “Nothing”, by which he means, says Wilhelm, only its “contrast with the world of reality”. Lao-tzu describes its nature as follows:
>
>>We put thirty spokes together and call it a wheel; but it is on the space where there is nothing that the utility of the wheel depends. […] Therefore just as we take advantage of what is, we should recognize the utility of what is not.
>
>“Nothing” is evidently “meaning” or “purpose, and it is only called Nothing because it does not manifest itself in the world of the senses, but is only its organizer. Lao-tzu says:
>
>>Because the eye gazes but can catch no glimpse of it, it is called elusive. Because the ear listens but cannot hear it, it is called the rarefied. Because the hand feels for it but cannot find it, it is called the infinitesimal… These are called the shapeless shapes, forms without form, vague semblances. Go towards them, and you can see no front; go after them, and you see no rear.
>
>Wilhelm describes it as “a borderline conception lying at the extreme edge of the world of appearances”. […] “These seeds”, he continues, “point to something that corresponds firstly to the visible, i.e., something in the nature of an image; secondly to the audible, i.e., something in the nature of words; thirdly to extension in space, i.e., something with a form. But these three things are not clearly distinguished and definable, they are a non-spatial and non-temporal unity, having no above and below or front and back”. As the Tao Teh Ching says:
>
>>Incommensurable, impalpable, Yet latent in it are forms; Impalpable, incommensurable, Yet within it are entities. Shadowy it is and dim.
>
>Reality, thinks Wilhelm, is conceptually knowable because according to the Chinese view there is in all things a latent “rationality”. 

Zhuang Zhou, explained by Jung in Synchronicity – An acausal connecting principle.
>Chuang-tzu (a contemporary of Plato’s) says of the psychological premises on which Tao is based: “The state in which ego and non-ego are no longer opposed is called the pivot of Tao”. It sounds almost like a criticism of our scientific view of the world when he remarks that “Tao is obscured when you fix your eye on little segments of existence only” or “Limitations are not originally grounded in the meaning of life. Originally words had no fixed meanings. Differences only arose through looking at things subjectively”. […] “Outward hearing should not penetrate further than the ear; the intellect should not seek to lead a separate existence, thus the soul can become empty and absorb the whole world. It is Tao that fills the emptiness”. If you have insight, says Chuang-tzu, “you use your inner eye, your inner ear, to pierce to the heart of things, and have no need of intellectual knowledge”. This is obviously an allusion to the absolute knowledge of the unconscious, and to the presence of the microcosm of macrocosmic events.

Basilides, explained by Biaggi in Nihilism (1998).
>How to flee these supreme alienations that are time and matter? Basilides of Alexandria, which is, where he teaches, around 130 CE, at the heart of multiple traditions and influences (Judaism, Neoplatonism, Hermeticism, Coptic Christianity {which would be nihilistic, in the eyes of Crevier}), asserts that to fight illusion, the remedy does not lie in the accumulation of knowledge but in pure ignorance, an absolute spiritual asceticism which tends towards this nothingness that words cannot translate, that intelligence does not allow to understand. The gnostic quest consists in merging into an original time when nothing was and, specifies Basilides, “when I say nothing, it does not mean that there was nothing, it means that nothing itself did not exist”. It is to this vertiginous nihilism that gnosis opens and this primordial symbiosis is indeed fusion in God, named by Basilides, “He who is not”. Such proposals make gnosis, and some of the mystical practices, justiciable for the accusation of atheism or pantheism and they will not be in any way foreign to the persecutions which, in fact, cause the emptiness among the faithful men, by fire and by the sword. Mysticism accounts, in symbolic and poetic mode, for this fullness of emptiness, of silence which is not absence of words, defect of sounds, but the awakening imposed against sleep and nightmare by the meeting of a nothingness that never disturbs the din of the world. 

The previous texts have shown that, in the past, the notion of reality has been linked to something greater that any human can think of, whether it be called Truth, Meaning or God. Contemporary philosopher Vladimir Biaggi emphasizes the strong connection between the spiritual Western and Eastern traditions, with the help of two philosophers who have studied Buddhism: Arthur Schopenhauer (1788-1860, German) and Cioran (1911-1995, Romanian). It should be mentioned that Schopenhauer himself remarked correspondence between his doctrines and the Four Noble Truths of Buddhism (later discussed).

Biaggi, Nihilism (1998).
>Some of these mystical traditions as well as negative theologies have disturbing similarities or deep analogies with Taoism and Buddhism. Schopenhauer had already noted that all these ascetics and anchorites advocate and seek, beyond their differences, “true calm, absolute indifference to earthly things, abnegation of will, creation in God, complete self-forgetfulness and annihilation in the contemplation of God”. Annihilation is so correctly presented, wrongly, throughout 19th century Europe as the essential purpose of Buddhism, that Ernest Renan calls it the church of nihilism. The search for emptiness, the extinction which characterizes it, is given by the Sanskrit verb nirva which means to extinguish in “nothingness”. However, it must again be stressed with Cioran that “nothingness for Buddhism (and for the East in general) does not carry the somewhat sinister meaning that we attribute to it. It merges with a borderline experience of light, or in other words, with a state of eternal absence of light, of radiant emptiness”. The nirvana of Buddhism is again the attainment of fullness by austerity, which leads to the voluntary forgetting and death of the fiction that constitutes the “I”. Is there really a difference in nature between the oriental search for creative emptiness and such a fragment of Master Eckhart reminding initiates that true nobility consists in plunging into the emptiness of God?

We can now start with a second batch of thinkers, much less concerned about theology and nihilism, but more about the essence of reality. 

Thales, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>The idea of the smallest, indivisible ultimate building blocks of matter first came up in connection with the elaboration of the concepts of Matter, Being and Becoming which characterized the first epoch of Greek philosophy. This period started in the sixth century BC with Thales, the founder of the Milesian school, to whom Aristotle ascribes the statement: “Water is the material cause of all things”. This statement, strange as it looks to us, expressed, as Nietzsche has pointed out, three fundamental ideas of philosophy. First, the question as to the material cause of all things; second, the demand that this question be answered in conformity with reason, without resort to myths or mysticism; third, the postulate that ultimately it must be possible to reduce everything to one principle. Thales’ statement was the first expression of the idea of a fundamental substance, of which all other things were transient forms. The word “substance” in this connection was certainly in that age not interpreted in the purely material sense which we frequently ascribe to it today. Life was connected with or inherent in this “substance” and Aristotle ascribes to Thales also the statement: All things are full of gods. Still the question was put as to the material cause of all things. […] Of all things we know water can take the most various shapes; it can in the winter take the form of ice and snow, it can change into vapor, and it can form the clouds. It seems to turn into earth where the rivers form their delta, and it can spring from the earth. Water is the condition for life. Therefore, if there was such a fundamental substance, it was natural to think of water first.

Anaximander, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>The idea of the fundamental substance was then carried further by Anaximander, who was a pupil of Thales and lived in the same town. Anaximander denied the fundamental substance to be water or any of the known substances. He taught that the primary substance was infinite, eternal and ageless and that it encompassed the world. This primary substance is transformed into the various substances with which we are familiar. Theophrastus quotes from Anaximander: “Into that from which things take their rise they pass away once more, as is ordained, for they make reparation and satisfaction to one another for their injustice according to the ordering of time”. In this philosophy the antithesis of Being and Becoming plays the fundamental role. The primary substance, infinite and ageless, the undifferentiated Being, degenerates into the various forms which lead to endless struggles. The process of Becoming is considered as a sort of debasement of the infinite Being – a disintegration into the struggle ultimately expiated by a return into that which is without shape or character. The struggle which is meant here is the opposition between hot and cold, fire and water, wet and dry, etc. The temporary victory of the one over the other is the injustice for which they finally make reparation in the ordering of time. According to Anaximander, there is “eternal motion”, the creation and passing away of worlds from infinity to infinity. […] All different elementary particles could be reduced to some universal substance which we may call energy or matter, but none of the different particles could be preferred to the others as being more fundamental. The latter view of course corresponds to the doctrine of Anaximander, and I am convinced that in modern physics this view is the correct one.

Anaximenes, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>The third of the Milesian philosophers, Anaximenes, an associate of Anaximander, taught that air was the primary substance. “Just as our soul, being air, holds us together, so do breath and air encompass the whole world”. Anaximenes introduced into the Milesian philosophy the idea that the process of condensation or rarefaction causes the change of the primary substance into the other substances. The condensation of water vapor into clouds was an obvious example, and of course the difference between water vapor and air was not known at that time.

The Milesian school introduced new opinions contrary to the prevailing belief of how the world was organized, in which natural phenomena were explained solely by the will of anthropomorphized gods. The Milesians conceived of nature in terms of methodologically observable entities, and as such was one of the first truly scientific philosophies. The Milesian school is not synonymous with the Ionian, which includes the philosophies of the Milesians plus distinctly different Ionian thinkers such as Heraclitus. The Ionian School looked more into the thought behind everything while the Milesian School was more focused on nature.

Heraclitus, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>In the philosophy of Heraclitus of Ephesus, the concept of Becoming occupies the foremost place. He regarded that which moves, the first, as the basic element. The difficulty, to reconcile the idea of one fundamental principle with the infinite variety of phenomena, is solved for him by recognizing that the strife of the opposites is really a kind of harmony. For Heraclitus the world is at once one and many, it is just “the opposite tension” of the opposites that constitutes the unity of the One. […] When one carried the idea of fundamental unity to the extreme one came to that infinite and eternal undifferentiated Being which, whether material or not, cannot in itself explain the infinite variety of things. This leads to the antithesis of Being and Becoming and finally to the solution of Heraclitus, that the change itself is the fundamental principle, the “imperishable change, that renovates the world”, as the poets have called it. But the change in itself is not a material cause and therefore is represented in the philosophy of Heraclitus by the fire as the basic element, which is both matter and a moving force. We may remark at this point that modern physics is in some way extremely near to the doctrines of Heraclitus. If we replace the word “fire” by the word “energy” we can almost repeat his statements word for word from our modern point of view. […] Energy is a substance, since its total amount does not change, and the elementary particles can actually be made from this substance as is seen in many experiments on the creation of elementary particles. Energy can be changed into motion, into heat, into light and into tension. Energy may be called the fundamental cause for all change in the world.

Empedocles, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>Empedocles, from the south coast of Sicily, changed for the first time from monism to a kind of pluralism. To avoid the difficulty that one primary substance cannot explain the variety of things and events, he assumed four basic elements, earth, water, air and fire. The elements are mixed together and separated by the action of Love and Strife. Therefore, these latter two, which are in many ways treated as corporeal like the other four elements, are responsible for the imperishable change. Empedocles describes the formation of the world in the following picture: First, there is the infinite Sphere of the One, as in the philosophy of Parmenides. But in the primary substance all the four “roots” are mixed together by Love. Then, when Love is passing out and Strife coming in, the elements are partially separated and partially combined. After that the elements are completely separated and Love is outside the World. Finally, Love is bringing the elements together again and Strife is passing out, so that we return to the original Sphere. This doctrine of Empedocles represents a very definite turning toward a more materialistic view in Greek philosophy. The four elements are not so much fundamental principles as real material substances. Here for the first time the idea is expressed that the mixture and separation of a few substances, which are fundamentally different, explains the infinite variety of things and events. Pluralism never appeals to those who are wont to think in fundamental principles. But it is a reasonable kind of compromise, which avoids the difficulty of monism and allows the establishment of some order.

Anaxagoras gave a complete account of the universe: the heavens, the earth, geological and meteorological phenomena. Further, and intriguingly, Anaxagoras claims that the cosmic rotary motion could produce other worlds like our own. 

Anaxagoras, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>The next step toward the concept of the atom was made by Anaxagoras, who was a contemporary of Empedocles. He lives in Athens about thirty years, probably in the first half of the fifth century B.C. Anaxagoras stresses the idea of the mixture, the assumption that all change is caused by mixture and separation. He assumed an infinite variety of infinitely small “seeds”, of which all things are composed. The seeds do not refer to the four elements of Empedocles, there are innumerably many different seeds. But the seeds are mixed together and separated again and in this way all change is brought about. The doctrine of Anaxagoras allows for the first time a geometrical interpretation of the term “mixture”: Since he speaks of the infinitely small seeds, their mixture can be pictured as the mixture between two kinds of sand of different colors. The seeds may change in number and in relative position. Anaxagoras assumes that all seeds are in everything, only the proportion may change from one thing to another. He says: “All things will be in everything, nor is it possible for them to be apart, but all things have a portion of everything”. The universe of Anaxagoras is set in motion not by Love and Strife, like that of Empedocles, but by “Nous”, which we may translate as “Mind”.

The Greek tradition regarded Leucippus as the founder of atomism. Little is known about him, and his views are hard to distinguish from those of his associate Democritus. He is sometimes said to have devised the atomist philosophy in order to escape from the problems raised by Parmenides.

Leucippus, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>From this philosophy it was only one step to the concept of the atom, and this step occurred with Leucippus and Democritus of Abdera. The antithesis of Being and Not-Being in the philosophy of Parmenides is here secularized into the antithesis of the “Full” and the “Void”. Being is not only One, it can be repeated an infinite number of times. This is the atom, the indivisible smallest unit of matter. The atom is eternal and indestructible, but it has a finite size. Motion is made possible through the empty space between the atoms. Thus for the first time in history there was voiced the idea of the existence of smallest ultimate particles – we would say of elementary particles, as the fundamental blocks of matter. […] The atoms in the philosophy of Leucippus do not move merely by chance. Leucippus seems to have believed in complete determinism, since he is known to have said: “Naught happens for nothing, but everything from a ground and of necessity”. The atomists did not give any reason for the original motion of the atoms, which just shows that they thought of a causal description of the atomic motion; causality can only explain later events by earlier events, but it can never explain the beginning.

Democritus
>[Explained by Bohr in Atomic physics and human knowledge (1957)] Democritus, who with so deep intuition emphasized the necessity of atomism for any rational account of the ordinary properties of matter, attempted, as is well known, also to utilize atomistic ideas for the explanation of the peculiarities of organic life and even of human psychology.
>
>[Explained by Bohm and Hiley in The undivided universe (1993)] The idea of Democritus was inspired by a consideration of a philosophical paradox in the very notion of movement, which was shown by Zeno and another members of the Eleatic school. They demonstrated logical inconsistencies, not only in the idea of movement itself, but also in the idea of an empty space in which movement could take place. From their demonstrations they concluded that only being is and that nonbeing is not, from which it followed that the concepts of becoming along with those of movement and empty space had no meaning. Through contemplation of this paradox, Democritus was led to a resolution. He proposed that being was discontinuous in the form of distinct and separated atoms, each of which was an indivisible and eternal being. Democritus thus retained, in a certain restricted way, the Eleatic concept of being. But he then postulated that there was an empty space in which these atomic beings could move in such a way that the collective aspect of their motion then explained the large scale properties of apparently continuous matter with its continuous changes. On the other hand if we suppose that matter was truly continuous rather than only apparently so, then, as the Eleatics showed, there was no way in which its movement and transformation could have meaning.
>
>[Explained by [Werner Heisenberg](/references/wernerheisenberg.html)] According to this new concept of the atom, matter did not consist only of the “Full”, but also of the “Void”, of the empty space in which the atoms move. The logical objection of Parmenides against the Void, that not-being cannot exist, was simply ignored to comply with experience. From our modern point of view we would say that the empty space between the atoms in the philosophy of Democritus was not nothing; it was the carrier for geometry and kinematics, making possible the various arrangements and movements of atoms. But the possibility of empty space has always been a controversial problem in philosophy. In the theory of general relativity the answer is given that geometry is produced by matter or matter by geometry. This answer corresponds more closely to the view held by many philosophers that space is defined by the extension of matter. But Democritus clearly departs from this view, to make change and action possible. The atoms of Democritus were all of the same substance, which had the property of being, but had different sizes and different shapes. They were pictured therefore as divisible in a mathematical but not in a physical sense. The atoms could move and could occupy different positions in space. But they had no other physical properties. They had neither color nor smell nor taste. The properties of matter which we perceive by our senses were supposed to be produced by the movements and positions of the atoms in space. Just as both tragedy and comedy can be written by using the same letters of the alphabet, the vast variety of events in this world can be realized by the same atoms through their different arrangements and movements. Geometry and kinematics, which were made possible by the void, proved to be still more important in some way than pure being. Democritus is quoted to have said: “A thing merely appears to have color, it merely appears to be sweet or bitter. Only atoms and empty space have a real existence”. […] It is obvious that if anything in modern physics should be compared with the atoms of Democritus it should be the elementary particles like proton, neutron, electron, meson. Democritus was well aware of the fact that if the atoms should, by their motion and arrangement, explain the properties of matter – color, smell, taste – they cannot themselves have these properties. Therefore, he has deprived the atom of these qualities and his atom is thus a rather abstract piece of matter. But Democritus has left to the atom the quality of “being”, of extension in space, of shape and motion. He has left these qualities because it would have been difficult to speak about the atom at all if such qualities had been taken away from it. On the other hand, this implies that his concept of the atom cannot explain geometry, extension in space or existence, because it cannot reduce them to something more fundamental. The modern view of the elementary particle with regard to this point seems more consistent and more radical. […] If one wants to give an accurate description of the elementary particle – and here the emphasis is on the word “accurate” – the only thing which can be written down as description is a probability function. But then one sees that not even the quality of being (if that may be called a “quality”) belongs to what is described. It is a possibility for being or a tendency for being. Therefore, the elementary particle of modern physics is still far more abstract than the atom of the Greeks, and it is by this very property more consistent as a clue for explaining the behavior of matter. In the philosophy of Democritus all atoms consist of the same substance if the word “substance” is to be applied here at all. The elementary particles in modern physics carry a mass in the same limited sense in which they have other properties. Since mass and energy are, according to the theory of relativity, essentially the same concepts, we may say that all elementary particles consist of energy. This could be interpreted as defining energy as the primary substance of the world.

For Plato, the world that appears to our senses is defective, but there is a more real and perfect realm, populated by eternal entities called “forms”.

Plato
>[Explained by Jung in Synchronicity – An acausal connecting principle (1952)] Synchronicity postulates a meaning which is a priori in relation to human consciousness and apparently exists outside man. Such an assumption is found above all in the philosophy of Plato, which takes for granted the existence of transcendental images or models of empirical things, the forms (species), whose reflections we see in the phenomenal world. This assumption not only presented no difficulty to earlier centuries but was on the contrary perfectly self-evident.
>
>[Explained by [Werner Heisenberg](/references/wernerheisenberg.html)] The basic ideas of atomic theory were taken over and modified, in part, by later Greek philosophers. For the sake of comparison with modern atomic physics it is important to mention the explanation of matter by Plato in his dialogue Timaeus. Plato was not an atomist; on the contrary, Diogenes Laertius reported that Plato disliked Democritus so much that he wished all his books to be burned. But Plato combined ideas that were near to atomism with doctrines of the Pythagorean school and the teachings of Empedocles. […] Plato knew of the discovery of the regular solids made by the Pythagoreans and of the possibility of combining them with the elements of Empedocles. He compared the smallest parts of the element earth with the cube, of air with the octahedron, of fire with the tetrahedron, and of water with the icosahedron. There is no element that corresponds to the dodecahedron, here Plato only says “there was yet a fifth combination which God used in the delineation of the universe”. […] One atom of fire and two atoms of air can be combined to give one atom of water. But the fundamental triangles cannot be considered as matter, since they have no extension in space. It is only when the triangles are put together to form a regular solid that a unit of matter is created. The smallest parts of matter are not the fundamental Beings, as in the philosophy of Democritus, but are mathematical forms. Here it is quite evident that the form is more important than the substance of which it is the form. […] In the first centuries of Greek culture the strongest impulse had come from the immediate reality of the world in which we live and which we perceive by our senses. This reality was full of life and there was no good reason to stress the distinction between matter and mind or between body and soul. But in the philosophy of Plato one already sees that another reality begins to become stronger. In the famous simile of the cave Plato compares men to prisoners in a cave who are bound and can look in only one direction. They have a fire behind them and see on a wall the shadows of themselves and of objects behind them. Since they see nothing but the shadows, they regard those shadows as real and are not aware of the objects. Finally one of the prisoners escapes and comes from the cave into the light of the sun. For the first time he sees real things and realizes that he had been deceived hitherto by the shadows. For the first time he knows the truth and thinks only with sorrow of his long life in the darkness. The real philosopher is the prisoner who has escaped from the cave into the light of truth, he is the one who possesses real knowledge. This immediate connection with truth or, we may in the Christian sense say, with God is the new reality that has begun to become stronger than the reality of the world as perceived by our senses. The immediate connection with God happens within the human soul, not in the world, and this was the problem that occupied human thought more than anything else in the two thousand years following Plato. In this period the eyes of the philosophers were directed toward the human soul and its relation to God, to the problems of ethics, and to the interpretation of the revelation but not to the outer world. It was only in the time of the Italian Renaissance that again a gradual change of the human mind could be seen, which resulted finally in a revival of the interest in nature.

Aristotle thinks of the form as prior to the matter, and thus more fundamental than the matter. As a result, he extended the notion of matter beyond its original role as metaphysical substrate. In Metaphysics (literally, ‘after the Physics’), Aristotle offers a suggestive argument to the effect that matter alone cannot be substance.

Aristotle
>[Explained by Bohr in Atomic physics and human knowledge (1957)] In view of the fantastic character of such extreme materialistic conceptions [of Democritus], it was a natural reaction when Aristotle, with his masterly comprehension of the knowledge of his time in physics as well as in biology, rejected atomic theory entirely and tried to provide a sufficiently broad frame for an account of the wealth of natural phenomena on the basis of essentially teleological ideas. The exaggeration of the Aristotelian doctrine, on its side, was, however, clearly brought to light by the gradual recognition of elementary laws of nature valid as well for inanimate bodies as for living organisms.
>
>[Explained by [Werner Heisenberg](/references/wernerheisenberg.html)] In the philosophy of Aristotle, the total space of the universe was finite (though it was infinitely divisible). Space was due to the extension of bodies, it was connected with the bodies; there was no space where there were no bodies. The universe consisted of the earth and the sun and the stars: a finite number of bodies. Beyond the sphere of the stars there was no space; therefore, the space of the universe was finite.
>
>[Explained by Hawking in Brief answers to the big questions (2018)] Aristotle, the most famous of the Greek philosophers, believed that the universe had existed for ever. Something eternal is more perfect than something created. He suggested the reason we see progress was that floods, or other natural disasters, had repeatedly set civilization back to the beginning. The motivation for believing in an eternal universe was the desire to avoid invoking divine intervention to create the universe and set it going. Conversely, those who believed that the universe had a beginning used it as an argument for the existence of God as the first cause, or prime mover, of the universe.

We conclude this section with theology. First, with Augustine (354-430, Algerian) who was perhaps the greatest Christian philosopher of Antiquity and certainly the one who exerted the deepest and most lasting influence. Just as the late-antique Platonists developed their cosmological thinking by commenting on Plato’s Timaeus, Augustine’s natural philosophy is largely a theory of creation based on an exegesis of the opening chapters of Genesis. His idea about time being created is very actual as we will see in modern physics.

Saint Augustine, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>A similar situation is found with respect to the problem of the infinity of time. In the Confessions of St. Augustine, for instance, this question takes the form: What was God doing before He created the world? Augustine is not satisfied with the joke: “God was busy preparing Hell for those who ask foolish question”. This, he says, would be too cheap an answer, and he tries to give a rational analysis of the problem. Only for us is time passing by; it is expected by us as future; it passes by as the present moment and is remembered by us as past. But God is not in time; a thousand years are for Him as one day, and one day as a thousand years. Time has been created together with the world, it belongs to the world, therefore time did not exist before the universe existed. For God the whole course of the universe is given at once. There was no time before He created the world. It is obvious that in such statements the world “created” at once raises all the essential difficulties. This world as it is usually understood means that something has come into being that has not been before, and in this sense it presupposes that concept of time. Therefore, it is impossible to define in rational terms what could be meant by the phrase “time has been created”. This fact reminds us again of the often discussed lesson that has been learned from modern physics: that every word or concept, clear as it may seem to be, has only a limited range of applicability.

Saint Anselm of Canterbury (1033–1109, Italian) was a Christian philosopher and theologian. He is best known for the celebrated “ontological argument” for the existence of God.

Saint Anselm, Proslogion (1078).
>I began to ask myself whether there might not perhaps be found someone argument which should have no need of any other argument beside itself to prove it, and might suffice by itself to demonstrate that God really exists and is the Supreme Good, which needeth nothing beside itself to give it being or well-being, but without which nothing else can have either the one or the other; and whereof all other things are true which we believe concerning the divine essence. […] Even the fool is certain that something exists, at least in his understanding, than which nothing greater can be conceived; because, when he hears this mentioned, he understands it, and whatsoever is understood, exists in the understanding. And surely that than which no greater can be conceived cannot exist only in the understanding. For if it exists indeed in the understanding only, it can be thought to exist also in reality; and real existence is more than existence in the understanding only. If then that than which no greater can be conceived exists in the understanding only, then that than which no greater can be conceived is something a greater than which can be conceived: but this is impossible. Therefore it is certain that something than which no greater can be conceived exists both in the understanding and also in reality.


### Modern philosophy <a name="p23"></a>

ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE ADD THE PICTURE 

Baruch Spinoza (1632-1677, Dutch), popularized pantheism (reality is identical with divinity; all-things compose an all-encompassing, transcendent god) in the Western culture. Yet, it is not clear that this is the proper way to look at his conception of God. Still, for Spinoza, God and Nature are essentially the same thing. For Robert Misrahi (1926-, French), “there is neither immortality nor the mystery of the afterlife, nor metempsychosis, nor backworld” in the Ethics. Yet something of the human mind is eternal. Misrahi concludes that the Ethics are not concerned with arbitrariness but solipsism. In general, pantheism is the view that rejects the transcendence of God and as such, Spinoza’s ideas sparkled the pantheism controversy, a German debate on this subject that started with Friedrich Jacobi (1743-1819, German). The later, close to the following views of Blaise Pascal (1623-1662, French), accused Spinoza of atheism but got himself accused of irrationalism. Johann Goethe (1749-1832, German) said “Spinoza does not demonstrate the existence of God; it is existence which is God”. Spinoza had a major impact on philosophy. Georg Hegel (1770-1831, German) said that “Spinoza is the crucial moment of modern philosophy: either Spinozism or there will be no philosophy”. Einstein shared Spinoza’s pantheistic idea of God which is (usually) not a personal God.

Spinoza, Ethics (1674).
>Nature does not work with an end in view. For the eternal and infinite Being, which we call God or Nature, acts by the same necessity as that whereby it exists. […] The reason or cause why God or Nature exists, and the reason why he acts, are one and the same. Therefore, as he does not exist for the sake of an end, so neither does he act for the sake of an end; of his existence and of his action there is neither origin nor end. […] The human mind cannot be absolutely destroyed with the body, but there remains of it something which is eternal. […] Yet it is not possible that we should remember that we existed before our body, for our body can bear no trace of such existence, neither can eternity be defined in terms of time, or have any relation to time.

Pascal, Pensées (1670).
>We know the existence and the nature of the finite, because we are finite and extended like it. We know the existence of the infinite and ignore its nature, because it has expanded like us, but is not bounded like us. But we do not know the existence or the nature of God, because he has neither extension nor bounds. But by faith we know of its existence. […] Man has this choice, this unique choice: either nothingness or a God. Choosing nothingness, he makes himself God and makes a ghost a God, because it is impossible, if there is no God, that man and everything around him are not only a ghost.

Einstein, The world as I see it (1934).
>Let us recognize at the base of all scientific work of a certain scale, a conviction very comparable to religious sentiment, since it accepts a world founded on reason, an intelligible world! This conviction, linked to a deep feeling of a superior reason, revealing itself in the world of experience, translates for me the idea of God. In simple language, we would translate like Spinoza, by the term “pantheism”.

René Descartes (1596-1650, French) offered a new vision of the natural world that continues to shape our thought today: a world of matter possessing a few fundamental properties and interacting according to a few universal laws. This natural world included an immaterial mind that, in human beings, was directly related to the brain; in this way, Descartes formulated the modern version of the mind–body problem. While, as we have seen, Pascal rejected the suggestion that one could prove the existence of God by rational arguments, Descartes provided arguments for the existence of God, to show that the essence of matter is extension, and that the essence of mind is thought. 

Descartes, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>Descartes realizes that what we know about our mind is more certain than what we know about the outer world. But already his starting point with the “triangle” God-World-I simplifies in a dangerous way the basis for further reasoning. The division between matter and mind or between soul and body, which had started in Plato’s philosophy, is now complete. God is separated from the I and from the world. God in fact is raised so high above the world and men that He finally appears in the philosophy of Descartes only as a common point of reference that establishes the relation between the I and the world. […] The position to which the Cartesian partition has led with respect to the “res extensa” was what one may call metaphysical realism. 

According to metaphysical realism, the world is as it is independently of how humans or other inquiring agents take it to be. Many philosophers believe metaphysical realism is just plain common sense. Others believe it to be a direct implication of modern science, which paints humans as fallible creatures adrift in an inhospitable world not of their making. Nonetheless, metaphysical realism is controversial.

Descartes, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>The world, i.e., the extended things, “exist”. This is to be distinguished from practical realism, and the different forms of realism may be described as follows: We “objectivate” a statement if we claim that its content does not depend on the conditions under which it can be verified. Practical realism assumes that there are statements that can be objectivated and that in fact the largest part of our experience in daily life consists of such statements. Dogmatic realism claims that there are no statements concerning the material world that cannot be objectivated. […] When Einstein has criticized quantum theory he has done so from the basis of dogmatic realism. This is a very natural attitude. […] But quantum theory is in itself an example for the possibility of explaining nature by means of simple mathematical laws without this basis. […] Metaphysical realism goes one step further than dogmatic realism by saying that “the things really exist”. This is in fact what Descartes tried to prove by the argument that “God cannot have deceived us”. The statement that the things really exist is different from the statement of dogmatic realism in so far as here the word “exists” occurs, which is also meant in the other statement “cogito ergo sum”… “I think therefore I am”. But it is difficult to see what is meant at this point that is not yet contained in the thesis of dogmatic realism; and this leads us to a general criticism of the statement “cogito ergo sum”, which Descartes considered as the solid ground on which he could build his system. It is in fact true that this statement has the certainty of a mathematical conclusion, if the words “cogito” and “sum” are defined in the usual way or, to put it more cautiously and at the same time more critically, if the words are so defined that the statement follows. But this does not tell us anything about how far we can use the concepts of “thinking” and “being” in finding our way. It is finally in a very general sense always an empirical question how far our concepts can be applied.

Heisenberg continues by saying that “the difficulty of metaphysical realism was felt soon after Descartes and became the starting point for the empiristic philosophy, for sensualism and positivism. The three philosophers who can be taken as representatives for early empiristic philosophy are Locke, Berkeley and Hume”. There are many different positions which qualify as antirealism. The most important strains of antirealism have been varieties of empiricism which, given their emphasis on experience as a source and subject matter of knowledge, are naturally set against the idea of knowledge of unobservables. John Locke (1632-1704, English) wrote one of the first great defenses of modern empiricism which concerns itself with determining the limits of human understanding.

Locke, explained by [Werner Heisenberg](/references/wernerheisenberg.html).
>Locke holds, contrary to Descartes, that all knowledge is ultimately founded in experience. This experience may be sensation or perception of the operation of our own mind. Knowledge, so Locke states, is the perception of the agreement or disagreement of two ideas. 

Successors of Locke such as George Berkeley (1685-1753, Irish), defended idealism, the view that reality consists exclusively of minds and their ideas. He influenced David Hume (1711-1776, Scottish), defending empiricism, skepticism and naturalism.

Berkeley
>[Explained by [Werner Heisenberg](/references/wernerheisenberg.html)] The next step was taken by Berkeley. If actually all our knowledge is derived from perception, there is no meaning in the statement that the things really exist; because if the perception is given it cannot possibly make any difference whether the things exist or do not exist. Therefore, to be perceived is identical with existence. 
>
>[Explained by Einstein in The world as I see it (1934)] Berkeley insists that we do not directly grasp the objects of the outside world by our senses, but that the organs of our senses are affected by phenomena causally linked to the presence of objects.

Hume
>[Explained by [Werner Heisenberg](/references/wernerheisenberg.html)] This line of argument then was extended to an extreme skepticism by Hume, who denied induction and causation and thereby arrived at a conclusion which if taken seriously would destroy the basis of all empirical science.
>
>[Explained by Einstein in The world as I see it (1934)] Hume understood well that concepts considered essential by ourselves – for example the causal relation – cannot be obtained from the matter provided by the senses. This intelligence leads him to an intellectual skepticism vis-à-vis all knowledge.
>
>[Explained by Chomsky in Language and nature (1995)] Hume was concerned with “the science of human nature”, and sought to find “the secret springs and principles, by which the human mind is actuated in its operations”, including those “parts of [our] knowledge” that are derived “by the original hand of nature”, an enterprise he compared to Newton’s.

[Werner Heisenberg](/references/wernerheisenberg.html)
>The criticism of metaphysical realism which has been expressed in empiristic philosophy is certainly justified in so far as it is a warning against the naive use of the term “existence”. […] Our perceptions are not primarily bundles of colors or sounds; what we perceive is already perceived as something, the accent here being on the word “thing”, and therefore it is doubtful whether we gain anything by taking the perceptions instead of the things as the ultimate elements of reality. The underlying difficulty has been clearly recognized by modern positivism. This line of thought expressed criticism against the naive use of certain terms like “thing”, “perception”.

Like most of his great contemporaries (Descartes, Spinoza, Malebranche), Gottfried Leibniz (1646–1716, German) developed a number of arguments for the existence of God. Yet, unlike Descartes and Spinoza at least, Leibniz also expended great efforts in explaining and justifying God’s justice in this world. Departing from Leibniz, Immanuel Kant (1724–1804, German) synthesized early modern rationalism and empiricism. He set the terms for much of modern philosophy, and continues to exercise a significant influence today in many fields. The fundamental idea of Kant’s “critical philosophy” is human autonomy. For him, knowledge of the world as a whole, or of entities that transcend this world (the immortal soul or God) is not humanly possible. Kant reported that Hume’s work woke him from his “dogmatic slumbers”. Before Kant’s first Critique, empiricists (Hume) and rationalists (Leibniz) assumed that all synthetic statements required experience to be known. Kant contested this assumption by claiming that elementary mathematics, like arithmetic, are synthetic a priori, in that its statements provide new knowledge not derived from experience. This becomes part of his over-all argument for transcendental idealism. That is, he argues that the possibility of experience depends on certain necessary conditions – which he calls a priori forms – and that these conditions structure and hold true of the world of experience. He claimed that mathematic judgments are synthetic a priori and that space and time are not derived from experience but rather are its preconditions.

Kant
>[The critique of pure reason (1787)] Human reason has the peculiar fate in one species of its cognitions that it is burdened with questions which it cannot dismiss, since they are given to it as problems by the nature of reason itself, but which it also cannot answer, since they transcend every capacity of human reason. […] I had to deny knowledge in order to make room for faith. […] Space and time are the framework within which the mind is constrained to construct its experience of reality.
>
>[Explained by Hawking in Brief answers to the big questions (2018)] The problem of whether or not the universe had a beginning was a great concern to the German philosopher Immanuel Kant. He felt there were logical contradictions, or antinomies, either way. If the universe had a beginning, why did it wait an infinite time before it began? He called that the thesis. On the other hand, if the universe had existed for ever, why did it take an infinite time to reach the present stage? He called that the antithesis. Both the thesis and antithesis depended on Kant’s assumption, along with almost everyone else, that time was absolute. That is to say, it went from the infinite past to the infinite future independently of any universe that might or might not exist.
>
>[Explained by [Werner Heisenberg](/references/wernerheisenberg.html)] A combination of those two lines of thought that started from Descartes, on the one side, and from Locke and Berkeley, on the other, was attempted in the philosophy of Kant, who was the founder of German idealism. That part of his work which is important in comparison with the results of modern physics is contained in The Critique of Pure Reason. He takes up the question whether knowledge is only founded in experience or can come from other sources, and he arrives at the conclusion that our knowledge is in part “a priori” and not inferred inductively from experience. Therefore, he distinguishes between “empirical” knowledge and knowledge that is “a priori”. At the same time he distinguishes between “analytic” and “synthetic” propositions. Analytic propositions follow simply from logic, and their denial would lead to self-contradiction. Propositions that are not “analytic” are called “synthetic”. […] Space and time are, he says, a priori forms of pure intuition. […] Kant has not followed the line of Berkeley and Hume, though that would have been logically consistent. He kept the notion of the “thing-in-itself” as different from the percept, and in this way kept some connection with realism. Coming now to the comparison of Kant’s doctrines with modern physics, it looks in the first moment as though his central concept of the “synthetic judgments a priori” had been completely annihilated by the discoveries of our century. The theory of relativity has changed our views on space and time, it has in fact revealed entirely new features of space and time, of which nothing is seen in Kant’s a priori forms of pure intuition. The law of causality is no longer applied in quantum theory and the law of conservation of matter is no longer true for the elementary particles. Obviously Kant could not have foreseen the new discoveries, but since he was convinced that his concepts would be “the basis of any future metaphysics that can be called science” it is interesting to see where his arguments have been wrong. […] The a priori concepts which Kant considered an undisputable truth are no longer contained in the scientific system of modern physics. […] Space cannot be finite, since we cannot imagine that there should be an end to space; to whichever point in space we come we can always imagine that we can go beyond. At the same time space cannot be infinite, because space is something we can imagine (else the word “space” would not have been formed) and we cannot imagine an infinite space. For this second thesis the argument of Kant has not been verbally reproduced. The sentence “space is infinite” means for us something negative; we cannot come to an end of space. For Kant it means that the infinity of space is really given, that it “exists” in a sense that we can scarcely reproduce. Kant’s result is that a rational answer to the question whether space is finite or infinite cannot be given because the whole universe cannot be the object of our experience.

Carl Gauss (1777-1855, German) was an uncompromising believer in the priority of empiricism in science. He did not adhere to the views of Kant, Hegel and other idealist philosophers of the day. He was not a churchman and kept his religious views to himself. Moral rectitude and the advancement of scientific knowledge were his avowed principles.

Gauss, explained by Jung in Synchronicity – An acausal connecting principle (1952).
>The idea of an a priori meaning may also be found in the older mathematics, as in the mathematician Jacobi’s paraphrase of Schiller’s poem “Archimedes and His Pupil”. He praises the calculation of the orbit of Uranus and closes with the lines: “What you behold in the cosmos is only the light of God’s glory; In the Olympian host Number eternally reigns”. The great mathematician Gauss is the putative author of the saying: “God does arithmetic”. But in a letter of 1830 Gauss says: “We must in all humility admit that if number is merely a product of our mind, space has a reality outside our mind”.

On the contrary, Arthur Schopenhauer (1788-1860, German) contended that at its core, the universe is not a rational place. Inspired by Plato and Kant, both of whom regarded the world as being more amenable to reason, Schopenhauer developed their philosophies into an instinct-recognizing outlook. He believed that the supreme principle of the universe is likewise apprehensible through introspection, and that we can understand the world as various manifestations of this general principle. For Schopenhauer, this is not the principle of self-consciousness and rationally-infused will, but is rather what he simply calls “will” – a mindless, aimless, non-rational impulse at the foundation of our instinctual drives, and at the foundational being of everything. His position is that will and representations are one and the same reality, regarded from different perspectives. For him, “the concept of nothing is essentially relative” since it “always refers to a definite something that it negates”. By saying so, he retains only Kant’s position on the nihil privativum (a “minus” can only be understood in contrast to “plus”) and not on the nihil negativum (nothing, in every respect). What for him is negated is the “world as representation”, the “mirror of the will”. Negation of the will exposes to emptiness but that emptiness is in fact the only thing that is and it is the world we think real that is nothing. 
 
Schopenhauer, The world as will and representation (1819).
>What is universally assumed as positive, what we call being, the negation of which is expressed by the concept nothing in its most general significance, is exactly the world as representation, which I have shown to be the objectivity, the mirror, of the will. […] The form of this representation is space and time; and so … everything that exists must be in some place and at some time. […] We must not even evade [nothingness], as the Indians do, by myths and meaningless words, such as reabsorption in Brahman, or the Nirvana of the Buddhists. On the contrary, we freely acknowledge that what remains after the complete abolition of the will is, for all who are still full of the will, assuredly nothing. But also conversely, to those in whom the will has turned and denied itself, this very real world of ours with all its suns and galaxies, is – nothing. This is also the Prajna-Paramita of the Buddhists, the “beyond all knowledge”, in other words, the point where subject and object no longer exist.

From this stage, a lot of ontological and epistemological arguments have already been expressed. They form the basis of what more recent philosophers, epistemologists and logicians have accomplished. To keep this history short as well as avoid problems of classifications and interpretations, we will end there. We will just see a few ideas from psychiatry (Carl Jung), linguistics (Noam Chomsky) and literature (Vladimir Biaggi citing Goethe and Hugo).

Jung, Synchronicity – An acausal connecting principle.
>Agrippa is suggesting that there is an inborn “knowledge” or “imagination” in living organisms, an idea which recurs in our own day in Hans Driesch. Whether we like it or not, we find ourselves in this embarrassing position as soon as we begin seriously to reflect on the teleological processes in biology or to investigate the compensatory function of the unconscious, not to speak of trying to explain the phenomenon of synchronicity. Final causes, twist them how we will, postulate a foreknowledge of some kind. It is certainly not a knowledge that could be connected with the ego, and hence not a conscious knowledge as we know it, but rather a self-subsistent “unconscious” knowledge which I would prefer to call “absolute knowledge”. It is not cognition but, as Leibnitz so excellently calls it, a “perceiving” which consists – or to be more cautious, seems to consist – of images, of subjectless “simulacra”. These postulated images are presumably the same as my archetypes, which can be shown to be formal factors in spontaneous fantasy products. Expressed in modern language, the microcosm which contains “the images of all creation” would be the collective unconscious. Agrippa says of this: “That which we call the quintessence: because it is not from the four Elements, but a certain fifth thing, having its being above, and besides them”.

Chomsky, Language and nature (1995).
>As for the matter of cognitive reach, if humans are part of the natural world, not supernatural beings, then human intelligence has its scope and limits, determined by initial design. We can thus anticipate that certain questions will not fall within their cognitive reach, just as rats are unable to run mazes with numerical properties, lacking the appropriate concepts. Such questions we might call “mysteries-for-humans”, just as some questions we raise, and others we do not know how to formulate properly or at all.

Biaggi, Nihilism (1998).
>A sentence from Valéry reminds us that there are words with value more than meaning. The preliminary examination of the word nihilism should plunge us into abysses of perplexity: how to succeed in fixing the meaning of a term whose immediate horizon denies any meaning to reality and whose etymology, by the very presence of nihil (“nothing” in Latin), returns us to a bottomless thought, radically negative, which, beyond doubt, proclaims with constancy and despair a refusal of everything, an adhesion to nothing? This thought of emptiness is perhaps the sign of an emptiness of thought (to think about nothing would already be to think nothing, from the Sophists) or the expression of a wish, that of Mephistopheles: “it would be better than nothing existed”, “because all that exists is worthy of destruction” (Goethe, Faust). […] Hugo already foresaw the effects of this “metaphysical school of the north, a little imbued with fog” whose “negation of the infinite leads straight to nihilism” and, at the same time, to solipsism: “With nihilism no discussion possible. Because the logical nihilist doubts that his interlocutor exists, and is not sure to exist himself. From his point of view, it is possible that he is for himself only a ‘conception of his mind’ ”(Les Misérables).


### From philosophy to natural science <a name="p24"></a>

Berkeley influenced thought in the physical sciences. He believed that matter was a collection of sensations existing only in the mind. He proposed that “there can be any motion other than relative” and influenced Mach. Mach made major contributions to physics, philosophy, and physiological psychology. He is known for his anti-realist stance in opposition to atomism and in general for his positivist-empiricist approach to epistemology. In physics, the speed of sound bears his name, as he was the first to systematically study super-sonic motion. Mach adopted the philosophy of Berkeley that we should find natural laws based on the connection between sensations. His critique of Newtonian ideas of absolute space and time, that he believed to be sensations, were an inspiration to Einstein (who did not embrace idealism as Berkeley did), who credited Mach as being the philosophical forerunner of relativity theory. His systematic skepticism of the old physics was important to a generation of young German physicists.

Mach, The analysis of sensations (1897).
>The physiology of the senses … demonstrates, that spaces and times may just as appropriately be called sensations as colors and sounds. […] The question which is often asked, whether the world is real or whether we merely dream it, is devoid of all scientific meaning. […] The distinction between dream and waking, between appearance and reality, is quite otiose and worthless. […] For us, therefore, the world does not consist of mysterious entities, which by their interaction with another, equally mysterious entity, the ego, produce sensations, which alone are accessible. For us, colors, sounds, spaces, times, … are provisionally the ultimate elements, whose given connection it is our business to investigate. […] The plain man is familiar with blindness and deafness, and knows from his everyday experience that the look of things is influenced by his senses; but it never occurs to him to regard the whole world as the creation of his senses. He would find an idealistic system, or such a monstrosity as solipsism, intolerable in practice.

McTaggart shares with Mach an idealistic strain of positivism in denying the objective reality of time. He endorsed both ontological idealism and epistemological realism. Epistemological realism, as formulated by himself, is the view that knowledge is true (justified) belief, and that truth consists in correspondence with reality. McTaggart’s version of ontological idealism was inspired by his reading of Hegel.	

McTaggart, The nature of existence (1927).
>Absolute reality is timeless. […] This makes the value of absolute reality infinite in amount.

Moore was one of the trinity of philosophers at Trinity College Cambridge (the others were Russell and Wittgenstein) who made Cambridge one of centres of what we now call ‘analytical philosophy’. Moore was first drawn to philosophy through contact with McTaggart and under McTaggart’s influence he fell briefly under the spell of British idealism, especially the work of Bradley. Later he rejected idealism from McTaggart and Bradley, in addition to Kant’s conception of the a priori as a muddled form of subjectivism or psychologism.
	
Moore, The refutation of idealism (1903).
>When any Idealist thinks he is aware of himself or of any one else, this cannot really be the case. […] All that can be said is that there is an awareness in him, with a certain content: it can never be true that there is in him a consciousness of anything. And similarly he is never aware either of the fact that he exists or that reality is spiritual. The real fact, which he describes in those terms, is that his existence and the spirituality of reality are contents of an awareness, which is aware of nothing – certainly not, then, of its own content. […] What reason have we for supposing that anything exists corresponding to our sensations? but: What reason have we for supposing that material things do not exist, since their existence has precisely the same evidence as that of our sensations? That either exist may be false; but if it is a reason for doubting the existence of matter, that it is an inseparable aspect of our experience, the same reasoning will prove conclusively that our experience does not exist either, since that must also be an inseparable aspect of our experience of it. […] Reality may be spiritual, for all I know; and I devoutly hope it is. […] It is only with Idealistic arguments that I am concerned. […] Idealists have no reason whatever for their conclusion.

Jung influenced one philosophical interpretation (not the science) of quantum physics with the concept of synchronicity regarding some events as non-causal. That idea influenced the physicist Pauli (with whom, via a letter correspondence, he developed the notion of unus mundus (Latin: “one world”) in connection with the notion of nonlocality) and some other physicists.

Jung, Synchronicity – An acausal connecting principle (1952).
>For the unconscious psyche space and time seem to be relative; that is to say, knowledge finds itself in a space-time continuum in which space is no longer space, nor time time. If, therefore, the unconscious should develop or maintain a potential in the direction of consciousness, it is then possible for parallel events to be perceived or “known”. […] We have absolutely no scientific means of proving the existence of an objective meaning which is not just a psychic product. […] For those who are interested in psychology I should like to mention here that the peculiar idea of a self-subsistent meaning is suggested in dreams. […] These dreams seem to point to the presence of a formal factor in nature. They describe not just a lusus naturae, but the meaningful coincidence of an absolutely natural product with a human idea apparently independent of it. This is what the dreams are obviously saying, and what they are trying to bring nearer to consciousness through repetition.

McTaggart’s Hegelianism was also important for the development of other philosophers such as Bertrand Russell, whose early work was inspired by the idealism defended by McTaggart. He defended neutral monism, the view that the world consists of just one type of substance which is neither exclusively mental nor exclusively physical.

Russell, An inquiry into meaning and truth (1940).
>We all start from “naive realism”, i.e., the doctrine that things are what they seem. We think that grass is green, that stones are hard, and that snow is cold. But physics assures us that the greenness of grass, the hardness of stones, and the coldness of snow, are not the greenness, hardness, and coldness that we know in our own experience, but something very different. The observer, when he seems to himself to be observing a stone, is really, if physics is to be believed, observing the effects of the stone upon himself. Thus science seems to be at war with itself: when it most means to be objective, it finds itself plunged into subjectivity against its will. Naive realism leads to physics, and physics, if true, shows that naive realism is false. Therefore naive realism, if true, is false; therefore it is false.

Realism, for Einstein, is not a philosophical doctrine about the interpretation of scientific theories. It is a physical postulate. He developed the thesis of spatial separability, the claim that spatial separation is a sufficient condition for the individuation of physical systems, and its assumption is here made into almost a necessary condition for the possibility of an intelligible science of physics.

Einstein, The world as I see it (1934).
>Hume, by his lucid criticism, allows a decisive progress in philosophy. But it causes, without responsibility on his side, a real danger, because this criticism gives rise to an erroneous “fear of metaphysics”, highlighting a defect in contemporary empirical philosophy. This vice corresponds to the other extreme of the cloudy philosophy of antiquity when it believed it could dispense with sensitive data, or even despise it. Despite my admiration for Russell’s insightful analysis in Meaning and Truth, I fear that there too the specter of metaphysical fear has caused some damage. […] I do not see any “metaphysical” danger in welcoming the object (object in the sense of physics) as an independent concept in the system linked to the space-time structure belonging to it. […] The creative principle resides in mathematics. In a certain sense, therefore I hold it true that pure thought can grasp reality, as the ancients dreamed.

Pauli attempted to ground theoretical physics in positivism (empiricism). He then began instead to trust his intuitive visualizations of entities that formed an underlying reality to the sensible physical world. These visualizations included holistic kernels of mathematical-physical entities that later became for him synonymous with Jung’s mandalas.

[Wolfgang Pauli](/references/wolfgangpauli.html)
>In contrast to the purely empirical conception according to which natural laws can with virtual certainty, be derived from the material of experience alone, many physicists have recently emphasized anew the fact that intuition and the direction of attention play a considerable role in the development of the concepts and ideas, generally far transcending mere experience, that are necessary for the erection of a system of natural laws (that is, scientific theory). From the standpoint of this not purely empiristic conception, which we also accept, there arises the question, What is the nature of the bridge between the sense perceptions and the concepts? All logical thinkers have arrived at the conclusion that pure logic is fundamentally incapable of constructing such a link. It seems most satisfactory to introduce at this point the postulate of a cosmic order independent of our choice and distinct from the world of phenomena. Whether one speaks of the “participation of natural things in ideas” or of a “behaviour of metaphysical things – those, that is, which are in themselves real”, the relation between sense perception and idea remains predicated upon the fact that both the soul of the perceiver and that which is recognized by perception are subject to an order thought to be objective.

### In natural science <a name="p25"></a>

ADD PICTURE ADD PICTURE ADD PICTURE AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

Einstein describes the basis of natural science as “believing in an outside world independent of the subject who perceives it”. He acknowledges that only the speculative way can help to understand the world since “the perceptions of the senses offer only indirect results on this external world or on physical reality”. As a result “our conceptions of physical reality only offer momentary solutions”. This can be seen through any history of natural science such as Newton’s theory corrected by Maxwell’s theory and then by quantum theory. 
Johannes Kepler (1571–1630, German) is one of the most significant representatives of the Scientific Revolution of the 16th century. He was a Platonist for giving priority to the role of geometry in the structure of the world and an Aristotelian for accentuating the role of experience and causality in epistemology. He wanted to investigate the innermost structure of the cosmos.

Kepler
>[Harmonices mundi (1619)] All ideas or formal concepts of the harmonies, as I have just discussed them, lie in those beings that possess the faculty of rational cognition, and they are not at all received within by discursive reasoning; rather they are derived from a natural instinct and are inborn in those beings as the number (an intellectual thing) of petals in a flower or the number of seed cells in a fruit is innate in the forms of the plants.
>
>[Ad Vitellionem paralipomena (1604)] Therefore the bodies themselves which, as such, are confined by the limits of their surfaces and are thus unable to expand into spherical form, are endowed with various powers, nesting in the bodies, which are somewhat freer than the bodies themselves, possessing no corporeal matter but consisting of a particular matter of their own that assumes geometrical dimensions, and which powers flow out from them and aspire to the circular form – as can be clearly seen, especially, in the magnet but also in many other things. Is it any wonder, then, if that principle of all beauty in the world, which the divine Moses introduces into scarcely created matter, even on the first day of creation, as (so to speak) the Creator’s instrument, by which to give visible shape and life to all things – is it any wonder, I say, if this primary principle and this most beautiful being in the whole corporeal world, the matrix of all animal faculties, and the bond between the physical and the intellectual world, submitted to those very laws by which the world was to be formed?
>
>[Tertius interveniens (1610)] For it is by no means to be pronounced foolishness that man is naturali necessitate diversified and qualified in accordance with the Configurationibus stellarum; this might really far rather be called an “influence” of the nature of man into the star (as of fluid plaster into a mould) than, on the contrary, an “influence” of the star into man.
>
>[Explained by Pauli in The influence of archetypal ideas on the scientific theories of Kepler] The process of understanding nature as well as the happiness that man feels in understanding, that is, in the conscious realization of new knowledge, seems thus to be based on a correspondence, a “matching” of inner images pre-existent in the human psyche with external objects and their behaviour. This interpretation of scientific knowledge, of course, goes back to Plato and is, as we shall see, very clearly advocated by Kepler. He speaks in fact of ideas that are pre-existent in the mind of God and were implanted in the soul, the image of God, at the time of creation. These primary images which the soul can perceive with the aid of an innate “instinct” are called by Kepler archetypal (“archetypalis”). Their agreement with the “primordial images” or archetypes introduced into modern psychology by C. G. Jung and functioning as “instincts of imagination” is very extensive. […] In Kepler’s view the planets are still living entities endowed with individual souls. Since the earth had lost its unique position among the planets he had to postulate also an anima terrae. We shall see how the souls of the heavenly bodies play an essential role in Kepler’s particular views on astrology. Yet the de-animation of the physical world had already begun to operate in Kepler’s thought. He does, to be sure, occasionally mention the alchemical world-soul, the anima mundi, that sleeps in matter, is made responsible for the origin of a new star, and is said to have its seat – that is, its special concentration – in the sun. But it can be seen clearly that the anima mundi is no more than a kind of relic in Kepler’s mind and plays a subordinate role compared to the individual souls of the various heavenly bodies. […] From within an inner centre the psyche seems to move outward, in the sense of an extraversion, into the physical world in which, by definition, everything that occurs is automatic; so that the mind, itself in a state of rest, embraces this physical world, as it were, with its ideas.

Kepler’s works provided one of the foundations for Newton’s theory of universal gravitation. Isaac Newton (1642–1727, English) founded classical mechanics on the view that space is distinct from body and that time passes uniformly without regard to whether anything happens in the world. For this reason he spoke of absolute space and absolute time, so as to distinguish these entities from the various ways by which we measure them. Associated with these issues about the ontological status of space and time was the question of the nature of true motion. Newton defined the true motion of a body to be its motion through absolute space. He did not regard space and time as genuine substances (as are, paradigmatically, bodies and minds), but rather as real entities with their own manner of existence as necessitated by God’s existence (more specifically, his omnipresence and eternality).

Newton
>[Explained by Einstein in The world as I see it (1934)] First Newton’s system. Physical reality is characterized by the concepts of space, time, material points, force (the equivalence of action between material points). According to Newton, physical phenomena must be interpreted as movements of material points in space, movements governed by laws. The material point is the exclusive representative of reality, whatever the versatility of nature. […] This theoretical system, in its fundamental structure, presents itself as an atomic and mechanical system. So therefore all phenomena must be conceived from the mechanical point of view, that is to say simple movements of material points subject to Newton’s law of motion. […] Let us take the major difficulty: it resides essentially in the theory of light, because Newton, in full agreement with his system also conceives it as consisting of material points. Already at the time, the formidable question arose: where have the material points of light gone, when it is absorbed?
>
>[Explained by Pauli in The influence of archetypal ideas on the scientific theories of Kepler] In Newton the ideas of absolute space and absolute time entered even into his theological views.

Einstein, when he visited the University of Cambridge in 1922, was told by his host that he had done great things because he stood on Newton’s shoulders; Einstein replied: “No I don’t. I stand on the shoulders of Maxwell”. Indeed, Maxwell’s equations for electromagnetism have been called the “second great unification in physics” after the first one realised by Newton. Maxwell demonstrated that electric and magnetic fields travel through space as waves moving at the speed of light. He formulated the classical theory of electromagnetic radiation, bringing together for the first time electricity, magnetism, and light as different manifestations of the same phenomenon.

Maxwell, explained by Einstein in The world as I see it (1934)
>Before Maxwell I conceive of physical reality – that is to say, I represent the phenomena of nature in this way – as a set of material points. When there is a change, the partial differential equations describe and regulate the movements. After him, I conceive of physical reality as represented by continuous fields, which cannot be explained mechanically but which are regulated by partial differential equations. This modification of the conception of reality represents the most radical and fruitful revolution in physics since Newton.

Einstein, The world as I see it (1934).
>Finally the last born of theoretical physics is called quantum mechanics. It knows a great success but, in principle, it rejects in its basic structure the two programs, those which we designate for reasons of convenience under the names of Newton program and Maxwell program.

The revolution in physics that brought us to a quantum picture of the world was so radical that it does not merely force a rethinking of physics, but metaphysics as well. Quantum physics may imply that the world is fundamentally indeterministic, that it is fundamentally indeterminate, that causes are not always local to their effects, that there may be many more than three spatial dimensions, that wholes are not simply sums of their parts, that our world is just one among many, etc. Quantum mechanics is metaphysically revisionary even if it is not clear what form the revisions should take. The reason it is so hard to say what the implications of quantum mechanics are for metaphysics is that physicists and philosophers have developed a variety of alternative theories that all attempt to explain why quantum experiments turn out the way they do. These theories disagree on what exists and what rules that stuff obeys.

As David Mermin (1935-, American physicist) pointed out in 2012, “quantum theory is the most useful and powerful theory physicists have ever devised. Yet today, nearly 90 years after its formulation, disagreement about the meaning of the theory is stronger than ever. New interpretations appear every day. None ever disappear”. Interpretations are essentially of two types:
- Those that view quantum probabilities of measurement outcomes as determined by intrinsic properties:
	- Ontic interpretations view the quantum state as an intrinsic property of the observed system.
	- Epistemic interpretations view the quantum state as representing knowledge of an underlying objective reality in a sense somewhat analogous to that in which a state in classical statistical mechanics assigns a probability distribution to points in phase space. 
- Those that do not deny the existence of an objective world but, according to them, quantum theory does not deal directly with intrinsic properties of the observed system, but with the experiences an observer or agent has of the observed system.
	- “About knowledge” interpretations view the quantum state as an observer’s knowledge about the results of future experiments.
	- “About belief” interpretations view the quantum state as an agent’s expectations about the results of future actions. This category only concerns quantum Bayesianism (QBism) and won’t be discussed.

ADD TABLE/PICTURE
ADD pICTURE
AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

Maudlin, Space-time in the quantum world (1996).
>Realism in philosophy of science is generally contrasted with instrumentalism or empiricism, which views assert that one can have no grounds to believe that the unobservable ontology of a theory is accurate. In this sense, theories are neither realistic nor non-realistic, only interpretations of (or better: attitudes towards) theories.

Einstein and Bohr are known for having contrasting views.

Einstein, The world as I see it (1934).
>Finally the last born of theoretical physics is called quantum mechanics. It knows a great success but, in principle, it rejects in its basic structure the two programs, those which we designate for reasons of convenience under the names of Newton program and Maxwell program. Indeed the magnitudes represented in its laws do not pretend to represent reality itself, but only the probabilities of existence of an engaged physical reality. In my opinion Dirac has exposed, as admirably as possible, the logical order of this theory. He rightly observes that it would be almost illusory to describe a photon theoretically. […] Deep down, I am firmly convinced that physicists will not be content for long with such an insufficient description of reality, even if in a logically acceptable way we managed to formulate the theory, in accordance with the postulate of general relativity. So we must tentatively be satisfied with Maxwell’s program … we must try to describe physical reality by fields satisfying the partial differential equations rigorously excluding any singularity.

Bohr
>[In a discussion quoted by Heisenberg in Physics and beyond (1958)] I feel very much like Dirac: the idea of a personal God is foreign to me. But we ought to remember that religion uses language in quite a different way from science. The language of religion is more closely related to the language of poetry than to the language of science. True, we are inclined to think that science deals with information about objective facts, and poetry with subjective feelings. Hence we conclude that if religion does indeed deal with objective truths, it ought to adopt the same criteria of truth as science. But I myself find the division of the world into an objective and a subjective side much too arbitrary. The fact that religions through the ages have spoken in images, parables, and paradoxes means simply that there are no other way of grasping the reality to which they refer. But that does not mean that it is not a genuine reality. And splitting this reality into an objective and subjective side won’t get us very far.
>
>[Explained by Bell in Speakable and unspeakable in quantum mechanics (1987)] Bohr once declared when asked whether the quantum mechanical algorithm could be considered as somehow mirroring an underlying quantum reality:
There is no quantum world. There is only an abstract quantum mechanical description. It is wrong to think that the task of physics is to find out how Nature is. Physics concerns what we can say about Nature.
>
>[Explained by Cushing in The causal quantum theory program (1996)] In 1966, an influential history of quantum physics sketched the philosophical currents present when quantum theory was formulated in the first quarter of this century (e.g., those I referred to in the preceding paragraph) and intimated a significant influence of Høffding’s teaching on Bohr’s own work in the foundations of quantum mechanics (Jammer 1989). Subsequent research has substantiated the impact of Høffding’s philosophy on Bohr (Faye 1991). Not only did Bohr attend Høffding’s lectures as a student, but he also read his works and corresponded with him later in life. One of Høffding’s tenets was that in life decisive events proceed through sudden “jerks” or discontinuities. Bohr explicitly acknowledged the influence of Høffding’s philosophy on his own formulation of complementarity.

Bohmian mechanics, which is also called the de Broglie-Bohm theory, the pilot-wave model, and the causal interpretation of quantum mechanics, is a version of quantum theory discovered by Louis de Broglie in 1927 and rediscovered by David Bohm in 1952. It is the simplest example of what is often called a “hidden variables” interpretation of quantum mechanics. All the physics notion necessary to understand the following will be vastly described in later chapters. Yet, to put it shortly, in Bohmian mechanics a system of particles is described in part by its wave function, evolving, as with other classic interpretations, according to Schrödinger’s equation. However, the wave function provides only a partial description of the system. This description is completed by the specification of the actual positions of the particles. The latter evolve according to the “guiding equation”, which expresses the velocities of the particles in terms of the wave function. Thus, in Bohmian mechanics the configuration of a system of particles evolves via a deterministic motion choreographed by the wave function.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory (1993).
>To understand what is meant by a quantum state we can begin with Dirac’s notion that each physical quantity is represented by a Hermitian operation which is called an observable. When this is measured by a suitable apparatus the system is left with a wave function corresponding to an eigenfunction of this observable. In general such a measurement will, in agreement with Heisenberg’s principle, alter this wave function in an uncontrollable and unpredictable way. […] Once we obtain such an eigenfunction we can measure the same observable again and again, in principle, in a time so short that the wave function does not change significantly (except for a phase factor which is not relevant). Each measurement will then reproduce the same result. In terms of the ‘naive’ ontology that pervades ordinary experience, this leads one to suppose that, between measurements of the same observable, the system continues to exist with the same wave function (again, except for a phase factor). Therefore one could say that during this time the system is in a certain state of being, i.e. it stands independently of its being observed. Of course, this state might change in longer times of its own accord and, in addition, it would also change if a different observable were measured. In contrast, Bohr would never allow the type of language that admitted the independent existence of any kind of quantum object which could be said to be in a certain state. That is to say, he would not regard it as meaningful to talk about, for example, a particle existing between quantum measurements even if the same results were obtained for a given observable in a sequence of such measurements. Rather, as we have seen, he considered the experimental arrangement and the content (meaning) of the result to be a single unanalyzable whole. To talk of a state in abstraction from such an experimental arrangement would, for Bohr, make no sense.

The “Copenhagen interpretation” is an expression of the meaning of quantum mechanics that was largely devised from 1925 to 1927 by Niels Bohr and Werner Heisenberg. It is one of the oldest of numerous proposed interpretations of quantum mechanics, and remains one of the most commonly taught. Yet, Bohr’s interpretation of complementarity (objects have certain pairs of complementary properties which cannot all be observed or measured simultaneously) and the textbook Copenhagen interpretation (i.e. wave-particle duality) are incompatible. There are indeed different versions of Copenhagen interpretations based on statements from some of the main characters. On one side of the spectrum there is Bohr who did not think of quantum measurement in terms of a collapse of the wave function (the act of measurement affects the system, causing the set of probabilities to reduce to only one of the possible values immediately after the measurement); in the middle we find Heisenberg talking about the collapse as an objective physical process but thinking that this couldn’t be analyzed any further because of its indeterministic nature, and at the opposite side Johann von Neumann (1903-1957, Hungarian) and Eugene Wigner argued that the human mind has a direct influence on this collapse. Unfortunately, von Neumann’s dualistic view has become part of the Copenhagen methodology by people opposing this interpretation.

The idea of von Neumann and Wigner can be explained by a theoretic strong anthropic principle. According to this idea, observations of the universe must be compatible with the conscious and sapient life that observes it. Proponents of the anthropic principle reason that it explains why this universe has the age and the fundamental physical constants necessary to accommodate conscious life.
Wheeler speculated that reality is created by observers in the universe. He also coined the term “participatory anthropic principle”, a version of a strong anthropic principle. For Wheeler, information is the fundamental reality and he proposed an “it from bit” doctrine: information (“bit”) sits at the core of physics, and every “it”, whether a particle or field, derives its existence from observations.

Wheeler, Geons, black holes, and quantum foam: a life in physics (1998).
>By whatever higgledy-piggledy chance, space arose, time arose, laws of physics arose. Perhaps, as with the origin of life on Earth, there were limits to what might arise. It wasn’t “anything goes”. But it also wasn’t “only one thing goes”. And, just as life arose from nonlife on Earth, something arose from nothing in the universe. That “nothing” from which something arose should not, however, be confused with the emptiness of a vacuum. It is nothing in a profounder sense. It is nothingness. Why is the universe what it is? What other way might it have been? These are questions that have not been answered, indeed scarcely addressed.

It was in 1961 that Wigner suggested that a conscious observer plays a fundamental role in quantum mechanics. His paper would serve as inspiration for later mystical works by others but Wigner’s ideas were primarily philosophical and are not considered “in the same ballpark” as the mysticism that would follow. An example of such misuse is the New Age speaker Deepak Chopra’s “quantum theory” that asserts, by using physicist interpretations, that aging is caused by the mind. For this misappropriation, Chopra was awarded the parody Ig Nobel Prize in the physics category for “his unique interpretation of quantum physics as it applies to life, liberty, and the pursuit of economic happiness”. However, some physicists also had controversial and badly-perceived views. It should be noted that Schrödinger had a lifelong interest in the Vedanta philosophy of Hinduism, which influenced his speculations about the possibility that individual consciousness is only a manifestation of a unitary consciousness pervading the universe. Bohm had extensive interactions with the psychological philosopher Jiddu Krishnamurti (1895-1986, Indian) on a similar subject. Einstein and Planck were deists while Pauli was considered to have been a deist and a mystic, due to his views on synchronicity. 

Pauli
>[The influence of archetypal ideas on the scientific theories of Kepler (1955)] It is certain that modern physics has generalized the old confrontation of the apprehending subject with the apprehended object into the idea of a cleavage or division that exists between the observer or the means of observation, on the one hand, and the system observed, on the other. While the existence of such a division is a necessary condition of human cognition, modern physics holds that its placement is, to a certain extent, arbitrary and results from a choice co-determined by considerations of expediency and hence partially free. Furthermore, whereas older philosophical systems have located the psychical on the subjective side of the division, that is, on the side of the apprehending subject, and the material on the other side – the side of that which is objectively observed – the modern point of view is more liberal in this respect: microphysics shows that the means of observation can also consist of apparatuses that register automatically; modern psychology proves that there is on the side of that which is observed introspectively an unconscious psyche of considerable objective reality. Thereby the presumed objective order of nature is, on the one hand, relativized with respect to the no less indispensable means of observation outside the observed system; and, on the other, placed beyond the distinction of “physical” and “psychical”.
>
>[Explained by Cushing in The causal quantum theory program (1996)] Pauli had been one of the harshest critics of de Broglie’s pilot-wave theory (of which Bohm’s is a lineal descendant) at the 1927 Solvay Congress and he maintained the same attitude toward Bohm’s theory. […] Any dream of a way back to the classical style of Newton and Maxwell seemed to him both hopeless and in bad taste. […] His letter to Fierz in 1952 was quite sarcastic and contemptuous of Bohm’s work. Fierz has offered the view that de Broglie was a Catholic and Bohm a Communist and that, to Pauli, this was the black and red alliance. […] For Pauli more was at stake than just physics. Whereas materialism considers the spiritual as a mere byproduct of the material, Pauli especially understood the “epistemological lesson” of atomic physics as a signpost pointing in another direction. He, like Heisenberg, thought that quantum theory had clearly shown materialism to be unsatisfactory. Pauli was quite explicit in holding that it is wrong to imagine the existence of the material external world without including the observer and his psyche in that world view. The ideas of a detached observer and of realism were to be abandoned. He saw the Copenhagen interpretation as not being compatible with a materialistic world view. On the downside for the causal program from Pauli’s perspective was the fact that psychological problems associated with observations were quite neglected by Bohm, as were the unconscious in his causal-deterministic universe. As a result, Pauli felt, Bohm saw no quantum paradoxes, but at the price of no free will either. He wanted no part of a deterministic, clock-work universe and believed that the trend of Western culture since the seventeenth century had been dangerous. The “irrational influences” that statistical causality implied were important for him. Pauli’s concerns spilled over into the theological realm as well. There were deep philosophical, moral and religious issues involved here. An attack on the canonical version of quantum theory resonated quite negatively and strongly with Pauli’s beliefs about God and his views on epistemology. It was inconceivable to him that anything like a return to a “classical” world with causality and picturable, continuous processes in space-time was either possible or anything less than a disgusting loss of nerve and a return to darkness (Laurikainen 1988).

Albert, Elementary quantum metaphysics (1996).
>Once upon a time, the twentieth-century investigations of the behaviors of sub-atomic particles were thought to have established that there can be no such thing as an objective, observer-independent, scientifically realist, empirically adequate picture of the physical world. And it was part and parcel of thinking things like that, it was (you might even say) the essence of thinking things like that, that one looked at quantum-mechanical wave functions not as representing physical objects directly, but (say) as representing what observers know of such objects, or as representing imaginary ensembles of such objects, or as representing the probabilities of the outcomes of measurements on such objects, or something like that. And it has consequently been essential to the project of digging one’s way out of those sorts of confusions, it has been essential (that is) to the project of quantum-mechanical realism (in whatever particular form it takes – Bohm’s theory, or modal theories, or Everettish theories, or theories of spontaneous localization), to learn to think of wave functions as physical objects in and of themselves. […] Insofar as we are committed to realism, there was simply never anything other than physical objects that wave functions could have been. And of course the space those sorts of objects live in, and (therefore) the space we live in, the space in which any realistic understanding of quantum mechanics is necessarily going to depict the history of the world as playing itself out (if space is the right name for it) is configuration-space. And whatever impression we have to the contrary (whatever impression we have, say, of living in a three-dimensional space, or in a four-dimensional space-time) is somehow flatly illusory. I learned all this (insofar as I can reconstruct it now) from a few casual remarks scattered here and there in various papers and private communications of John Bell. And it has seemed so straightforward and so ineluctable to me since then as not to merit any further discussion. But it turns out not to have seemed that way to everybody. It turns out (as a matter of fact) that this sort of talk still frequently manages to surprise people, even to appall them.

## Causality and determinism <a name="p3"></a>

### Introduction <a name="p31"></a>

Determinism is the philosophical belief that all events are determined completely by previously existing causes. The opposite of determinism is some kind of indeterminism or randomness. Determinism is often contrasted with free will. Determinism is often taken to mean causal determinism, which in physics is known as cause-and-effect. As Jung put it: “The causality principle asserts that the connection between cause and effect is a necessary one”.

Causal determinism can be viewed as the idea that every event is necessitated by antecedent events and conditions together with the laws of nature. This notion is ancient, but became subject to clarification and mathematical analysis in the eighteenth century. For example, Jung said that “in Leibnitz, causality is neither the only view nor the predominant one”. Determinism is deeply connected with our understanding of the physical sciences and their explanatory ambitions, on the one hand, and with our views about human free action on the other. Philosophers and physicists have debated both the truth of determinism, and the truth of free will, which led Heisenberg to say that “The fact that atomic processes cannot be fully determined is often used as an argument in favor of free will and divine intervention”.

Quantum mechanics is widely thought to be a strongly non-deterministic theory. Popular belief (even among most physicists) holds that many phenomena can only be given a probabilistic description. The theory does not say what happens in a given case, but only says what the probabilities of various results are. Ironically, quantum mechanics is one of the best prospects for a genuinely deterministic theory in modern times. Everything hinges on what interpretational and philosophical decisions one adopts. A survey of determinism’s status in some prominent physical theories, does not really tell us anything about whether determinism is true of our world. Instead, it raises a couple of further disturbing possibilities for when we reach a final theory. We may have difficulty establishing whether this theory is deterministic or not, depending on whether the theory comes loaded with unsolved interpretational or mathematical puzzles. We may also have reason to worry that this final theory, if indeterministic, has an empirically equivalent yet deterministic rival.

Four separate views on determinism are acknowledged. Compatibilism refers to the view that free will is, in some sense, compatible with determinism. The three incompatibilist positions, on the other hand, deny this possibility. The hard incompatibilists hold that both determinism and free will do not exist, the libertarianists that determinism does not hold, and free will might exist, and the hard determinists that determinism does hold and free will does not exist.

### Metaphysical considerations <a name="p32"></a>

The idea that the entire universe is a deterministic system has been articulated in both Eastern and non-Eastern religion, philosophy, and literature. The concept of Karma deals with similar philosophical issues to the western concept of determinism. Karma is understood as a spiritual mechanism which causes the entire cycle of rebirth. Karma, either positive or negative, accumulates according to an individual’s actions throughout their life, and at their death determines the nature of their next life in the cycle of lives. Most major religions originating in India hold this belief to some degree. Buddhist philosophy contains several concepts which some scholars describe as deterministic to various levels. McTaggart expresses similar karmic ideas.

Kamalasila
>[Stages of meditation (IXth CE)] Omniscience is … rare because it does not happen all the time and in all places, and … not everything can become omniscient. As a result, it depends definitively on causes and conditions.
>
>[Comments by the current Dalai Lama (2001)] Temporary things depend on causes. […] Omniscient transcendent wisdom refers to the consciousness that knows all things. Omniscience is not a quality that is found in the ground, stones, rocks or mountains. […] Consciousness is … that which has the function of knowing. […] Efficient phenomena can be produced at certain times and not at others. At a certain moment, when favorable conditions occur and adverse conditions are absent, a consciousness can be transformed into omniscience, that is to say, into knowledge of all phenomena. […] One of the postulates of Buddhist causation is that an effect can only register in the order of serial continuity with its cause. In other words, the cause and its effect must be of the same nature (from a grain of rice is born a rice sprout, not barley). Likewise, from an inanimate cannot be born an animate such as consciousness. […] It is said of the cycle of existence that it has no beginning. […] The cycle of existence is by no means a product of the will of Isvara (the “Mighty Lord”, in this case the god Brahma of the Hindus. However the remark applies to any form of religious idea of a God creator of the universe), which some believe is the Creator.

McTaggart, The nature of existence (1900).
>Now we cannot doubt that a character may remain determined by an event which has been forgotten. I have forgotten most of the good and evil acts which I have done in my present life. And yet each has left a trace on my character. And so a man may carry over into the next life the dispositions and tendencies which he has gained by the moral contests of this life, and the value of these experiences will not have been destroyed by the death which has destroyed the memory of them.

Some of Descartes’ followers adopted an occasionalist position, according to which God mediates the causal relations between mind and body; mind does not affect body, and body does not affect mind, but God gives the mind appropriate sensations at the right moment, and he makes the body move by putting it into the correct brain states at a moment that corresponds to the volition, for example, to pick up a pencil. 

Descartes, explained by Bohr in Atomic physics and human knowledge (1957).
>In this so-called classical mechanics all reference to purpose is eliminated, since the course of events is described as automatic consequences of given initial conditions. […] On the philosophical side, it was especially Descartes who stressed the similarity between animals and automata, but ascribed to human beings a soul interacting with the body in a certain gland in the brain.

Spinoza was a determinist thinker, and argued that human freedom can be achieved through knowledge of the causes that determine our desire and affections. He defined human servitude as the state of bondage of the man who is aware of his own desires, but ignorant of the causes that determined him. On the other hand, the free or virtuous man becomes capable, through reason and knowledge, to be genuinely free, even as he is being “determined”.

Spinoza, explained by Misrahi in commentaries to Spinoza’s Ethics (2005).
>The very firm negation of free will therefore doesn’t prevent the progressive construction of the paths and conditions of possibility of true freedom, that is to say, action by adequate causation, based on adequate knowledge. The first three parts of the Ethics are intended to make possible the establishment of the “free man” (homo liber) and the knowledge of his style of existence. […] It is not the absence of free will which opposes freedom, it is the existence of “passions”: they alone, and not the determinism of nature in us and outside of us, produce the servitude. […] “God” and “Nature” are one and the same being: the world is one. And there is an infinite Nature without beginning, without end, without exteriority. It is absolute autonomy. This is what the idea of “causality by oneself” means: Nature (or, if you prefer, “God”) is the infinite being without will which is what it is and which is the only one reference to which it can be related. This Being “cause of itself” is not a personal conscience which would have the creative functions of a Father, the protective function of a Providence, the legal and moral function of a Judge or the directing function of a Monarch. Nothing of the sort. Being is being that is, and it is the very power of the laws that define it: the natural laws of determinism, precise and constant.

Friedrich Nietzsche (1844-1900, German) is known as a critic of Judeo-Christian morality and religions in general. One of the arguments he raised against the truthfulness of these doctrines is that they are based upon the concept of free will, which, in his opinion, does not exist. Nietzsche praised Arthur Schopenhauer’s “immortal doctrines of the intellectuality of intuition, the apriority of the law of causality, […] and the non-freedom of the will”.
	
Nietzsche
>[Thus spoke Zarathustra, explained by Goldschmidt (1983] It is no coincidence that Nietzsche alludes here to the philosophical problem of freedom which since Luther has never ceased to preoccupy German thought. The thought of Fichte, Schelling or Hegel was built around the concept of freedom. The idea of freedom (it is largely based on the Selbstbewusstein, “self-awareness”, what Jean-Jacques Rousseau called “feeling of existence”) comes up several times in Zarathoustra.
>
>[Thus spoke Zarathustra (1883)] Once on a time, Zarathustra also cast his fancy beyond man, like all backworldsmen. The work of a suffering and tortured God, did the world then seem to me. The dream – and diction – of a God, did the world then seem to me; coloured vapours before the eyes of a divinely dissatisfied one. Good and evil, and joy and woe, and I and thou – coloured vapours did they seem to me before creative eyes. The creator wished to look away from himself, – thereupon he created the world. Intoxicating joy is it for the sufferer to look away from his suffering and forget himself. Intoxicating joy and self-forgetting, did the world once seem to me. 

Like Spinoza, Einstein was a strict determinist who believed that human behavior was completely determined by causal laws. For that reason, he refused the chance aspect of quantum theory, famously telling Niels Bohr: “God does not play dice with the universe”. Einstein was also an incompatibilist. 

Einstein, The world as I see it (1934).
>I refuse to believe in free will. I am not free, but sometimes constrained by pressures foreign to me or sometimes by intimate convictions. Schopenhauer’s words: “Man can do what he wants, but he cannot will what he wills”, accompany me in all situations throughout my life and reconcile me with the actions of others, even if they are rather painful to me. This awareness of the lack of free will keeps me from taking myself and my fellow men too seriously as acting and deciding individuals, and from losing my temper. […] I cannot worry about the meaning or the purpose of my own existence or that of others, because, from a strictly objective point of view, it is absurd. And yet, as a man, certain ideals direct my actions and guide my judgments. […] They are called the good, the beautiful, the true.

### In natural science <a name="p33"></a>

Determinism in the West is often associated with Newtonian physics, which depicts the physical matter of the universe as operating according to a set of fixed, knowable laws. The “billiard ball” hypothesis, a product of Newtonian physics, argues that once the initial conditions of the universe have been established, the rest of the history of the universe follows inevitably. Pierre-Simon Laplace (1749-1827, French) published the first articulation of scientific determinism. According to determinism, if someone (the demon) knows the precise location and momentum of every atom in the universe, their past and future values for any given time are entailed; they can be calculated from the laws of classical mechanics. In other words, it would be theoretically possible to compute the time and place of every event that will ever occur. In this sense, the basic particles of the universe operate in the same fashion as the rolling balls on a billiard table, moving and striking each other in predictable ways to produce predictable results.

More recently, there is a tradition that stretches back at least as far as Russell (1913) that denies that there is any place for causal notions in the fundamental sciences. The argument goes that, since at least the nineteenth century, the laws that govern physical behavior in fundamental sciences such as physics are almost always differential equations. Such equations are notable for specifying, given some initial conditions, exact properties of systems for all time. And thus if everything is specified for all time, there is no place left for causality. Thus Russell advocates that “causality” should be eliminated from the philosophers lexicon, because it is certainly not a part of the scientific lexicon.

In this way, the original motivation in the early 20th century for relating quantum theory to consciousness was essentially philosophical. It is fairly plausible that conscious free decisions (“free will”) are problematic in a perfectly deterministic world, so quantum randomness might indeed open up novel possibilities for free will. On the other hand, randomness is problematic for goal-directed volition! Quantum theory introduced an element of randomness standing out against the previous deterministic worldview preceding it, in which randomness expresses our ignorance of a more detailed description. Synchronicities of Jung are the idea that meaningful connections in the world manifest through coincidence with no apparent causal link. What he referred to as “acausal connecting principle”.

Jung, Synchronicity – An acausal connecting principle (1952).
>It was modern psychology and parapsychology which proved that causality does not explain a certain class of events and that in this case we have to consider a formal factor, namely synchronicity, as a principle of explanation. […] Sir James Jeans reckons radioactive decay among the causeless events which, as we have seen, include synchronicity. He says: “Radioactive break-up appeared to be an effect without a cause, and suggested that the ultimate laws of nature were not even causal”. This highly paradoxical formula, coming from the pen of a physicist, is typical of the intellectual dilemma with which radioactive decay confronts us. It, or rather the phenomenon of “half-life”, appears as an instance of acausal orderedness. […] If natural law were an absolute truth, then of course there could not possibly be any processes that deviate from it. But since causality is a statistical truth, it holds good only on average and thus leaves room for exceptions which must somehow be experienceable, that is to say, real. I try to regard synchronistic events as acausal exceptions of this kind. They prove to be relatively independent of space and time; they relativize space and time in so far as space presents in principle no obstacle to their passage and the sequence of events in time is inverted, so that it looks as if an event which has not yet occurred were causing a perception in the present. But if space and time are relative, they causality too loses its validity, since the sequence of cause and effect is either relativized or abolished.

In the following are the ideas of some renowned scientists about determinism.

de Broglie, Une tentative d’interprétation causale et non linéaire de la mécanique ondulatoire (1956).
>There is determinism of the evolution of probabilities between two measures, but not determinism of the sequence of observable facts.

[Wolfgang Pauli](/references/wolfgangpauli.html)
>Every observation, therefore, interferes on an indeterminable scale both with the instruments of observation and with the system observed and interrupts the causal connection of the phenomena preceding it with those following it. This uncontrollable interaction between observer and system observed, taking place in every process of measurement, invalidates the deterministic conception of the phenomena assumed in classical physics: the series of events taking place according to pre-determined rules is interrupted, after a free choice has been made by the beholder between mutually exclusive experimental arrangements, by the selective observation which, as an essentially non-automatic occurrence, may be compared to a creation in the microcosm or even to a transmutation the results of which are, however, unpredictable and beyond human control.

Bohr
>[Atomic physics and human knowledge (1957)] According to the generalized interpretation of the psycho-physical parallelism, the freedom of the will is to be considered as a feature of conscious life which corresponds to functions of the organism that not only evade a causal mechanical description but resist even a physical analysis carried to the extent required for an unambiguous application of the statistical laws of atomic mechanics.
>
>[In a discussion quoted by Heisenberg in Physics and beyond (1969)] If we speak of free will, we refer to a situation in which we have to make decisions. This situation and the one in which we analyze the motives of our actions or even the one in which we study physiological processes, for instance the electrochemical processes in our brain, are mutually exclusive. In other words, they are complementary, so that the question whether natural laws determine events completely or only statistically has no direct bearing on the question of free will. Naturally, our different ways of looking at things must fit together in the long run, i.e., we must be able to recognize them as noncontradictory parts of the same reality, though we cannot yet tell precisely how. When we speak of divine intervention, we quite obviously do not refer to the scientific determination of an event, but to the meaningful connection between this event and others or human thought. Now this intellectual connection is as much a part of reality as scientific causality; it would be much too crude a simplification if we ascribed it exclusively to the subjective side of reality.

Born, Explained by Cushing in The causal quantum theory program (1996).
>Max Born weighs in with his own views on the role of causality in physics:
>
>>In quantum theory it is the principle of causality, or more accurately that of determinism, which must be dropped and replaced by something else…. We now have a new form of the law of causality…. It is as follows: if in a certain process the initial conditions are determined as accurately as the uncertainty relations permit, then the probabilities of all possible subsequent states are governed by exact laws…. No concealed parameters can be introduced with the help of which the indeterministic description could be transformed into a deterministic one. Hence if a future theory should be deterministic, it cannot be a modification of the present one but must be essentially different. How this could be possible without sacrificing a whole treasure of well-established results I leave to the determinists to worry about.
>
>Support for similar beliefs can also be found in John von Neumann’s work, while Albert Einstein and Erwin Schrödinger remained incredulous at such certitude on these matters. These positions that I have attributed to Bohr, Heisenberg and Born are not merely the products of a bygone era, but are echoed in modern textbooks on quantum mechanics.


Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>Since there is no final theory, it cannot be said that the universe is either ultimately deterministic or ultimately indeterministic. Therefore we cannot from physical theories alone draw any conclusions, for example, about the ultimate limits of human freedom. […] It is essential to look more carefully at … tacit identification of determinism with predictability and controllability. This identification is clearly characteristic of positivist philosophy. In this philosophy, science is not regarded as dealing with what is, so that concepts cannot be regarded as reflecting reality. Rather that have to be defined empirically, i.e. in relation to their manifestation in observation and experience. In such a philosophy it follows that determinism can have no meaning beyond predictability and controllability. Since the quantum theory was first formulated, the relationship of determinism to predictability and controllability has been clarified by the discovery that a very general class of deterministic systems (i.e. those having unstable and chaotic motions) are neither predictable nor controllable, as has been discussed in some detail by Penrose. Thus the identification of determinism with predictability and controllability has been invalidated. It follows that the mere uncontrollability and unpredictability of quantum phenomena does not necessarily imply that there can be no quantum world, which could in itself be determinate. […] It may be readily seen that this general approach implies a certain kind of limit to determinism. For in it, each theory is evidently an abstraction from a totality that is inherently unlimited, both qualitatively and quantitatively. As such, it evidently cannot be entirely self-determined, because it always depends in various ways on what has been left out. So the necessity of its self-determination is only relative and not absolute. Even here this self-determination may include unstable chaotic orders which approximate randomness. However, even the chaotic or stochastic character may be contingent and determined by conditions in domains not covered by the particular theory in question. For example, random variations in the trajectory of a Brownian particle may be partially or even totally determined by atomic motions at a deeper level. So ultimately our overall world view is neither absolutely deterministic nor absolutely indeterministic. Rather it implies that these two extremes are abstractions which constitute different views or aspects of the overall set of appearances. […] Relativity requires strict continuity, strict causality and strict locality in the order of the movement of particles and fields. And in essence quantum mechanics implies the opposite.

Bell, Speakable and unspeakable in quantum mechanics (1987).
>It has been argued that quantum mechanics is not locally causal and cannot be embedded in a locally causal theory. That conclusion depends on treating certain experimental parameters, typically the orientations of polarization filters, as free variables. Roughly speaking it is supposed that an experimenter is quite free to choose among the various possibilities offered by his equipment. But it might be that this apparent freedom is illusory. Perhaps experimental parameters and experimental results are both consequences, or partially so, of some common hidden mechanism. Then the apparent non-locality could be simulated. This possibility is the starting point of a paper by Clauser, Horne and Shimony (CHS hereafter), which is valuable in particular for a careful mathematical formulation of the assumption which excludes such a conspiracy. In this connection they severely criticize my own ‘theory of local beables’ (B hereafter). Much of their criticism is perfectly just. In B there were jumps in the argument, and the assumption in question was not stated at the appropriate place, but only later and inadequately. However, I do not agree with CHS that this assumption, when carefully formulated, is an unreasonable one. […] Here I would entertain the hypothesis that experimenters have free will. But according to CHS it would not be permissible for me to justify the assumption of free variables ‘by relying on a metaphysics which has not been proved and which may well be false’. Disgrace indeed, to be caught in a metaphysical position! But it seems to me that in this matter I am just pursuing my profession of theoretical physics. I would insist here on the distinction between analyzing various physical theories, on the one hand, and philosophizing about the unique real world on the other hand. In this matter of causality it is a great inconvenience that the real world is given to us once only. We cannot know what would have happened if something had been different. We cannot repeat an experiment changing just one variable; the hands of the clock will have moved, and the moons of Jupiter. Physical theories are more amenable in this respect. We can calculate the consequences of changing free elements in a theory, be they only initial conditions, and so can explore the causal structure of the theory. I insist that B is primarily an analysis of certain kinds of physical theory. […] It is remarkably difficult to get this point across, that determinism is not a presupposition of the analysis. There is a widespread and erroneous conviction that for Einstein determinism was always the sacred principle. The quotability of his famous ‘God does not play dice’ has not helped in this respect. Among those who had great difficulty in seeing Einstein’s position was Born. Pauli tried to help him in a letter of 1954: “… I was unable to recognize Einstein whenever you talked about him in either your letter or your manuscript. It seemed to me as if you had erected some dummy Einstein for yourself, which you then knocked down with great pomp. In particular Einstein does not consider the concept of ‘determinism’ to be as fundamental as it is frequently held to be (as he told me emphatically many times) … he disputes that he uses as a criterion for the admissibility of a theory the question: ‘Is it rigorously deterministic?’… he was not at all annoyed with you, but only said you were a person who will not listen”. […] Bohm showed explicitly how parameters could indeed be introduced, into nonrelativistic wave mechanics, with the help of which the indeterministic description could be transformed into a deterministic one. More importantly, in my opinion, the subjectivity of the orthodox version, the necessary reference to the ‘observer,’ could be eliminated.

American philosophers of science Arthur Fine and Mara Beller recently summed up some of these ideas and their implications. They acknowledge a possibility for determinism when looked up from the perspective of Bohm, and further developments of Bohmian mechanics.

Fine, On the interpretation of Bohmian mechanics (1996).
>Quantum mechanics is deterministic with respect to the evolution of the state function. Its indeterminism, if any, shows up only if we look at the values of the quantum observables. The problem here, however, is not that the initial values fail to determine the final values, or that they only do so stochastically. It is, rather, that we cannot specify the initial “state” of the system in the classical way, by giving a complete set of values for all the variables of state. In particular, there is no specification of simultaneous values for position and linear momentum. A standard formulation of determinism holds that for a closed system initial values for all the state variables determine final values. This formulation presupposes determinateness: it is possible to assign initial values to all the relevant variables at the same time. In the first instance, the failure of determinism in the quantum theory is the failure of determinateness. Bohmian mechanics overcomes the problem in the simplest possible way; namely, by placing primary emphasis on exactly one variable and letting the others trail in its footsteps. […] Moritz Schlick – the leader of the Vienna Circle and a student of Planck’s – summed up the impact of quantum mechanics on causality (which he does not distinguish from determinism) this way. “The principle of causality [“All events are in principle predictable”.]… constitutes an imperative to seek regularity, to describe events by laws…. And what quantum physics teaches us is just this: that the principle is bad, useless, impractical within the limits precisely laid down by the principle of indeterminacy” (Schlick 1931). Rudolf Carnap drew much the same conclusion some thirty-five years later. “[…] Classical physicists were convinced that, with the progress of research, laws would become more and more exact, and that there is no limit to the precision that can be obtained in predicting observable events…. Determinism in [thisl classical sense has been abandoned” (Carnap 1966). […] Shall we accept chaotic systems as deterministic or not? One could find some justification for going either way. The situation is similar in Bohmian mechanics (Dürr 1992). One could say that Bohmian mechanics is deterministic in principle but that for all practical purposes we find the same limitations on predictability there as in quantum mechanics, and hence the same degree of indeterminism. […] There is a clear sense in which Bohmian mechanics and quantum mechanics are in the same boat with respect to determinism and a clear sense in which they differ. If one takes determinism to be about ontology and evolution equations, one could view Bohmian mechanics as a theory of a deterministic world that, ironically, we may only grasp as though it were indeterministic. Alternatively if determinism is epistemological and regulative, one could view Bohmian mechanics as utilizing the resources of deterministic evolution in a reductio; viz., to argue for the abandonment of determinism by showing that it is fruitless to seek after exact predictability. In trying to balance between ontology and epistemology, we are broaching questions of realism.

Beller, Bohm and the “inevitability” of acausality (1996).
>One would expect that the proponents of the Copenhagen interpretation are in possession of some very strong arguments, if not of inevitability, at least of high plausibility. Yet a critical reading reveals that all the far-reaching, or one might say far-fetched, claims of inevitability are built on shaky circular arguments, on intuitively appealing, but incorrect, statements, on metaphorical allusions to quantum “inseparability” and “indivisibility”, which have nothing to do with quantum entanglement and non-locality. Acausality, in the writings of Bohr, Heisenberg, Pauli and Born, is not a sharply defined concept. Precisely because acausality is used as a tool of legitimization, as a sword to pin down the opponents, its meaning changes from text to text, from context to context. Despite Bohr’s life-long preoccupation with causality, his use of this concept is unsystematic and sometimes contradictory. Bohr had a very “thick” notion of causality: sometimes it is a cause-effect relationship, sometimes it is “determinism”, sometimes it is an epistemological – other times ontological – definition, sometimes causality is equated only with applicability of conservation laws of energy and momenta, other times with simultaneous applicability of both space-time and energy-momenta concepts. Sometimes Bohr’s understanding of causality is a probabilistic one, applying to an individual system, and sometimes, it is a statistical ensemble interpretation. Bohr often conflates determinism and predictability, which are substantially different notions. I will follow in this paper Bohr’s, Heisenberg’s and Bohm’s use of “determinism” and “causality” as interchangeable notions, despite the differences between them. […] Bohr’s arguments of the “inevitability” of acausality and complementarity similarly cannot withstand close scrutiny. I have argued elsewhere that, in the best case, Bohr’s arguments of inevitability are disguised arguments of consistency (Beller 1993). “The uncontrollable disturbance”, or uncontrollable interaction between the measuring and the measured, is the crucial component of all these arguments, and the basic building block on which Bohr’s web of arguments for complementarity rests. […] Bohm offers a brief critical historical analysis of the idea of classical determinism. The Greek idea of “fate” – the course of events which is beyond the power of humans to change – belongs to one of the earliest ideas of complete determinism. Yet it is only with the invention of machinery, which allowed the control of motion with precision, that the idea of a mechanistic and deterministic world view developed in the sixteenth through the nineteenth centuries. When work was done mostly by hand or by aid of animals, one could not control motions exactly – one could push in the right general direction, and push backward if one had gone too far. “It is very likely that the modern form of the idea of complete determinism was suggested… by its resemblance to complex and precisely constructed machines, such as clocks” (Bohm 1951).

Regarding cosmology, Greene, whose area of research is String Theory, explains the novelties introduced by the study of black holes. The work of theoretical physicists Stephen Hawking (1942-2018, English), Cumrun Vafa (1960-, Iranian) and Andrew Strominger (1955-, American) are quoted.

Greene, The elegant universe (2000).
>The probabilistic aspect of quantum mechanics considerably softens Laplace’s determinism by transforming the inevitability of events into contingencies, but the latter remain perfectly determined within the conventional framework of quantum theory. In 1976 Hawking realized that the existence of black holes would transgress even this softened form of determinism. Once again, the calculations behind this claim are appallingly complicated, but the central idea is quite simple. When something falls into a black hole, it also takes its wave function. But this implies that, in its task of determining the wave functions at all future times, our “sufficiently vast” intelligence will be irreparably lured. To fully predict the future, you need to fully understand all of today’s wave functions. If some disappear into the abyss of black holes, the information they represent is lost. At first glance, one might think that this complication due to black holes does not deserve so much concern. Since everything that lies below the horizon of the black hole is cut from the rest of the Universe, can we not just forget what had the misfortune to fall into it? Furthermore, from a philosophical point of view, can we not say that the Universe has not really lost the information contained in the material sucked in by the black hole, that it is just locked in a region of space that we, rational beings, seek to avoid at all costs? Before Hawking discovered that black holes are not really black, the answer to all of these questions was yes. But after Hawking showed that black holes were radiating, the story was transformed. Radiation carries energy, and the black hole that emits it therefore sees its mass decrease: it evaporates little by little. In doing so, the distance between the central part of the black hole and its horizon is slowly reduced, and, as this veil recedes, regions of space, initially decoupled, return to cosmic life. And our philosophical speculations are now faced with the following fact: does the information contained in the things that the black hole swallowed – the data we imagined lost in its bowels – resurface as the black hole evaporates? This is the information required for quantum determinism to survive; this question therefore leads directly to wonder if black holes do not permeate the evolution of our Universe from a fortuitous sequence of events, even more fundamental. At the time of writing, the answer to this question is not unanimous among physicists. For years, Hawking has fervently defended the hypothesis that information does not reappear: according to him, black holes destroy information, and therefore “introduce a new type of uncertainty into physics, in addition of the usual uncertainty associated with quantum theory”. In fact, Hawking, in association with Kip Thorne, of Caltech, made a bet with John Preskill (also of Caltech) concerning the fate of the information trapped by the black hole: Hawking and Thorne bet that the information was lost forever, while Preskill bet the information would reappear as the black hole radiated and shrunk. The challenge? Information, in the proper sense of the term: “The loser(s) will reward the winner(s) by offering him the encyclopedia of his choice”. The issue is not yet resolved, but Hawking recently recognized that the new approach to black holes offered by string theory shows, as we explained above, that there could be a way for information to resurface. The novelty is that, for the type of black holes studied by Strominger and Vafa as well as by other physicists since their initial article, the information can be recorded and then restored by its constituent branes. This eventuality, as Strominger recently put it, “has led some string theorists to claim victory, claiming that information is recovered when the black hole evaporates. In my opinion, this conclusion is premature; there is still a lot of work to do to find out if this is really the case”. Vafa agrees and adds that he remains “agnostic on this point, which could result in one or the other outcome”. The answer to this question is one of the main challenges of current research. Here’s how Hawking presents it: “Most physicists want to believe that information is not lost, as this would ensure that the world is a safe and predictable place. But I think if you take general relativity seriously, you have to accept the possibility that space-time is knotting itself and that information is lost in its folds. The question of whether or not there is a loss of information is one of the main questions in theoretical physics today”.


## Mind and matter <a name="p4"></a>
	
### Introduction <a name="p41"></a>

The problem of how mind and matter are related to each other has many facets, and it can be approached from many different starting points. The historically leading disciplines in this respect are philosophy and psychology, which were later joined by behavioral science, cognitive science and neuroscience. In addition, the physics of complex systems and quantum physics have played stimulating roles in the discussion from their beginnings.
William James was a thinker in and between the disciplines of physiology, psychology and philosophy. His work contains seeds of pragmatism and phenomenology, and influenced generations of thinkers in Europe and America, including Edmund Husserl (1859-1938, Austrian), Russell and Ludwig Wittgenstein (1889-1951, Austrian). James emphasizes the importance of Kant on this matter.

James, Does ‘consciousness’ exist? (1904).
>‘Thoughts’ and ‘things’ are names for two sorts of object, which common sense will always find contrasted and will always practically oppose to each other. Philosophy, reflecting on the contrast, has varied in the past in her explanations of it, and may be expected to vary in the future. At first, ‘spirit and matter,’ ‘soul and body,’ stood for a pair of equipollent substances quite on a par in weight and interest. But one day Kant undermined the soul and brought in the transcendental ego, and ever since then the bipolar relation has been very much off its balance.

In this chapter, the relation between mind and matter will be explored from the viewpoint of some philosophers but then mainly of physicists. For this reason, notions of physics will have to be explained but will not be fully detailed, for space and clarity. One needs to understand what the quantum of action or a wave function is to get a clear picture of what can be nowadays termed “matter”. Indeed, many questions were raised by the introduction of quantum mechanics. Can quantum particles be legitimately understood as particles any more, even in the broadest sense, when we take, e.g., their localization properties into account? How can one spell out what a field is and can “quantum fields” in fact be understood as fields? Could it be more appropriate not to think of, e.g., quarks, as the most fundamental entities at all, but rather of properties or processes or events?

### Metaphysical considerations <a name="p42"></a>

The mind-body problem is a debate concerning the relationship between thought and consciousness in the human mind, and the brain as part of the physical body. It is distinct from the question of how mind and body function chemically and physiologically, as that question presupposes an interactionist account of mind-body relations. This question arises when mind and body are considered as distinct, based on the premise that the mind and the body are fundamentally different in nature. The problem was addressed by René Descartes in the 17th century, resulting in Cartesian dualism, and by pre-Aristotelian philosophers, and in earlier Asian traditions. A variety of approaches have been proposed. Most are either dualist or monist. Dualism maintains a rigid distinction between the realms of mind and matter. Monism maintains that there is only one unifying reality, substance or essence, in terms of which everything can be explained. Each of these categories contains numerous variants that would require a demanding literature survey. In this section, a few examples throughout the ages will be given.

The Buddha (Vth to IVth BCE) described the mind and the body as depending on each other and taught that the world consists of mind and matter which work together, interdependently.

Kamalasila, Stages of meditation, explained by the current Dalai Lama (2001).
>The nature of the mind is such that when you get it accustomed to the qualities, these can be developed without limits. Unlike those of the mind, the qualities of the body do not have this capacity to grow to the point of exceeding all limits. This is simply due to the fact that the body is made up of coarse elements and that the attributes of such coarse forms do not have the potential to develop without limits.

Plato believed that the material world is a shadow of a higher reality that consists of concepts he called Forms. Objects in our everyday world “participate in” these Forms, which confer identity and meaning to material objects. As the body is from the material world, the soul is from the world of Forms and is thus immortal. He believed the soul was temporarily united with the body and would only be separated at death, when it would return to the world of Forms. Since the soul does not exist in time and space, as the body does, it can access universal truths. For Plato, ideas (or Forms) are the true reality, and are experienced by the soul. The body is empty in that it cannot access the abstract reality of the world; it can only experience shadows. For Aristotle mind is a faculty of the soul and the soul is a property exhibited by the body. When the body perishes, so does the soul. Now, here is another view from Hippocrates.

Hippocrates, explained by Jung in Synchronicity – An acausal connecting principle.
>With us details are important for their own sakes; for the Oriental mind they always complete a total picture. In this totality, as in primitive or in our own medieval, pre-scientific psychology (still very much alive!), are included things which seem to be connected with one another only “by chance”, by a coincidence whose meaningfulness appears altogether arbitrary. This is where the theory of correspondentia comes in, which was propounded by the natural philosophers of the Middle Ages, and particularly the classical idea of the sympathy of all things. Hippocrates says:
>
>>There is one common flow, one common breathing, all things are in sympathy. The whole organism and each one of its parts are working in conjunction for the same purpose… the great principle extends to the extremest part, and from the extremest part it returns to the great principle, to the one nature, being and not being.
>
>The universal principle is found even in the smallest particle, which therefore corresponds to the whole.

By distinguishing between primary and secondary qualities, Galileo Galilei (1564-1642, Italian) continued the ontological dualism he himself inherited from the middle ages, which to some extent goes back even further to the ancient Greeks such as the above mentioned Plato’s cave (theory of Forms). As Plato discovered, once reality is so divided into Forms and appearances, it is difficult to bring it back together. Such dualism, then, is reflected in Galileo’s division between the outer, primary world of nature and the inner, secondary world of the effects caused by that outer world. According to Kepler, light rays were believed to enter the eye point by point and form an image in the back of the eye. Thus in vision, copies of the natural world are formed on the retina. Kepler expresses a deep commitment to Neo-platonic emanationism.

Kepler, Ad Vitellionem paralipomena (1604).
>The sun is a certain body in which resides that faculty of communication itself to all things which we call light. For this reason alone its rightful place is the middle point and centre of the whole world, so that it may diffuse itself perpetually and uniformly throughout the universe. All other beings that share in light imitate the sun.

Extending many of the key ideas of Galileo and Kepler, Descartes also distinguished between the primary and secondary qualities of objects, believing the mathematical structure of things to be primary. This distinction is one facet of Descartes’ larger dualistic ontology reflecting in the distinction between mind and matter. The mind, located in the pineal gland, is considered an unextended, self-contained substance that thinks. Matter is the realm of extended bodies in motion. The behavior of these bodies of matter is govern by inertia.
Einstein explicitly spoke of his belief in pre-established harmony of Leibniz, through which a physicist finds basic laws of Nature. With Leibniz’s monads “always in accord without having knowledge of one another”, we can see a link with the EPR experiment, and a non-local field, notions we will later develop.

Leibniz, explained by Jung in Synchronicity – An acausal connecting principle (1952).
>Gottfried Wilhelm von Leibnitz (1646-1716) appeared with his idea of pre-established harmony, that is, an absolute synchronism of psychic and physical events. […] For him God is the creator of order. He compares soul and body to two synchronized clocks and uses the same simile to express the relations of the monads or entelechies with one another. Although the monads cannot influence one another directly because, as he says, they “have no windows” (relative abolition of causality!), they are so constituted that they are always in accord without having knowledge of one another. He conceives each monad to be a “little world” or “active indivisible mirror”. Not only is man a microcosm enclosing the whole in himself, but every entelechy or monad is in effect such a microcosm. Each “simple substance” has connections, “which express all the others”. It is a “perpetual living mirror of the universe”. He calls the monads of living organisms “souls”: “the soul follows its own laws, and the body its own likewise, and they accord by virtue of the harmony pre-established among all substances, since they are all representations of one and the same universe”. This clearly expressed the idea that man is a microcosm. “Souls in general”, says Leibnitz, “are the living mirrors or images of the universe of created things”.

For Kant, there exists a world of a priori forms beyond mind and matter, which are seen as necessary preconditions for understanding. Some of these forms, space and time being examples, today seem to be pre-programmed in the brain. Kant views the mind-body interaction as taking place through forces that may be of different kinds for mind and body. Kant can be viewed both as a dualist and a monist. As a result, it is now time to introduce monism. Monism attributes oneness or singleness to a concept, such as existence. Various kinds of monism can be distinguished. Priority monism states that all existing things go back to a source that is distinct from them; e.g., in Neoplatonism everything is derived from The One. In this view only one thing is ontologically basic or prior to everything else. Existence monism posits that, strictly speaking, there exists only a single thing, the Universe, which can only be artificially and arbitrarily divided into many things. Substance monism asserts that a variety of existing things can be explained in terms of a single reality or substance. Substance monism posits that only one kind of stuff exists, although many things may be made up of this stuff, e.g., matter or mind. 

James, Does ‘consciousness’ exist? (1904).
>For the thinkers I call neo-Kantian, the word consciousness to-day does no more than signalize the fact that experience is indefeasibly dualistic in structure. It means that not subject, not object, but object-plus-subject is the minimum that can actually be. The subject-object distinction meanwhile is entirely different from that between mind and matter, from that between body and soul. Souls were detachable, had separate destinies; things could happen to them. To consciousness as such nothing can happen, for, timeless itself, it is only a witness of happenings in time, in which it plays no part. […] Consciousness as such is entirely impersonal – ‘self’ and its activities belong to the content. To say that I am self-conscious, or conscious of putting forth volition, means only that certain contents, for which ‘self’ and ‘effort of will’ are the names, are not without witness as they occur.. […] Experience, I believe, has no such inner duplicity; and the separation of it into consciousness and content comes, not by way of subtraction, but by way of addition. […] A given undivided portion of experience, taken in one context of associates, play the part of a knower, of a state of mind, of ‘consciousness’; while in a different context the same undivided bit of experience plays the part of a thing known, of an objective ‘content’. In a word, in one group it figures as a thought, in another group as a thing. […] The dualism connoted by such double-barrelled terms as ‘experience’, ‘phenomenon’, ‘datum’, ‘Vorfindung’ – terms which, in philosophy at any rate, tend more and more to replace the single-barrelled terms of ‘thought’ and ‘thing’ – that dualism, I say, is still preserved in this account, but reinterpreted, so that, instead of being mysterious and elusive, it becomes verifiable and concrete. […] My thesis is that if we start with the supposition that there is only one primal stuff or material in the world, a stuff of which everything is composed, and if we call that stuff ‘pure experience’, the knowing can easily be explained as a particular sort of relation towards one another into which portions of pure experience may enter. […] The entering wedge for this more concrete way of understanding the dualism was fashioned by Locke when he made the word ‘idea’ stand indifferently for thing and thought, and by Berkeley when he said that what common sense means by realities is exactly what the philosopher means by ideas. Neither Locke nor Berkeley thought his truth out into perfect clearness, but it seems to me that the conception I am defending does little more than consistently carry out the ‘pragmatic’ method which they were the first to use.

Spinoza’s metaphysics of substance has been called neutral monism; it is a form of monism because it allows for only one substance, and it is neutral because he describes the one substance as both a body and a mind. Both Spinoza and Hume are often identified as the originators of neutral monism. This view is in agreement with the more familiar versions of monism: idealism and materialism. What distinguishes neutral monism from its monistic rivals is the claim that the intrinsic nature of ultimate reality is neither mental nor physical. This negative claim also captures the idea of neutrality: being intrinsically neither mental nor physical in nature ultimate reality is said to be neutral between the two. Spinoza is also viewed as a pantheist; the belief that everything composes an all-encompassing, immanent God, or that the universe (or nature) is identical with divinity. 

Spinoza, explained by Misrahi in commentaries to Spinoza’s Ethics (2005).
>The monism of the substance is that of Deus sive Natura. […] This second part is the establishment of a doctrine as important as that of the monism of Nature. The human individual is not substantial, nor dualistic, he is the contemporary unity and without internal interaction of a spirit and body, the spirit being very precisely the idea, that is to say consciousness of the body, nothing more (it is not a soul and is not immortal), but nothing less (it is necessarily, albeit obscurely, awareness of its body and its modifications). […] Spinozism is neither idealism (because Being is Extended) nor materialism (because Being is Thought). Matter must be explained by matter and spirit by spirit (which neither materialists nor idealists do). […] What “first and foremost” is the essence of man is the body and its idea.

Nietzsche
>[Thus spoke Zarathustra, explained by Goldschmidt (1983)] This will, of which we have too often wanted to see a somewhat brutal vitalism, by naively confusing energy and will, is the rigorous conjunction of the soul and the body. Nietzsche never dissociates one from the other and by this he turns out to be extremely close to Spinoza, the only philosopher, perhaps to have attempted the same effort of liberation. For Nietzsche as for Spinoza, thought is thought of the body: idea corporis, thought of a bodily reality, in other words “consciousness through and through” (Misrahi). A careful reading would reveal at first glance a surprising agreement between the vision of man in Spinoza and the overman in Nietzsche. Desire (“Sehnsucht” in Nietzsche and “conatus” in Spinoza) is for both of them the fundamental driving force of their work. The desire is, for Nietzsche, pursuit of what will bring the body and the mind to their greatest power and we find there too Spinoza. When Spinoza says that you do not want something because it is good but that it is good because you want it, he is very close to Nietzsche who throughout his Zarathustra tries only the same effort of man’s liberation. For Nietzsche as for Spinoza, joy is essential and for both there is no fixed ultimate goal, an objectively established end.
>
>[Thus spoke Zarathustra (1883)] “I am body and soul” is what the child says. And why shouldn’t we speak like children? But the one who is awake, the one who knows, says: “I am body through and through, and nothing beyond that; and the soul is just a word for something that belongs to the body”. The body is reason, a great reason, a multiplicity that has one meaning, a war and a peace, a flock and a shepherd. Your little reason, too, my brother, whom you call “spirit” is a tool of your body, a small tool, a small toy of your big reason.

Mach occupies a central position in the history of neutral monism. He influenced James and Russell and, through them, all of the writers on neutral monism in the English speaking world. For Mach, material objects and the ego are dissolved into elements/sensations that are related in certain complex ways. The world presents itself as “a viscous mass [of elements], at certain places (as in the ego) more firmly coherent than in others”. The neutral elements (only a minute fraction of which are sensations) and their relations are the basic reality.
	
Mach
>[The science of mechanics: a critical and historical account of its development (1893)] Only by granting free sway to reason and experience in the provinces in which they alone are determinative, shall we, to the weal of mankind, approach, slowly, gradually, but surely, to that ideal of a monistic view of the world which is alone compatible with the economy of a sound mind.
>
>[The analysis of sensations and the relation of the physical to the psychical (1902)] As soon as we have perceived that the supposed unities “body” and “ego” are only makeshifts, designed for provisional orientation and for definite practical ends (so that we may take hold of bodies, protect ourselves against pain, and so forth), we find ourselves obliged, in many more advanced scientific investigations, to abandon them as insufficient and inappropriate. The antithesis between ego and world, between sensation (appearance) and thing, then vanishes. […] Science has simply to accept this connection, and to get its bearings in it, without at once wanting to explain its existence. […] The philosophical spiritualist is often sensible of the difficulty of imparting the needed solidity to his mind-created world of bodies; the materialist is at a loss when required to endow the world of matter with sensation. The monistic point of view, which reflexion has evolved, is easily clouded by our older and more powerful instinctive notions.

Idealism is a monistic view. Simply it is the belief that the very substance of thought is the true reality. Idealism is a diverse group of metaphysical philosophies which assert that “reality” is in some way indistinguishable or inseparable from human understanding and/or perception; that it is in some sense mentally constituted, or otherwise closely connected to ideas. Kant was a pioneer of modern idealist thought. For him, idealism does “not concern the existence of things”, but asserts only that our “modes of representation” of them, above all space and time, are not “determinations that belong to things in themselves” but essential features of our own minds. Kant called this position “transcendental” and “critical” idealism, since it describes the way in which “reality” is utterly transcended by, and cannot be thought separate from, the categories with which they are structured by and in human understanding.

Epistemologically, idealism manifests as a skepticism about the possibility of knowing any mind-independent thing. In contrast to materialism, idealism asserts the primacy of consciousness as the origin and prerequisite of phenomena. Idealism holds consciousness or mind to be the “origin” of the material world – in the sense that it is a necessary condition for our positing of a material world – and it aims to explain the existing world according to these principles. Moore was critical to idealism and his rejection concerned the monism which was characteristic of British idealism.

Moore, The refutation of idealism (1903).
>Modern Idealism, if it asserts any general conclusion about the universe at all, asserts that it is spiritual. There are two points about this assertion to which I wish to call attention. These points are that, whatever be its exact meaning, it is certainly meant to assert (1) that the universe is very different indeed from what it seems, and (2) that it has quite a large number of properties which it does not seem to have. Chairs and tables and mountains seem to be very different from us; but, when the whole universe is declared to be spiritual, it is certainly meant to assert that they are far more like us than we think. The Idealist means to assert that they are in some sense neither lifeless nor unconscious, as they certainly seem to be; and I do not think his language is so grossly deceptive, but that we may assume him to believe that they really are very different indeed from what they seem. And secondly when he declares that they are spiritual, he means to include in that term quite a large number of different properties. When the whole universe is declared to be spiritual, it is meant not only that it is in some sense conscious, but that it has what we recognize in ourselves as the higher forms of consciousness. That it is intelligent; that it is purposeful; that it is not mechanical; all these different things are commonly asserted of it.

In the philosophy of mind, double-aspect theory is the view that the mental and the physical are two aspects of, or perspectives on, the same substance. It is also called dual-aspect monism. The theory’s relationship to neutral monism is ill-defined, but one proffered distinction says that whereas neutral monism allows the context of a given group of neutral elements to determine whether the group is mental, physical, both, or neither, double-aspect theory requires the mental and the physical to be inseparable and mutually irreducible (though distinct). In that sense, Spinoza, Kant, Schopenhauer, Bohm, Jung and Pauli can be viewed as double-aspect theorists. Spinoza believed that the Existence had two aspects, Extension and Mind, which together were to be taken as two of an infinite set of attributes comprising God (or Nature). There is a dual-aspect interpretation of Immanuel Kant’s noumenon (a posited object or event that exists independently of human sense and/or perception). Schopenhauer considered the fundamental aspects of reality to be Will and Representation. Bohm used implicate and explicate order as a means of displaying dual-aspects. From the work of Pauli and Jung results a philosophical approach that has been titled the Pauli-Jung conjecture, of dual-aspect monism which has a very specific further feature, namely that different aspects may show a complementarity in a quantum physical sense. That is, the Pauli-Jung conjecture implies that with regard to mental and physical states there may be incompatible descriptions of different parts that emerge from the whole. This stands in close analogy to quantum physics, where complementary properties cannot be determined jointly with accuracy.

Jung, Synchronicity – An acausal connecting principle (1952).
>For the unconscious psyche space and time seem to be relative; that is to say, knowledge finds itself in a space-time continuum in which space is no longer space, nor time time. If, therefore, the unconscious should develop or maintain a potential in the direction of consciousness, it is then possible for parallel events to be perceived or “known”. […] We have absolutely no scientific means of proving the existence of an objective meaning which is not just a psychic product. […] For those who are interested in psychology I should like to mention here that the peculiar idea of a self-subsistent meaning is suggested in dreams. […] These dreams seem to point to the presence of a formal factor in nature. They describe not just a lusus naturae, but the meaningful coincidence of an absolutely natural product with a human idea apparently independent of it. This is what the dreams are obviously saying, and what they are trying to bring nearer to consciousness through repetition.

Bohr, Atomic physics and human knowledge (1957).
>[At the] International Congress on Light Therapy held in Copenhagen in 1932, it was incidentally pointed out that even the psycho-physical parallelism as envisaged by Leibniz and Spinoza has obtained a wider scope through the development of atomic physics, which forces us to an attitude towards the problem of explanation recalling ancient wisdom, that when searching for harmony in life one must never forget that in the drama of existence we are ourselves both actors and spectators.

Wigner thinks that the question of the existence of almost anything is not a very relevant question. Solipsism for him may be logically consistent but materialist monism no. For Wigner, the body influences the mind but the mind does not influence the body. The equations of motion of quantum mechanics cease to be linear when consciousness comes into play. Bohm remarks that this is a renewal of Descartes’ dualism, and that this view is further developed by Stapp, Albert and Loewer. This text serves as an example that even today, the body-mind problem is still very actual and debated. The many-mind theory of Everett will be developed in the next chapter since it is linked to consciousness.

Wigner, Remarks on the mind-body question (1995).
>Until not many years ago, the “existence” of a mind or soul would have been passionately denied by most physical scientists. The brilliant successes of mechanistic and, more generally, macroscopic physics and of chemistry overshadowed the obvious fact that thoughts, desires, and emotions are not made of matter, and it was nearly universally accepted among physical scientists that there is nothing besides matter. The epitome of this belief was the conviction that, if we knew the positions and velocities of all atoms at one instant of time, we could compute the fate of the universe for all future. Even today, there are adherents to this view though fewer among the physicists than – ironically – among biochemists. […] It is perhaps important to point out at this juncture that the question concerning the existence of almost anything (even the whole external world) is not a very relevant question. All of us recognize at once how meaningless the query concerning the existence of the electric field in vacuum would be. All that is relevant is that the concept of the electric field is useful for communicating our ideas and for our own thinking.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>Recently these questions have been re-examined by Stapp and by Albert and Loewer in what is called the many-minds interpretation. The key step here is to assume that awareness or consciousness is not basically a physical property, but rather a property of mind which is supposed to be entirely different from matter. Thus the theory, unlike Everett’s, assumes a dualism between mind and matter along lines similar to that proposed much earlier by Descartes. The advantage of this approach is that we do not assume that matter has properties given to it by awareness, i.e. that physical meaning can be given in the quantum theory to all branches of a linear superposition of wave functions without contradicting appearances. It thus makes possible a more consistent way of separating one domain (matter) that satisfies the superposition principle of quantum theory and another domain (mind) that does not. Such a dualist approach is in certain ways reminiscent of Wigner’s suggestions, but it must be remembered that in the latter, mind and matter are supposed to interact in a two-way process. In the many-minds interpretation however there is no such interaction. Rather mind is assumed to be capable of becoming conscious of certain aspects of the state of matter without influencing the latter in any way at all. […] All of this is very close to the treatment given by Everett and differs only in that we have further assumed a unique last stage, i.e. mind, in which the linear superposition fails (whereas Everett says nothing about this question). By thus defining this unique last stage as a splitting of the mind and relating this split to the linear superposition of memories in the brain through random processes, the many-minds interpretation avoids difficulties of specifying the basis in which linear superposition breaks down. On the other hand, this advantage has been gained at the expense of not only bringing in duality of mind and matter, but also of having to assume special properties of mind which are difficult to understand, i.e. the ability to be aware of matter without any interaction and the ability to split in a random way into many minds, each of which is associated to a particular physical state. 

Thomas Nagel (1937-, American) is a philosopher known for his critique of material reductionist accounts of the mind. He argues against a materialist view of the emergence of life and consciousness, writing that “the standard neo-Darwinian view flies in the face of common sense”. He writes that “mind is a basic aspect of nature, and that any philosophy of nature that cannot account for it is fundamentally misguided”. He argues that the principles that account for the emergence of life may be “teleological, rather than materialist or mechanistic”, despite being an atheist and not a proponent of intelligent design.

Nagel, explained by Chomsky in Nature and language (1995).
>These concerns, at the origins of modern science, have something of the flavour of contemporary discussion of the “mind-body problem”. They also raise questions about what is at stake. Thomas Nagel observes that “the various attempts to carry out this apparently impossible task [of reducing mind to matter] and the arguments to show that they have failed, make up the history of the philosophy of mind during the past fifty years”. The hopeless task is to “complete the materialist world picture” by translating accounts of “mental phenomena” in terms of “a description that is either explicitly physical or uses only terms that can apply to what is entirely physical”, or perhaps gives “assertibility conditions” on “externally observable grounds” (Nagel 1993).

### Beauty and truth <a name="p43"></a>

Feynman is often quoted as saying: “You can recognize truth by its beauty and simplicity”. He had great respect for Dirac, who believed that theories in physics should be both simple and beautiful. Indeed, Dirac found in Einstein’s theory of general relativity “great mathematical beauty”. Mathematical beauty is concerned with the abstractness, purity, simplicity, depth or orderliness of mathematics. Jung remarks that the unconscious spontaneously produces pictures of wholeness and order. Einstein was concerned about simplicity.

Jung, Synchronicity – An acausal connecting principle (1952).
>Remarkably enough, the psychic pictures of wholeness which are spontaneously produced by the unconscious, the symbols of the self in mandala form, also have a mathematical structure. They are as a rule quaternities (or their multiples). These structures not only express order, they also create it. That is why they generally appear in times of psychic disorientation in order to compensate a chaotic state or as formulations of numinous experiences. It must be emphasized yet again that they are not inventions of the conscious mind but are spontaneous products of the unconscious, as has been sufficiently shown by experience. Naturally the conscious mind can imitate these patterns of order, but such imitations do not prove that the originals are conscious inventions. From this it follows irrefutably that the unconscious uses number as an ordering factor. […] The synchronicity principle possesses properties that may help to clear up the body-soul problem. Above all it is the fact of causeless order, or rather, of meaningful orderedness, that may throw light on psychophysical parallelism. 

Einstein, The world as I see it (1934).
>For our research, it turns out to be essential that all these forms and their relationships defined by laws, are obtained according to the principle of research of the simplest mathematical concepts and their connections. If we can limit the kinds of simple fields that exist mathematically and the simple equations possible between them, then the theorist can hope to grasp reality in its depth.

Pauli, in a discussion quoted by Heisenberg in Physics and beyond (1969).
>Einstein’s conception is closer to mine. His God is somehow involved in the immutable laws of nature. Einstein has a feeling for the central order of things. He can detect it in the simplicity of natural laws. We may take it that he felt this simplicity very strongly and directly during his discovery of the theory of relativity. Admittedly, this is a far cry from the contents of religion. I don’t believe Einstein is to tie to any religious tradition, and I rather think the idea of a personal God is entirely foreign to him. But as far as he is concerned there is no split between science and religion: the central order is part of the subjective as well as the objective realm, and this strikes me as being a far better starting point.

Wheeler, Geons, black holes, and quantum foam: a life in physics (1998).
>I think back to Baruch de Spinoza in the seventeenth century; to Bernhard Riemann, William Kingdon Clifford, and Ernst Mach in the nineteenth; and to Niels Bohr in the twentieth. Spinoza’s belief in the harmony, the beauty, and the ultimate comprehensibility of nature had a profound effect on Einstein, What else could have led Einstein to express his sympathy for the Good Lord if general relativity proved to be wrong? Through Einstein and his followers \– Dirac, Murray Gell-Mann, Feynman, and others \– it has become an article of faith in twentieth-century physics that if a theory is simple enough, comprehensive enough, “beautiful” enough, it must be right.

For some, it is difficult to defend the notion that the truth is recognizable by its beauty and simplicity. According to Lee Smolin (1955-, American), it is an idea that has contributed to “getting fundamental physics into its current mess”. Bohm and Hiley also comment on this basis for truth.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>We can ask whether symmetry and beauty are always a sure sign that we have reached an ultimate truth that can never be altered through further enquiry. Indeed in our approach we argue that there is no reason to assume the ultimate truth of any particular feature of knowledge, however beautiful we may feel it to be. […] Until the present century, the physical concepts were, for the most part, considered as primary, while mathematical equations were regarded as providing a more detailed and precise way of talking about these physical concepts. However, during the twentieth century with the advent of quantum theory (and to a lesser extent, relativity) the mathematics became much more highly developed and the physical interpretation much more abstract and indirect, as well as much less clear. As a result there was a constantly increasing focus on the mathematics, while the physical ideas were given less and less importance. Thus during the 1920s Sir James Jeans said that God must be a mathematician, implying by this that the universe was constructed on a mathematical plan and that its essence is best grasped in terms of the mathematics itself. Later, Heisenberg went much further along these lines and said very explicitly that the essential truth was in the mathematics. This view has become the common one among most of the modern theoretical physicists who now regard the equations as providing their most immediate contact with nature (the experiments only confirming or refuting the correctness of this contact). So without an equation there is really nothing to talk about. On the other hand, in the past we began talking about our concept of physical reality and used the equations to talk about them.

Ockham’s razor is a prudent heuristic, providing us with an intuitive guide to the comparisons of different hypotheses. It is often used by physicists when having to decide between different theories that cannot yet be proven. Simply put: other things being equal, we should prefer simpler ones. More specifically, the monk William of Ockham (1287-1347, English) meant that hypothetical entities are not to be multiplied without necessity. Yet, Ockham’s razor is an epistemological, not a metaphysical principle. It’s about how we know things, whereas Feynman’s and Dirac’s statements seem to be about the fundamental nature of reality. Wheeler also opposed their statements, by showing that in the current state, chaos seems to be the rule. He refers to the World Turtle, an old mytheme of a giant turtle (or tortoise) supporting or containing the world.

Wheeler, Geons, black holes, and quantum foam: a life in physics (1998).
>Quantum theory tells us that things get more chaotic, not more orderly, as we go deeper. The ultimate basis of reality is not likely to be found in the unpredictable fluctuations that characterize the smallest dimensions, the deepest layers. How can a turtle composed of seething quantum foam, in which even space and time cease to have meaning, be patiently holding up more solid turtles? […] Time itself, I have come to believe, participates in the general complexity, in the fluctuations, in the uncertainty. The smooth flow of time – or our smooth passage through it – is an illusion that is shattered when we look at short-enough intervals of time, and when we ask about time at the moment of the Big Bang, at a moment of gravitational collapse, at the moment of the Big Crunch.

However, some physical laws can be shown to be elegant even when concerned about chaos.

Greene, The elegant universe (2000).
>Physicists say that these two properties of physical laws – the fact that they don’t depend on where or when you use them – are symmetries of nature. What they mean is that nature treats every moment and every place the same way – symmetrically – making sure that the same basic laws are operating. As in art and music, these symmetries are fundamentally satisfying: they emphasize the order and coherence of the mechanisms of nature. The elegance of the richness, complexity and diversity of phenomena that emerge from a simple set of universal laws is at least in part what physicists think of when they speak of “beauty”.


### Quantum physics interpretations <a name="p44"></a>

Despite its status as a core part of contemporary physics, there is no consensus among physicists or philosophers of physics on the question of what, if anything, the empirical success of quantum theory is telling us about the physical world. This gives rise to the collection of philosophical issues known as “the interpretation of quantum mechanics”. One should not be misled by this terminology into thinking that what we have is an uninterpreted mathematical formalism with no connection to the physical world. Rather, there is a common core of interpretation that consists of recipes for calculating probabilities of outcomes of experiments performed on systems subjected to certain state preparation procedures. What are often referred to as different “interpretations” of quantum mechanics differ on what, if anything, is added to the common core. Arguably, two of the major approaches, hidden-variables theories and collapse theories, involve formulation of physical theories distinct from standard quantum mechanics; this renders the terminology of “interpretation” even more inappropriate.

Von Neumann, one of the greatest mathematicians of the twentieth century, claimed that he had proven that Einstein’s dream of a deterministic completion or reinterpretation of quantum theory was mathematically impossible. Physicists and philosophers of science almost universally accepted von Neumann’s claim. For example, Max Born, who formulated the statistical interpretation of the wave function, assured in 1949 that “No concealed parameters can be introduced with the help of which the indeterministic description could be transformed into a deterministic one. Hence if a future theory should be deterministic, it cannot be a modification of the present one but must be essentially different”.

Bohmian mechanics is a counterexample to the claims of von Neumann. Thus von Neumann’s argument must be wrong. In fact, according to John Bell, von Neumann’s assumptions (about the relationships among the values of quantum observables that must be satisfied in a hidden-variables theory) are so unreasonable that the “the proof of von Neumann is not merely false but foolish!” Recently, however, physicists more commonly cite Bell’s inequality in support of the contention that a deterministic completion of quantum theory is impossible. We still find, a quarter of a century after the rediscovery of Bohmian mechanics in 1952, statements such as these: “In my opinion, the most convincing argument against the theory of hidden variables was presented by J.S. Bell (1964)” (Wigner 1976). There was, however, one physicist who wrote on this subject with even greater clarity and insight than Wigner himself: the very Bell whom Wigner praises for demonstrating the impossibility of a deterministic completion of quantum theory such as Bohmian mechanics. Here’s how Bell reacted to Bohm’s discovery.

Bell, Speakable and unspeakable in quantum mechanics (1987).
>But in 1952 I saw the impossible done. It was in papers by David Bohm. Bohm showed explicitly how parameters could indeed be introduced, into nonrelativistic wave mechanics, with the help of which the indeterministic description could be transformed into a deterministic one. More importantly, in my opinion, the subjectivity of the orthodox version, the necessary reference to the ‘observer,’ could be eliminated. Moreover, the essential idea was one that had been advanced already by de Broglie in 1927, in his ‘pilot wave’ picture. But why then had Born not told me of this ‘pilot wave’? If only to point out what was wrong with it? Why did von Neumann not consider it? More extraordinarily, why did people go on producing ‘impossibility’ proofs after 1952, and as recently as 1978? When even Pauli, Rosenfeld and Heisenberg could produce no more devastating criticism of Bohm’s version than to brand it as ‘metaphysical’ and ‘ideological’? Why is the pilot wave picture ignored in text books? Should it not be taught, not as the only way, but as an antidote to the prevailing complacency? To show that vagueness, subjectivity, and indeterminism, are not forced on us by experimental facts, but by deliberate theoretical choice?

Wigner to the contrary notwithstanding, Bell did not establish the impossibility of a deterministic reformulation of quantum theory, nor did he ever claim to have done so. On the contrary, until his untimely death in 1990, Bell was the prime proponent, and for much of this period almost the sole proponent, of the very theory, Bohmian mechanics, that he supposedly demolished.

Bohmian mechanics inherits and makes explicit the nonlocality implicit in the notion, common to just about all formulations and interpretations of quantum theory, of a wave function on the configuration space of a many-particle system. It accounts for all of the phenomena governed by nonrelativistic quantum mechanics. In particular, the usual measurement postulates of quantum theory, including collapse of the wave function and probabilities given by the absolute square of probability amplitudes, emerge from an analysis of the two equations of motion: Schrödinger’s equation and the guiding equation. No invocation of a special, and somewhat obscure, status for observation is required.

Bohmian mechanics has never been widely accepted in the mainstream of the physics community. According to Sheldon Goldstein (1947-, American), “since it is not part of the standard physics curriculum, many physicists – probably the majority – are simply unfamiliar with the theory and how it works”. Sometimes the theory is rejected without explicit discussion of reasons for rejection. One also finds objections that are based on simple misunderstandings; among these are claims that some no-go theorem, such as von Neumann’s theorem, the Kochen-Specker theorem, or Bell’s theorem, shows that the theory cannot work. However, for Goldstein, receivable objections are that Bohmian mechanics is too complicated or inelegant and it makes precisely the same predictions as standard quantum mechanics (insofar as the predictions of standard quantum mechanics are unambiguous). Thus, it may not be regarded as a distinct theory but merely a reformulation of standard quantum theory.

Pylkkänen, Henry Stapp vs. David Bohm on mind, matter, and quantum mechanics (2019).
>One of the main aims of Bohm’s 1951 book Quantum theory was to explicate the physical meaning of the “orthodox” or Copenhagen interpretation (as it turned out, Bohm’s view in the book is close to Wolfgang Pauli’s view). After writing the book, Bohm was still not satisfied with quantum theory, feeling that something essential was missing, namely, a notion of “…an actual movement or activity by which one physical state could pass over into another one” (Bohm 1987). Discussions with Einstein in Princeton further encouraged him to look for a deterministic extension of quantum theory. In 1952, he published a “suggested interpretation of quantum theory in terms of hidden variables”. He had independently discovered and improved the “pilot wave” theory which de Broglie had presented in the Solvay conference in 1927, but which had been dismissed at the time. In the Bohm theory, the electron is a particle with a well-defined position and momentum, guided by a new type field that is described by the wave function. The field gives rise to a quantum potential Q. […] In the Bohm theory, the wave function Ψ thus not only can be used to calculate probabilities, it also describes an objectively existing field that influences the particle through the quantum potential.

While Einstein and Bohm both pursued a deterministic description of quantum mechanics, their philosophical concern was in fact primarily realism and not determinism. And yet, Einstein did not embraced Bohm’s ideas.

Einstein, Letter to Aron Kupperman (1954).
>From a purely logical standpoint there is, in my opinion, no principled objection to be made against Bohm’s completion of the quantum theory. Looked at from a physical standpoint, however, Bohm’s way out does not seem to me acceptable.

Fine, On the interpretation of Bohmian mechanics (1996).
>Although Bohm’s ideas failed to get Einstein’s endorsement, they did interest John Bell whose indoctrination into quantum metaphysics was never complete. Bell’s own investigations of nonlocality were sparked by the nonlocal character of the Bohm theory and when Bell came to believe that nonlocality was in the phenomena not just in our theories, Bohm’s way appeared more palatable and interesting to him than it had before. I believe that the impetus for the current exploration of Bohm’s program is due to Bell’s taking it seriously. Whatever the sociological currents, clearly we have reached a stage where we can consider Bohmian mechanics on its own, and even distinguish it from Bohm’s original ideas just as we distinguish Newton’s mechanics from Newtonian mechanics.

Bohm acknowledged that, at the time of his death, it was not possible to decide for one interpretation or another.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>Our interpretation and the many others that have been proposed lead, at least for the present, to the same predictions for the experimental results, there is no way experimentally to decide between them. Arguments may be made in favour or against any of them on various bases, which include not only those that we have given here, but also questions of beauty, elegance, simplicity and economy of hypotheses. However, these latter are somewhat subjective and depend not only on the particular tastes of the individual, but also on socially adopted conventions, consensual opinions and many other such factors which are ultimately imponderable and which can be argued many ways. […] Folse has made it clear that Bohr is not simply a positivist, but that the notion of some kind of independent physical reality underlies all his thinking.

Cushing, The causal quantum theory program (1996).
>Let me parse a scientific theory into two distinct components: its formalism and its interpretation. These are logically separable, even if they are often entangles in practice. Here a formalism means a set of equations and a set of calculational rules for making predictions that can be compared with experiment. Both standard quantum mechanics and Bohm’s version use the same set of rules for predicting the values of the observables. The (physical) interpretation refers to what the theory tells us about the underlying structure of these phenomena (i.e., a corresponding story about the furniture of the world – an ontology). Hence, one formalism with two different interpretations counts as two different theories. […] Bohm accepted the formalism of quantum mechanics and showed that more microstructure is consistent with it than had previously been appreciated. The usual calculational rules of standard quantum mechanics still give average values for observed quantities, but now there are actual particles and trajectories.

Beller goes further, regarding the Copenhagen interpretation.

Beller, Bohm and the “inevitability” of acausality (1996).
>I have argued elsewhere (Beller 1996) that the Copenhagen interpretation cannot be reconstructed as a coherent philosophical framework, for it is more adequately described as a collection of local, often contradictory, arguments in changing theoretical and socio-political circumstances. Heisenberg’s contradictory statements about the necessity of classical concepts in different contexts is another example for my claim.

We could find countless testimonies in favor or disfavor of any theories so we will stop here. To process with what is viewed as reality from physics’ perspective, we will focus on key concepts, such as those listed by Bohm below.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>Just what the points are that are not clear, will be specified in detail throughout this books […]. We can however outline a few of them here in a preliminary way. (1) Though the quantum theory treats statistical ensembles in a satisfactory way, we are unable to describe individual quantum processes without bringing in unsatisfactory assumptions, such as the collapse of the wave function. (2) There is by now the well-known nonlocality that has been brought out by Bell in connection with the EPR experiment. (3) There is the mysterious ‘wave-particle duality’ in the properties of matter that is demonstrated in a quantum interference experiment. (4) Above all, there is the inability to give a clear notion of what the reality of a quantum system could be. […] Because of nonlocality, quantum jiggling under quantum interference conditions and other quantum properties to which we have alluded, we may say that the quantum world is subtle. According to the dictionary this means “rarified, highly refined, delicate, elusive, indefinable”. Its root meaning is based on the Latin subtexlis which signifies “finely woven”. Clearly the quantum world as we have described it cannot be held in the hand or in any other way. The very effort to hold it, e.g. in measurement, produces thoroughgoing unpredictable and uncontrollable changes in it. Each element participates irreducibly in all the others. The absence of mutual externality and reparability of all the elements makes this world very elusive to grasp of our instruments. It slips through the ordinary ‘nets’ that we have devised to hold it. Nevertheless we are proposing that it is real and indeed that it constitutes a more basic reality than does the classical ‘world’. Indeed as we have shown, this classical ‘world’ comes out of the theory as relatively autonomous. This autonomy arises wherever the quantum potential can be neglected so that the classical world can be treated on its own as if it were independently existent. But according to our interpretation it is actually an abstraction from the subtle quantum world which is being taken as the ultimate ground of existence.

### Planck’s quantum of action and Heisenberg’s uncertainty <a name="p45"></a>

The Planck constant is a physical constant that is the quantum of electromagnetic action, which relates the energy carried by a photon (particle of light) to its frequency. A photon’s energy is equal to its frequency multiplied by the Planck constant. The Planck constant is of fundamental importance in quantum mechanics. Since energy and mass are equivalent (Einstein’s famous relation “energy = mass x constant”), the Planck constant also relates mass to frequency.

In short, in 1900, Planck discovered that energy can only be transmitted by small increments or steps, proportional to the frequency of its electromagnetic wave. For this discovery, he earned the Nobel Prize in Physics in 1918. In 1905, the value of incremental energy was associated by Einstein with a “quantum” or minimal element of the energy of the electromagnetic wave itself. Einstein’s explanation for these observations was that light itself is quantized; that the energy of light is not transferred continuously as in a classical wave, but only in small “packets” or quanta. The size of these “packets” of energy, which would later be named photons, was to be the same as Planck’s “energy element”. The light quantum behaved in some respects as an electrically neutral particle, as opposed to an electromagnetic wave. This discovery earned Einstein the Nobel Prize in Physics in 1921.

Bohr, Atomic physics and human knowledge (1957).
>A clue to the solution of this dilemma was, however, already provided by Planck’s discovery of the elementary quantum of action which was the outcome of a very different line of physical research. As it is well known, Planck was led to this fundamental discovery by his ingenious analysis of such features of the thermal equilibrium between matter and radiation which, according to the general principles of thermodynamics, should be entirely independent of any specific properties of matter, and accordingly of any special ideas on atomic constitution. The existence of the elementary quantum of action expresses, in fact, a new trait of individuality of physical processes which is quite foreign to the classical laws of mechanics and electromagnetism and limits their validity essentially to those phenomena which involve actions large compared to the value of a single quantum, as given by Planck’s new atomistic constant. This condition, though amply fulfilled in the phenomena of ordinary physical experience, does in no way hold for the behaviour of electrons in atoms, and it is indeed only the existence of the quantum of action which prevents the fusion of the electrons and the nucleus into a neutral massive corpuscle of practically infinitesimal extension.

de Broglie, Une tentative d’interprétation causale et non linéaire de la mécanique ondulatoire (1956).
>With the old conceptions, a corpuscle (material point) must have at all times a well-determined position in space over time: under the influence of the force field to which it is subjected, it describes a certain curve of the space, its trajectory. […] The determinism of the old Mechanics consists in that, the initial state of position and speed being supposed to be known, the subsequent states are rigorously determined. […] We thus come to one of the essential ideas of the new Mechanics. While the old Mechanics attributed to its equations a rigorous character and considered them as always valid, the new Mechanics gives the wave the essential role: it no longer considers the equations of the old Mechanics only as approximations, valid only when the approximation of geometric Optics is sufficient to describe the propagation of the wave. Classical Dynamics therefore only appears as an approximation. […] Hence this long-known conclusion: if we make Planck’s quantum of action tend towards zero in formulas, all quantum effects must disappear and classical dynamics must regain all its rigor. […] Classical Physics implicitly supposed that one could, using a fairly fine experimental technique, indefinitely reduce the disturbance that a measurement operation can exert on the state of things existing before measurement so that at errors of experience, each measurement would accurately reflect the situation existing before and also after the measurement. Quantum Physics has realized that the existence of the quantum of action does not allow to indefinitely reduce the disturbance that a measurement produced in the previous situation: the minimum residual disturbance, which is insignificant on a large scale, can no longer be neglected on a small scale. […] The measurement of a quantity therefore does not generally reveal a situation existing before the measurement, but a situation fabricated in a way by the measurement. In general, it cannot be said that the measured value of the quantity already existed before the measurement.

The uncertainty principle is certainly one of the most famous aspects of quantum mechanics. It has often been regarded as the most distinctive feature in which quantum mechanics differs from classical theories of the physical world. Roughly speaking, the uncertainty principle (for position and momentum) states that one cannot assign exact simultaneous values to the position and momentum of a physical system. Rather, these quantities can only be determined with some characteristic “uncertainties” that cannot become arbitrarily small simultaneously.

In spite of the fact that Heisenberg’s and Bohr’s views on quantum mechanics are often lumped together as (part of) “the Copenhagen interpretation”, there is considerable difference between their views on the uncertainty relations. On the one hand, Bohr was quite enthusiastic about Heisenberg’s ideas which seemed to fit wonderfully with his own thinking. Indeed, in his subsequent work, Bohr always presented the uncertainty relations as the symbolic expression of his complementarity viewpoint. On the other hand, he criticized Heisenberg severely for his suggestion that these relations were due to discontinuous changes occurring during a measurement process. Rather, Bohr argued, their proper derivation should start from the indispensability of both particle and wave concepts. He pointed out that the uncertainties in the experiment did not exclusively arise from the discontinuities but also from the fact that in the experiment we need to take into account both the particle theory and the wave theory. It is not so much the unknown disturbance which renders the momentum of the electron uncertain but rather the fact that the position and the momentum of the electron cannot be simultaneously defined in this experiment.

Beller, Bohm and the “inevitability” of acausality (1996).
>Our intuitions revolt against thinking of a motion in which the position and velocity are both precisely determined, and are thus in close harmony with the uncertainty principle. By the use of photography, this becomes especially clear. Our simplest intuition of an object with a definite position is that the object is not moving. To obtain a picture of movement, we must allow the position to blur slightly. It is the blurred picture of the speeding car which suggests that some space was covered during some time, that the car is moving. If we use a very fast camera and will get a sharp picture of the moving car, we will not deduce motion. Our intuitive presupposition is that “a continuously moving object has a somewhat indefinite range of positions” – we simply “cannot visualize simultaneously a particle having a definite momentum and position” (Bohm 1951). Bohm concludes that “quantum theory… gives a picture of the process of motion that is considerably closer to our simplest concepts than does classical theory” (Bohm 1951). But does not our concept of an object at rest, with fixed position and zero velocity, contradict the uncertainty principle, according to which an object with a precise position has a completely indefinite momentum? Bohm argues that in fact there is no contradiction. When we think of an object at a definite position, we can imagine that in a short time later it will move to a different position. Which position it will move to we cannot infer from the original position – all positions are equally consistent with it. This means that any velocity, or any value of momentum, is equally consistent with a picture of a particle in a definite position. Put differently, “if we think of an object in a given position, we simply cannot think of its velocity at the same time” (Bohm 1951). We see that the classical idea of continuity of motion is an unnatural, non-commonsensical idea. On a purely logical basis, there is no reason to choose the concept of a continuous trajectory in preference to that of discontinuous trajectory. The idea of continuous motion developed through experience with planetary orbits, gun trajectories, and by the theory of differential calculus, aimed at dealing with such motions. It is only by studying “such things for a while” and getting used to them that the “succeeding generation began to take the basic ideas for granted” (Bohm 1951).

Hawking, A brief history of time (1988).
>Heisenberg demonstrated that the uncertainty of the position of the particle multiplied by the uncertainty of its speed multiplied by the mass of the particle can never be less than a certain quantity, which is called the “Planck constant”. In addition, this limit does not depend on how we try to measure the position or the speed of the particle, nor on its type: the Heisenberg uncertainty principle is an inevitable fundamental property of the world. The principle of uncertainty has had a profound impact on how we view the world. Even after more than fifty years, its implications have not been fully accepted by many philosophers and are still the subject of controversy. The principle of uncertainty indicates the end of Laplace’s dream of developing a theory of science and a model of the universe that would have been completely determined: how to predict future events with accuracy if one is not even able to measure the present state of the universe with precision!

Einstein, The world as I see it (1934).
>I still believe in the possibility of a model of reality, that is to say of a theory representing things themselves, and not just the probability of their existence. […] In a theoretical model we must totally abandon the idea of being able to rigorously locate the particles. I think this conclusion is in line with the enduring result of Heisenberg’s uncertainty relationship. But we could very well conceive an atomic theory in the strict sense (and not based on an interpretation), without localization of particles in a mathematical model. […] If such a representation of the atomic structure had proved to be exact, I would have considered the enigma of quanta completely solved.

A quantum fluctuation is the temporary change in the amount of energy in a point in space, and it is explained by the uncertainty principle. This allows the creation of particle-antiparticle pairs of virtual particles (a transient quantum fluctuation that exhibits some of the characteristics of an ordinary particle, while having its existence limited by the uncertainty principle). Quantum fluctuations may have been necessary in the origin of the structure of the universe: according to the model of expansive inflation the ones that existed when inflation began were amplified and formed the seed of all current observed structure. Hawking and Greene give more details.


Hawking, Brief answers to the big questions (2018).
>When the universe is big, there are a very large number of rolls of the dice, and the results average out to something one can predict. But when the universe is very small, near the Big Bang, there are only a small number of rolls of the dice, and the uncertainty principle is very important. In order to understand the origin of the universe, one therefore has to incorporate the uncertainty principle into Einstein’s general theory of relativity. This has been the great challenge in theoretical physics for at least the last thirty years. We haven’t solved it yet, but we have made a lot of progress. […] The irregularities you see are predicted by inflation, and they mean that some regions of the universe had a slightly higher density than others. The gravitational attraction of the extra density slows the expansion of that region, and can eventually cause it to collapse to form galaxies and stars. So look carefully at the map of the microwave sky. It is the blueprint for all the structure in the universe. We are the product of quantum fluctuations in the very early universe. God really does play dice.

Greene, The elegant universe (2000).
>Heisenberg’s uncertainty relationships state that such frantic exchanges of energy and impulse take place continuously, over distances or microscopic times. Even in an empty region of space – an empty box, for example – the Heisenberg principle states that energy and momentum are “uncertain”. They fluctuate between extreme values, the more distant the smaller the box size or the timescale. It’s a bit like the inside of the box is a “borrowing” zone, continually extorting “loans” from the Universe and then “repaying” it. But who are the protagonists of these exchanges in an empty region of space? All. Literally everything. Energy (as well as impulse) is the ultimate currency. E = mc²: energy can become matter and vice versa. Thus, a sufficiently large energy fluctuation could, for example, give birth to an electron and its anti-material companion, the positron, even if the space was initially empty! Since this energy must be quickly reimbursed, the two particles will annihilate each other after a moment, thus reimbursing the energy borrowed for their creation. The same goes for all energy and impulse transactions – creation and then annihilation of particles, crazy oscillations of the electromagnetic field, fluctuations of the fields of weak and strong forces. With quantum uncertainty, the Universe at the microscopic scale becomes a swarming and chaotic arena. Feynman liked to joke: “Create, annihilate; create, annihilate… what a waste of time!” On average, borrowing and repayment balance out, so that an empty region has all the appearances of tranquility until you see it with microscopic precision. The principle of uncertainty, however, reveals that the macroscopic average blurs the wealth of activity in the microscopic world.


### Matter in modern physics: waves and particles <a name="p46"></a>

Bohr long struggle with wave-particle duality had prepared him for a radical step when the dispute between matrix and wave mechanics broke out in 1926–27. For the main contestants, Heisenberg and Schrödinger, the issue at stake was which view could claim to provide a single coherent and universal framework for the description of the observational data. The choice was, essentially between a description in terms of continuously evolving waves, or else one of particles undergoing discontinuous quantum jumps. By contrast, Bohr insisted that elements from both views were equally valid and equally needed for an exhaustive description of the data. His way out of the contradiction was to renounce the idea that the pictures refer, in a literal one-to-one correspondence, to physical reality. Instead, the applicability of these pictures was to become dependent on the experimental context. This is the gist of the viewpoint he called “complementarity”. Many of the creators of quantum field theory can be found in one of the two camps regarding the question whether particles or fields should be given priority in understanding the theory. While Dirac, the later Heisenberg, Feynman, and Wheeler opted in favor of particles, Pauli, the early Heisenberg, Tomonaga and Schwinger put fields first.

Lindley, Introduction of Heisenberg’s Physics and philosophy (2007).
>In what became known as the old quantum theory, originating with Niels Bohr in 1913, atoms were pictured as little solar systems. Electrons orbited the small, massive nucleus strictly according to the principles of Newtonian mechanics. The quantum principle came into this model with the additional restriction that only certain orbits, out of the infinite range possible, were in fact permitted. When an electron jumped between orbits, the atom either took in or gave out a quantum of electromagnetic energy – later dubbed a photon – corresponding to the energy difference between the orbits. By the early 1920s, the old quantum theory, developed particularly by Arnold Sommerfeld in Munich, had become overelaborate and unwieldy, and at the same time failed to explain numerous subtleties of atomic spectra. It seemed possible that electrons in atoms moved according to rules fundamentally different from classical mechanics. Werner Heisenberg, then an undergraduate student with Sommerfeld in Munich, came to know this crisis intimately, and it was he, in 1925, who came up with its strange and startling resolution. “The idea suggested itself”, he tells us here, “that one should write down the mechanical laws not as equations for the positions and velocities of the electrons, but as equations for the frequencies and amplitudes of their Fourier expansion”. […] His equations gave sensible answers only when the energy of the electron took on one of a restricted set of values. Heisenberg, as he is too modest to say directly in these essays, had discovered the germ of quantum mechanics. Remarkably, as Paul Dirac and Pascual Jordan later established systematically, the laws of classical mechanics passed over wholly unchanged into the new system of quantum mechanics. What changed were the quantities – the seemingly basic elements of mechanics, such as a particle’s position and velocity – that those laws govern. And this is where the trouble begins. Two years later, in his celebrated Uncertainty Principle, Heisenberg went on to prove that in quantum mechanics position and velocity do not possess the straightforward, unambiguous meaning that they enjoy in classical mechanics. Rather than being elementary properties of some particle, position and velocity become in a sense secondary characteristics that the experimenter must derive from some quantum system by making a suitable measurement. And measurement is not the simple act it used to be. The better you measure the position of a particle, the less you can find out its velocity and vice versa: In such terms the Uncertainty Principle is often expressed. A more careful statement, however, is that quantum particles have no intrinsic properties that neatly correspond to position and velocity. Even to think, in fact, of a quantum particle is misleading, because the concept labeled “particle” has connotations that no longer fully apply. Just a few months after, Erwin Schrödinger came up with the equation that bears his name, which offers a different imagery. In the Schrödinger picture, an electron belonging to an atom takes the form of a spread-out stationary wave – a wave that represents the probability, roughly speaking, of an electron being found in this place or that around the nucleus. Is an electron a wave or a particle? The answer, as Heisenberg insists in these essays, is that the words “wave” and “particle” are formalized in classical mechanics by derivation from our everyday experience, and by definition are mutually exclusive. A wave can’t be a particle and particle can’t be a wave. A quantum object, in itself, is neither one thing nor the other. If you decide to measure a wave-like property (wavelength, for instance, in a diffraction or interference experiment), the thing you are observing will look like a wave. Measure a particle property (position or velocity), on the other hand, and you will see particle-like behavior. […] In stressing the inadequacy of either the wave or the particle picture alone, Heisenberg tells us that “by playing with both pictures, by going from one picture to the other and back again, we finally get the right impression of the strange kind of reality behind our atomic experiments”. The Copenhagen strategy for dealing with this impasse is to continue using the old language – waves and particles, positions and velocities – but on the strict understanding that the concepts embodied in these words are no longer primary, but come to use only through agency of observation and measurement. […] In the early 1950s David Bohm came up with a way of recasting quantum mechanics so that it supported, he claimed, a more conventional philosophy while not losing any of its empirical success. According to Bohm, a particle’s properties include “hidden variables”, inaccessible to the observer, that determine the outcome of measurements. The seeming unpredictability of quantum events then arises from our ignorance of these hidden variables. […] Heisenberg offers a variety of cogent reasons why Bohmian mechanics is not as appealing as it might seem, but his basic attitude is that the hidden variables approach buys an elusive and partial return to classical realism at the cost of destroying much of the mathematical elegance and symmetry of quantum mechanics in its pure form. In a word, Bohmian mechanics is ugly. Opposition to the Copenhagen view also came, as is well known, from Einstein, a lifelong subscriber to “hard realism”.

Bohr, Atomic physics and human knowledge (1957).
>The notion of complementarity severs to symbolize the fundamental limitation, met with in atomic physics, of the objective existence of phenomena independent of the means of their observation. […] Complementary in the sense that only the totality of the phenomena exhausts the possible information about the objects. […] The viewpoint of complementarity may be regarded as a rational generalization of the very ideal of causality.

[Werner Heisenberg](/references/wernerheisenberg.html)
>Actually we need not speak of particles at all. For many experiments it is more convenient to speak of matter waves; for instance, of stationary matter waves around the atomic nucleus. Such a description would directly contradict the other description if one does not pay attention to the limitations given by the uncertainty relations. Through the limitations the contradiction is avoided. The use of “matter waves” is convenient, for example, when dealing with the radiation emitted by the atom. By means of its frequencies and intensities the radiation gives information about the oscillating charge distribution in the atom, and there the wave picture comes much nearer to the truth than the particle picture. Therefore, Bohr advocated the use of both pictures, which he called “complementary” to each other. The two pictures are of course mutually exclusive, because a certain thing cannot at the same time be a particle (i.e., substance confined to a very small volume) and a wave (i.e., a field spread out over a large space), but the two complement each other. By playing with both pictures, by going from the one picture to the other and back again, we finally get the right impression of the strange kind of reality behind our atomic experiments. […] Modern physics takes a definite stand against the materialism of Democritus and for Plato and the Pythagoreans. The elementary particles are certainly not eternal and indestructible units of matter, they can actually be transformed into each other. As a matter of fact, if two such particles, moving through space with a very high kinetic energy, collide, then many new elementary particles may be created from the available energy and the old particles may have disappeared in the collision. Such events have been frequently observed and offer the best proof that all particles are made of the same substance: energy. But the resemblance of the modern views to those of Plato and the Pythagoreans can be carried somewhat further. The elementary particles in Plato’s Timaeus are finally not substance but mathematical forms. “All things are numbers” is a sentence attributed to Pythagoras. The only mathematical forms available at that time were such geometric forms as the regular solids or the triangles which form their surface. In modern quantum theory there can be no doubt that the elementary particles will finally also be mathematical forms, but of a much more complicated nature. The Greek philosophers thought of static forms and found them in the regular solids. Modern science, however, has from its beginning in the sixteenth and seventeenth centuries started from the dynamic problem. The constant element in physics since Newton is not a configuration or a geometrical form, but a dynamic law. The equation of motion holds at all times, it is in this sense eternal, whereas the geometrical forms, like the orbits, are changing. Therefore, the mathematical forms that represent the elementary particles will be solutions of some eternal law of motion for matter. Actually this is a problem which has not yet been solved. […] Why do the physicists claim that their elementary particles cannot be divided into smaller bits? The answer to this question clearly shows how much more abstract modern science is as compared to Greek philosophy. The argument runs like this: How could one divide an elementary particle? Certainly only by using extreme forces and very sharp tools. The only tools available are other elementary particles. Therefore, collisions between two elementary particles of extremely high energy would be the only processes by which the particles could eventually be divided. Actually they can be divided in such processes, sometimes into very many fragments; but the fragments are again elementary particles, not any smaller pieces of them, the masses of these fragments resulting from the very large kinetic energy of the two colliding particles. In other words, the transmutation of energy into matter makes it possible that the fragments of elementary particles are again the same elementary particles. […] Fifty years ago, when the theory of relativity was formulated, this hypothesis of the equivalence of mass and energy seemed to be a complete revolution in physics, and there was still very little experimental evidence for it. In our times we see in many experiments how elementary particles can be created from kinetic energy, and how such particles are annihilated to form radiation; therefore, the transmutation from energy into mass and vice versa suggests nothing unusual. […] The equivalence of mass and energy has – besides its great importance in physics – also raised problems concerning very old philosophical questions. It has been the thesis of several philosophical questions. It has been the thesis of several philosophical systems of the past that substance or matter cannot be destroyed. In modern physics, however, many experiments have shown that elementary particles, e.g., positrons and electrons, can be annihilated and transmuted into radiation. Does this mean that these older philosophical systems have been disproved by modern experience and that the arguments brought forward by the earlier systems have been misleading? This would certainly be a rash and unjustified conclusion, since the terms “substance” and “matter” in ancient or medieval philosophy cannot simply be identified with the term “mass” in modern physics. If one wished to express our modern experience in the language of older philosophies, one could consider mass and energy as two different form of the same “substance” and thereby keep the idea of substance as indestructible.

We can now explore the view of Bohmian mechanics on this matter, especially in terms of basic ontology.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>The electron is a particle with well-defined position and momentum that is, however, profoundly affected by a wave that always accompanies it. Far from being hidden, this particle is generally what is most directly manifested in an observation. The only point is that its properties cannot be observed with complete precision (within the limits set by the uncertainty principle). […] For Heisenberg and Bohr, the basic properties of the particle, i.e. its position and momentum, are not merely uncertain to us, but rather there is no way to give them a meaning beyond the limit set by Heisenberg’s principle. They inferred from this that there is, as we have already pointed out, an inherent ambiguity in the state of being of the particle. And this in turn implied that, at the quantum level of accuracy, there is no way to say what the electron is and it does, such concepts being applicable approximately only in the classical (correspondence) limit. […] The notion of a particle with permanent identity has now been replaced by that of a particle with no permanent identity. Rather its basic ‘elements’ are constantly forming and dissolving in succession.

Hiley, Process and the implicate order (2005).
>The underlying notion basic to this approach is that of activity, or as we put it, of movement. […] Here movement is not to be thought of as a movement of things. There are no things. Rather things, such as particles, objects, and indeed subjects, are regarded as semiautonomous quasi-local features of this underlying activity. This view is to be extended down not only to atoms but to their basic constituents, electrons, neutrons, protons and even down to the level of quarks. Thus objects take their form from the totality through a quasi-stable inner movement and can only be regarded as ‘independent’ objects at some approximate level, depending upon their stability. For example, one such criterion could arise when the action function is much greater than Planck’s constant. Thus the classical level emerges in processes where Planck’s constant of action can be neglected. Where it cannot be neglected, we have quantum phenomena. We see this quite clearly in quantum entangled states where some of the properties of the individual ‘particles’ are not well-defined. This feature was already recognized by Bohr who regarded it as an essential ambiguity at the heart of nature. But care must be taken not to assume that these quantum effects only appear at microscopic dimensions and cannot have relevance to mind. The stability of matter, a desk for example, owes its rigidity to quantum processes. The distribution of the radiation from the sun is determined by quantum processes. Thus quantum processes can and do determine macroscopic behaviour.

Bohmian mechanics or de Broglie-Bohm theory, is also known as the pilot wave theory (a particle is accompanied by a pilot wave). The evolution over time of the configuration (that is, the positions of all particles or the configuration of all fields) is defined by the wave function by a guiding equation. However, before the original paper of de Broglie in 1927, Einstein developed the concept of “ghost fields”. It occurred to him while trying to develop a mathematical theory of light which would fully encompass its wavelike and particle-like aspects. A guiding wave obeying Maxwell’s classical laws would propagate following the normal laws of optics, but would not transmit any energy. This guiding wave, however, would govern the appearance of quanta of energy on a statistical basis. These ideas became widely known in the physics community, and through Born’s work in 1926, later became a key concept in the modern quantum theory.

Born, Explained by Daumer in Scattering theory from a bohmian perspective (1996).
>We begin with a quote taken from Born (1926):
>>Neither of these two conceptions appear satisfactory to me. I should like to attempt here to give a third interpretation and to test its utility on collision processes. In this attempt, I adhere to an observation of Einstein on the relationship of wave field and light quanta; he said, for example, that the waves are present only to show the corpuscular light quanta the way, and he spoke in the sense of a “ghost field”. This determines the probability that a light quantum … takes a certain path; … And here it is obvious to regard the de Broglie-Schrödinger waves as the ghost field or, better, “guiding field”.

Bell, Speakable and unspeakable in quantum mechanics (1987).
>Consider now the de Broglie–Bohm version. To the question ‘wave or particle?’ they answer ‘wave and particle’. The wave is that of wave mechanics – but conceived, in the tradition of Maxwell and Einstein, as an objective field, and not just as some ‘ghost wave’ of information (of some presumably well-informed observer?). The particle rides along on the wave at some position with velocity. […] Since the pilot wave picture still needs advertising, I will make here another modest attempt to publicize it. It is easy to find good reasons for disliking the de Broglie-Bohm picture. Neither de Broglie nor Bohm liked it very much; for both of them it was only a point of departure. Einstein also did not like it very much. He found it ‘too cheap’, although, as Born remarked, ‘it was quite in line with his own ideas’. But like it or lump it, it is perfectly conclusive as a counter example to the idea that vagueness, subjectivity, or indeterminism, are forced on us by the experimental facts covered by nonrelativistic quantum mechanics.

The quantum potential is a central concept of the de Broglie-Bohm formulation of quantum mechanics. It is a term within the Schrödinger equation (equation that describes the quantum-mechanical system) which acts to guide the movement of quantum particle. It was later elaborated upon as an information potential which acts on a quantum particle. Indeed, Bohm made the following analogy: “The ship or aeroplane (with its automatic Pilot) is a self-active system, i.e. it has its own energy. But the form of its activity is determined by the information content concerning its environment that is carried by the radar waves. This is independent of the intensity of the waves. We can similarly regard the quantum potential as containing active information. It is potentially active everywhere, but actually active only where and when there is a particle”. Below, Hiley gives further detail. DGZ refers to a modification of Bohm’s theory by Detlef Dürr, Sheldon Goldstein and Nino Zanghi in 1992.

Hiley, From the Heisenberg picture to Bohm (2002).
>Thus equation (3) can be regarded as a generalized expression for the conservation of energy provided we regard Q as a new form of potential energy which is negligible in the classical world and is apparent only in quantum systems. This energy has traditionally been called the quantum potential energy. It should not be thought as the source of some mysterious new force to be put into the Newtonian equations of motion. Rather it appears as a necessary consequence of the symplectic symmetry. […] Let me emphasise that what we have done above is very straightforward. Equation (3) is a direct result of the Schrödinger equation. Nothing mathematically new has been added. What we then do is to look at the extra term Q and ask “What is the physical reason that Q appears and what could it possibly represent?” Is this not a standard procedure in physics? […] This quantum potential energy must be taken seriously. Indeed without it energy would not be conserved and without understanding this energy the form of the trajectories are just another quantum mystery. All of this is in contrast to DGZ. They feel the quantum potential is somehow artificial and should be avoided. In some sense they are implicitly following on from Heisenberg (1958) who argued that the approach needed “some strange quantum potentials introduced ad hoc by Bohm”. There is no elaboration as to why he thought it was ad hoc and I continue to find it difficult to understand how a term that arises naturally from a basic equation is called ad hoc? […] DGZ are not alone in rejecting features of the approach that seem too strange. Others like Scully (1998) and Aharonov and Vaidman (1996) have used similar dismissive arguments, this time against the trajectories themselves. The trajectories are called ‘surreal’ apparently because the trajectories do not conform to what we would expect from classical grounds. We have shown in detail that their criticisms cannot be sustained (Hiley, Callaghan and Maroney 2001), but we must stress again that the appearance of apparently ‘bizarre’ behaviour is no reason for ignoring or even dismissing it. Quantum phenomena are ‘bizarre’ when compared with our intuitive ideas based on classical theories. I personally would be very surprised if the trajectories and the quantum potential did have classical properties. Quantum theory is a “highly non-classical theory” and we should expect surprises, and we should use all available approaches to explore in exactly what ways the behaviour is non-classical. […] In classical physics a potential is a force field generated by an outside agency and describes the potential evolution of the particle starting from a given position. As particles in different positions experience different effects we can think of the potential as being revealed through the movements of an ensemble of particles. […] How do we think about the quantum potential? It describes a field of energy so can it be regarded as producing a force on the particle? There are some problems with this view. Firstly, as we have already remarked above, the quantum potential has no external source so that there is nothing for the particle to ‘push against’. The energy is internal so clearly there is something more subtle involved. Here it is more like the role the gravitational field plays in general relativity where the gravitational energy curves space-time itself. […] The potential is no longer proportional to the amplitude of a ‘wave’ as one would expect from classical physics. Because of this the potential gives rise to effects that are totally different from those expected from a classical wave. In the classical case the force produced by a wave is directly proportional to its amplitude as any swimmer will know from direct experience. […] The quantum potential can produce strong long-range effects that do not necessarily fall off with distance. These are the typical properties of entangled wave functions. Thus even though the wave function spreads out, the effect of the quantum potential need not necessarily decrease. This is of course just the type of behaviour required for an explanation of the EPR paradox. If we examine the expression of the quantum potential in, say, the two-slit experiment, we find that it depends on the width of the slits, their distance apart and the momentum of the particle. In other words it contains information about the overall experimental arrangement. Of course this is not mathematically at all surprising because the wave function is a solution of the Schrödinger equation, which must necessarily reflect the boundary conditions. But we want to suggest that we can think of the process in terms of a local particle being fed this information locally through the information contained in the potential field as the particle evolves along its path.

Abner Shimony and Millard Baublitz explain the separation in Bohmian mechanics into the supporters of the guidance and causal views. The guidance view concerns the acceptation of a guiding principle that has been better accepted than the causal view, which regards the acceptation of the quantum potential. When Bohm developed his original ideas, he called them a Causal Interpretation. Later he felt that causal sounded too much like deterministic and preferred to call his theory the Ontological Interpretation.

Shimony and Baublitz, Tension in Bohm’s interpretation of quantum mechanics (1996).
>Two different physical views coexist uneasily in Bohm’s initial theory, which we shall call “the causal view” and “the guidance view”. […] What makes Bohm’s initial theory attractive philosophically and scientifically is a conjunction of some features of the causal view and some of the guidance view. […] Some of the most influential advocates of Bohmian mechanics – including Dürr (1992), Valentini (1991), and Cushing (1994) – treat the tension between the two views in the initial theory of Bohm by surgery, for they retain the guidance view and discard whatever of the causal view is incompatible with it. […] Unequivocally there are advocates of the guidance view, notably Bell (1987), Dürr, Goldstein and Zanghi (DGZ) (1992), Valentini (1991) and Cushing (1994). […] To our knowledge, there is no follower of Bohm who advocates the causal view in its purity. This fact does not imply, however, that the causal view is uninteresting, because it is an ingredient in the overall theory of Bohm and Hiley (1993), Dewdney (1987), Holland (1993), Kyprianidis (1988), and perhaps others. […] The greatest strength of the causal view is that it subsumes quantum mechanics under classical mechanics, once the innovation of the quantum potential is accepted. […] The indefiniteness of physical quantities, which the orthodox interpretation of quantum mechanics requires us to accept, is removed by this subsumption under classical mechanics, since a definite position and a definite velocity are attributed to each particle at every time. The causal view clarifies the status of the wave function by making it objective and physically real, an entity which acts upon particles by a force law formulated in the quantum mechanical potential, analogous to the way the electromagnetic field acts upon a particle by the Lorentz force law. The “guidance” of a particle by the wave function is, in principle, no more mysterious than the guidance of a charged particle by the electric and magnetic fields. In both cases there is no direct determination of the velocity of the particle by the force, but rather a determination of an acceleration. The greatest weakness of the causal view is its difficulty in recovering the Born rule. […] In both the causal and guidance views, the positions of the particles do not affect the temporal evolution of the wave function. There is a causal asymmetry, in that the wave affects the particles, but the particles are causally irrelevant to the wave. This asymmetry is troublesome. […] In sum, the quantum-mechanical account of the measuring process is mysterious but robust, while the causal view offers a theory of measurement that is not mysterious but also not robust.

Finally, an explanation of the guidance condition is also presented by Fine who gave an in-depth analysis of possible interpretations of Bohm’s model of 1952.

Fine, On the interpretation of Bohmian mechanics (1996).
>The guidance condition associates velocities with the wave function. That condition insures that the classical dynamical variables have pre-measurement values outside their spectrum and therefore values that could not possibly be disclosed by measurement (at least if observational equivalence with quantum mechanics is to be maintained). Moreover, by making the velocities a function of the wave function in coordinate representation, initial data on particle positions is required in order to fix the velocities. Unlike dispositions, the velocities are already there in pre-measurement situations, but hidden. The question naturally arises as to whether they are (or ought to be considered) “real”, in the usual sense. If reality without disclosure is truly blind, perhaps it would be better to treat Bohmian velocity, as what Vaihinger (1925) called a “scientific fiction”; that is, an expedient means to certain ends; namely, a means for calculating particle trajectories and positions. Born’s original picture comes to mind here. “[T]he guiding field, represented by a scalar function of the coordinates of all the particles involved and the time, propagates in accordance with Schrödinger’s differential equation. Momentum and energy, however, are transferred as if the corpuscles… actually moved”. (Born 1926; Ludwig 1968. Emphasis mine.) Taking Born’s “as if” seriously would interpret the dynamical variables as calculational tools. […] At the heart of Bohmian mechanics is the wave function and determinate particle positions, and perhaps we need to be realist about nothing else. In this interpretation the wave function does not actually guide the moving particles. It just assigns particle positions and shifting distributions. Thus one might regard as a kind of global hidden hand with just these functions. […] Maybe reality consists of a unitary global X that organizes particle aspects everywhere and at every moment in accord with a changing wave aspect.


### Wave function and Schrödinger equation <a name="p47"></a>

After having briefly introduced the wave function and the Schrödinger equation, it is now time to give detailed information in order to understand the coming sections. Both notions are of high importance if one wants to build an ontological theory. The wave function’s most common symbol is Ψ (psi). It is a mathematical description of the quantum state of an isolated quantum system. The wave function is a complex-valued probability amplitude, and the probabilities for the possible results of measurements made on the system can be derived from it. The Schrödinger equation determines how wave functions evolve over time, and a wave function behaves qualitatively like other waves, such as water waves or waves on a string, because the Schrödinger equation is mathematically a type of wave equation. This explains the name “wave function”, and gives rise to wave-particle duality. However, the wave function in quantum mechanics describes a kind of physical phenomenon, still open to different interpretations, which fundamentally differs from that of classic mechanical waves. The following authors give descriptions of what the wave function “really” is. As one can see, there is no consensus.

Hawking, Brief answers to the big questions (2018).
>In quantum mechanics, particles don’t have well-defined positions and speeds. Instead, they are represented by what is called a wave function. This is a number at each point of space. The size of the wave function gives the probability that the particle will be found in that position. The rate at which the wave function varies from point to point gives the speed of the particle. One can have wave function that is very strongly peaked in a small region. This will mean that the uncertainty in the position is small. But the wave function will vary very rapidly near the peak, up on one side and down on the other. Thus the uncertainty in the speed will be large. Similarly, one can have wave functions where the uncertainty in the speed is small but the uncertainty in the position is large. The wave function contains all that one can know of the particle, both its position and its speed. If you know the wave function at one time, then its values at other times are determined by what is called the Schrödinger equation. Thus one still has a kind of determinism, but it is not the sort that Laplace envisaged. Instead of being able to predict the positions and speed of particles, all we can predict is the wave function. This means that we can predict just half what we could according to the classical nineteenth-century view. Although quantum mechanics leads to uncertainty when we try to predict both the position and the speed, it still allows us to predict, with certainty, one combination of position and speed. 

One of the main difficulties of the common interpretation of quantum physics that Bohm underlined was the “collapse of the wave function”. It is also called the “reduction of wave packets”. As Heisenberg defines it, it is “the fact that the wave function or, more generally, the probability function changes discontinuously when the observer takes cognizance of a result of measurement”. Wave function collapse occurs when a wave function – initially in a superposition of several eigenstates – reduces to a single eigenstate due to interaction with the external world. This interaction is called an “observation”. It is the essence of a measurement in quantum mechanics which connects the wave function with classical observables like position and momentum. If collapse were a fundamental physical phenomenon, rather than just the epiphenomenon of some other process, it would mean nature was fundamentally stochastic, i.e. nondeterministic, an undesirable property for a theory. This issue remained until quantum decoherence entered mainstream opinion after its reformulation in the 1980s. 

Wheeler regards the wave function as representing information in the mind of the observer, i.e. a measure of our knowledge of reality. However, Schrödinger, Bohm and Everett argued that the wave function must have an objective, physical existence. Einstein thought that a complete description of physical reality should refer directly to physical space and time, as distinct from the wave function, which refers to an abstract mathematical space. Before an understanding of decoherence was developed, the Copenhagen interpretation of quantum mechanics treated wave-function collapse as a fundamental, a priori process. Decoherence provides an explanatory mechanism for the appearance of wave function collapse and was first developed by David Bohm in 1952. Decoherence was then used by Hugh Everett in 1957 to form the core of his many-worlds interpretation. However, decoherence was largely ignored for many years (with the exception of Zeh’s work), and not until the 1980s did decoherent-based explanations of the appearance of wave-function collapse become popular. Some versions of the Copenhagen interpretation have been modified to include decoherence. Decoherence does not claim to provide a mechanism for the actual wave-function collapse; rather it puts forth a reasonable mechanism for the appearance of wave-function collapse. The quantum nature of the system is simply “leaked” into the environment so that a total superposition of the wave function still exists, but exists – at least for all practical purposes – beyond the realm of measurement. Of course, by definition, the claim that a merged but unmeasurable wave function still exists cannot be proven experimentally. Decoherence explains why a quantum system begins to obey classical probability rules after interacting with its environment.

Albert, Elementary quantum metaphysics (1996).
>The sorts of physical objects that wave functions are, on this way of thinking, are (plainly) fields – which is to say that they are the sorts of objects whose states one specifies by specifying the values of some set of numbers at every point in the space where they live, the sorts of objects whose states one specifies (in this case) by specifying the values of two numbers (one of which is usually referred to as an amplitude, and the other as a phase) at every point in the universe’s so-called configuration space. […] The values of the amplitude and the phase are thought of (as with all fields) as intrinsic properties of the points in the configuration space with which they are associated. […] On Bohm’s theory, for example, the world will consist of exactly two physical objects. One of those is the universal wave function and the other is the universal particle. And the story of the world consists, in its entirety, of a continuous succession of changes of the shape of the former and a continuous succession of changes in the position of the latter. And the dynamical laws that govern all those changes – that is: the Schrödinger equation and the Bohmian guidance condition – are completely deterministic, and (in the high-dimensional space in which these objects live) completely local. […] On the GRW theory (or for that matter on any theory of collapse), the world will consist of exactly one physical object – the universal wave function. What happens, all that happens, is that that function changes its shape in accord with the theory’s dynamical laws. And those changes are not entirely continuous, and the laws which govern them are not entirely deterministic.

Whether the wave function really exists, and what it represents, are major questions in the interpretation of quantum mechanics. Many famous physicists of a previous generation puzzled over this problem, such as Schrödinger, Einstein and Bohr. 

Kuttner and Rosenblum, Quantum enigma, physics encounters consciousness (2011).
>In quantum theory there is no atom in addition to the wavefunction of the atom. This is so crucial that we say it again in other words. The atom’s wave-functions and the atom are the same thing; “the wave function of the atom” is a synonym for “the atom”. Since the wavefunction is synonymous with the atom itself, the atom is simultaneously in both boxes. The point of that last paragraph is hard to accept. That is why we keep repeating it.


Nauenberg, Does quantum mechanics require a conscious observer? (2011).
>If the wavefunction is a physical object like an atom, then the proponents of this flawed concept must require the existence of a mechanism that lies outside the principles governing the time evolution of the wavefunction in order to account for the so-called “collapse” of the wavefunction after a measurement has been performed. But the wavefunction is not a physical object like, for example, an atom which has an observable mass, charge and spin as well as internal degrees of freedom. Instead, the wavefunction is an abstract mathematical function that contains all the statistical information that an observer can obtain from measurements of a given system. In this case there isn’t any mystery that its mathematical form must change abruptly after a measurement has been performed. […] The surprising fact that mathematical abstractions can explain and predict real physical phenomena has been emphasised by Wigner (Wigner 1960), who wrote: “The miracle of appropriateness of the language of mathematics for the formulation of the laws of physics is a wonderful gift which we neither understand nor deserve”.

Wigner, Remarks on the mind-body question (1995).
>The information given by the wave function is communicable. If someone else somehow determines the wave function of a system, he can tell me about it and, according to the theory, the probabilities for the possible different impressions (or “sensations”) will be equally large, no matter whether he or I interact with the system in a given fashion. In this sense, the wave function “exists”.

Fine, On the interpretation of Bohmian mechanics (1996).
>Beyond the wave-particle duality, in Bohmian mechanics the wave function itself acquires a multiple personality. In one guise it behaves as what Einstein dubbed a Führungsfeld (guiding field), directing the particles along trajectories determined by solutions to the velocity equations. Paraphrasing Einstein, this is the role to which Born called attention in his second fundamental paper on collision processes (“the waves are present only to show the corpuscular light quanta the way” (1926). In satisfying Schrödinger’s equation, however, the wave function also assumes the role of a true Gespensterfeld (spook field), acting as a “spook” or secret agent that reflects and transmits salient information about the environment, including measurement interactions. The “spooky” action-at-a-distance of ordinary quantum mechanics derives from this latter role.

De Broglie explains that Bohm “took up” his pilot-wave theory and gave the wave function a “physical reality”. De Broglie finds this point of view “inadmissible” yet he finds connections between Bohm’s theory and his own double-solution theory (he considers an additional wave that interacts with the Ψ-wave), an improvement of the pilot-wave theory. All of them are called by de Broglie “causal theory” and he explains what the objective reality in that sense means. We cannot access it but it exists.

de Broglie
>[Une tentative d’interprétation causale et non linéaire de la mécanique ondulatoire (1956)] The crucial question is then to know whether the probabilistic interpretation of the Ψ wave which certainly leads to exact forecasts constitutes a “complete” representation beyond which there is no reason to seek an objective description of the reality or if, on the contrary, the description of phenomena using the single wave Ψ is “incomplete” and must give way to a deeper and more detailed description of physical reality. […] Each time a new observation brings us new information about the corpuscle, the shape of the wave Ψ is modified. This can only be understood if the wave Ψ is only a representation of an objective reality. As Mr. Schrödinger said recently, the Ψ wave has something “psychological” about it. […] In the summer of 1951, to my great surprise, I became aware of a Memoir by David Bohm which then appeared in the Physical Review. In this Memoir, Mr. Bohm repeated my theory of the pilot wave by considering the Ψ wave as a physical reality. He developed a number of interesting remarks on this subject and, in particular, sketched a theory of measurement which seemed to dispel the objections which M. Pauli had addressed to me in 1927. […] Mr. Takabayasi has also taken up these objections in Memoirs in which he has specified in an interesting way certain aspects of Bohm’s theory. […] The principle of Mr. Bohm’s demonstration is to note that the Ψ wave of a corpuscle or system is always slightly disturbed by the existence of small external actions (for example weak collisions). […] In reality, there are always small random external actions and small random fluctuations in the boundary conditions: it is them which, by causing a kind of Brownian movement of the corpuscle (or of the representative point) which makes it constantly jumping from one undisturbed movement to another, ensure the realization of the density of presence in |Ψ|². We thus manage, while preserving the average physical significance of the trajectories predicted by the causal theory, to superimpose a kind of Brownian motion on them. It is curious to observe that this way we would synthesize the conceptions of causal theory with the assertion repeatedly repeated by Mr. Einstein that the success of the statistical interpretation of wave mechanics implies the existence of movements underlying corpuscles of Brownian character. […] Mr. Bohm shows by calculation that the quantum potential, energy and momentum of the corpuscle then fluctuate constantly and very quickly and he adds: “If the particle happens to reach a region of space where the amplitude of the Ψ wave is small, these fluctuations will become quite violent. So we see that in general, the motion of the particle in a non-stationary state is very irregular and complicated, much more like Brownian motion than the regular orbit of a planet around the Sun”. […] Mr. Bohm first analyzed collision phenomena in causal theory. […] Since the corpuscles always keep a position in physical space, all the difficulties of the current interpretation relating to correlated systems are overcome. Causality will also be restored because the final result will depend on the initial positions of the corpuscles; but in practice the trajectories of these particles will be extremely complicated and rapidly variable with the initial positions and their exact calculation will be impossible. Statistics will therefore be introduced, but only in the classical way: its intervention will result both from the impossibility of determining exactly the initial positions without disturbing the whole phenomenon and from our inability to follow the trajectories while, in the interpretation at present, we admit that nothing determines the result of the interaction and we assume a priori that the description of the phenomena is intrinsically and inevitably statistical.
>
>[The reinterpretation of wave mechanics (1970)] The theory of the double solution should naturally lead to the introduction of some thermodynamic considerations in wave mechanics. […] The thermodynamic conception of the particle leads us to think that even when it seems to us that a particle is isolated from all macroscopic bodies capable of exchanging heat with the particle, it is constantly in thermal contact with a kind of thermostat hidden in what we call the vacuum. […] Any attempt to establish the exact nature of this hidden thermostat seems premature, but it appears related to the “subquantum level” proposed by Bohm and Vigier fifteen years ago, or at least to a part of this subquantum level. During its guidance movement, the mass of the particle generally varies. We must interpret this phenomenon by saying that it exchanges heat with the hidden thermostat. The heat exchanges are linked to the variations of the quantum potential, that is, to the variations of the wave amplitude at the point where the particle is found; one sees that the wave acts as an intermediary between the particle and the hidden thermostat. It is normal to suppose that a particle is a very simple system, and because of this simplicity it is preferable not to attribute to it a proper temperature and entropy. The hidden thermostat, on the contrary, whatever its real nature, must be a very complex system which permits us to attribute an entropy and an apparent temperature to the particle.

Bohm and Hiley give their interpretation of the wave-function and about its existence. They also develop the difference between their ideas and those of de Broglie.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>An intermediate approach adopted by Heisenberg. Though, this point of view was indeed proposed earlier by Bohm. He suggested that the wave function represented, not an actual reality, but rather a set of potentialities that could be realised according to the experimental conditions. A helpful analogy may be obtained by considering a seed, which is evidently not an actual plant, but which determines potentialities for realizing various possible forms of the plant according to conditions of soil, rain, sunlight, wind, etc. Thus when the measurement of a given observable was repeated, this would correspond to a plant producing a seed, which growing under the same conditions, produced the same form of plant again (so that there was no continuously existent plant). Measurement of another observable would correspond to changing the experimental conditions, and this could produce a statistical range of possible plants of different forms. Returning to the quantum theory, it is clear that in this approach the apparatus is regarded as actually helping to ‘create’ the observed results. It must be emphasized, however, that Bohr specifically rejected this suggestion which he probably felt gave too much independent reality to whatever is supposed to be represented by the wave function. […] [von Neumann] concluded that the wave function yielded the most complete possible description of what we have been calling quantum reality, which is thus totally contained in the concept of a quantum state. […] Difficulty arises in essence because von Neumann introduced the basically ontological notion that the wave function represents a quantum state that somehow ‘stands on its own’ (although, of course, in interaction with the classical level). Bohr avoids this problem by never speaking of a quantum object that could stand on its own, but rather by speaking only of a phenomenon which is an unanalyzable whole. The question of interaction between a quantum level and a classical level thus cannot arise. Therefore in this sense, he is more consistent than von Neumann. […] Von Neumann based this belief on his theorem to which we have alluded earlier, that claimed to show that a more detailed description would not be compatible with the laws of the quantum theory. This proof was however questioned by Bohm in 1952 and later by Bell. A number of those who followed along von Neumann’s lines, refined his arguments in several ways, but these refinements were also shown by Bell to make tacit assumptions about ontological theories that are too limited. […] The idea of a ‘pilot wave’ that guides the movement of the electron was first suggested by de Broglie in 1927, but only in connection with the one body system. De Broglie presented this idea at the 1927 Solvay Congress where it was strongly criticized by Pauli. His most important criticism was that, in a two-body scattering process, the model could not be applied coherently. In consequence de Broglie abandoned his suggestion. The idea of a pilot wave was proposed again in 1952 by Bohm in which an interpretation for the many-body system was given. This latter made it possible to answer Pauli’s criticism and indeed opened the way to a coherent interpretation including a theory of measurement which was applicable over a wide range of quantum phenomena. As a result de Broglie took up his original ideas again and continued to develop them in various ways. An important part of de Broglie’s early approach was to try to explain the assumptions underlying the pilot wave interpretation in terms of what he called the theory of the ‘double solution’. This was based on the assumption of a non-linear field equation which, in the linear approximation, approached the ordinary Schrödinger equation. However, for large amplitude, the non-linearity became important. He suggested that there would exist solutions which would correspond to a stable singularity or pulse when the amplitude was high and would gradually shade off into solutions of the linear Schrödinger equation at larger distances. The pulse would evidently correspond to a particle. […] However, a closer analysis shows that this is actually only a necessary condition and not a sufficient one. Indeed we can see that it cannot be sufficient by considering the fact that energy and momentum conservation are necessary consequences of the kinds of equations discussed by de Broglie. The momentum in the singularity will be very large in comparison with that available in the extremely weak pilot wave. Therefore it will not be possible to obtain solutions of the field equations which would lead to the very great accelerations that are in general implied by the guidance relation (e.g. as seen in our discussion of the two slit interference experiment). Rather, we have seen that to obtain a powerful effect from a very weak field we need something like our concept of active information. For the phase clearly depends only on the form of the field and not on the amplitude. In our approach, it is this form which ‘in-forms’ the energy of the self¬-movement of the particle. Therefore the key difference of our idea from that of de Broglie is that we do not attempt to explain the guidance relation in a simple mechanical way as an effect of non-linear propagation of fields. Instead we are appealing to the notion that a particle has a rich and complex inner structure which can respond to information and direct its self-motion accordingly. […] The relationship between parts of a system described above implied a new quality of wholeness of the entire system going beyond anything that can be specified solely in terms of the actual spatial relationships of all the particles. […] In our interpretation of the quantum theory, we see that the interaction of parts is determined by something that cannot be described solely in terms of these parts and their preassigned interrelationships. Rather it depends on the many-body wave function (which, in the usual interpretation, is said to determine the quantum state of the system). This many-body wave function evolves according to Schrödinger’s equation. Something with this kind of dynamical significance that refers directly to the whole system is thus playing a key role in the theory. […] We may say that while the basic law refers inseparably to the whole universe, this law is such as to imply that the universe tends to fall into a large number of relatively independent parts, each of which may, however, be constituted of further sub-units that are nonlocally connected. Therefore we can deal with these relatively independent parts in the traditional way as we do our experiments. […] In our interpretation we do not assume that the basic reality is described primarily by the wave function. Rather we begin with the assumption that there are particles following definite trajectories. We then assume that the wave function describes a qualitatively new kind of quantum field which determines the guidance conditions and the quantum potential acting on the particle. We are not denying the reality of this field, but we are saying that its significance is relatively subtle in the sense that it contains active information that ‘guides’ the particle in its self-movement under its own energy. The behaviour of the particles is profoundly affected by this field so that it has fundamentally new features relative to those of classical physics. The effects of the quantum field will, however, become manifest only at the classical level in the collective movement of large numbers of particles.
 
### Information: Young’s double-slits experiments <a name="p48"></a>

The double-slit experiment is a demonstration that light and matter can display characteristics of both classically defined waves and particles; moreover, it displays the fundamentally probabilistic nature of quantum mechanical phenomena. The experiment was first performed with light by Thomas Young (1773-1829, English) in 1801. In 1927, Clinton Davisson (1881-1958, American) and Lester Germer (1896-1971, American) demonstrated that electrons show the same behavior, which was later extended to atoms and molecules.

According to Richard Feynman, the two-slit experiment for electrons is “a phenomenon which is impossible, absolutely impossible, to explain in any classical way, and which has in it the heart of quantum mechanics. In reality it contains the only mystery”. This experiment “has been designed to contain all of the mystery of quantum mechanics, to put you up against the paradoxes and mysteries and peculiarities of nature one hundred per cent”. As to the question, “How does it really work? What machinery is actually producing this thing? Nobody knows any machinery. Nobody can give you a deeper explanation of this phenomenon than I have given; that is, a description of it”.

In this experiment, a beam of particles (such as electrons) travels through a barrier that has two slits. If one puts a detector screen on the side beyond the barrier, the pattern of detected particles shows interference fringes characteristic of waves arriving at the screen from two sources (the two slits); however, the interference pattern is made up of individual dots corresponding to particles that had arrived on the screen. The system seems to exhibit the behaviour of both waves (interference patterns) and particles (dots on the screen).

If we modify this experiment so that one slit is closed, no interference pattern is observed. Thus, the state of both slits affects the final results. We can also arrange to have a minimally invasive detector at one of the slits to detect which slit the particle went through. When we do that, the interference pattern disappears. The Copenhagen interpretation states that the particles are not localized in space until they are detected, so that, if there is no detector on the slits, there is no information about which slit the particle has passed through. If one slit has a detector on it, then the wavefunction collapses due to that detection. This dramatic effect of observation can be understood as, in fact, a simple consequence of Bohmian mechanics. To see this, one must consider the meaning of determining the slit through which the particle passes. This must involve interaction with another system that the Bohmian mechanical analysis must include.

In the de Broglie-Bohm theory, the wavefunction is defined at both slits, but each particle has a well-defined trajectory that passes through exactly one of the slits. The final position of the particle on the detector screen and the slit through which the particle passes is determined by the initial position of the particle. Such initial position is not knowable or controllable by the experimenter, so there is an appearance of randomness in the pattern of detection. In 1952, Bohm used the wavefunction to construct a quantum potential that, when included in Newton’s equations, gave the trajectories of the particles streaming through the two slits. In particular, when a particle is sent into a two-slit apparatus, the slit through which it passes and its location upon arrival on the photographic plate are completely determined by its initial position and wave function. While each trajectory passes through only one slit, the wave passes through both; the interference profile that therefore develops in the wave generates a similar pattern in the trajectories guided by the wave. In effect the wavefunction interferes with itself and guides the particles by the quantum potential in such a way that the particles avoid the regions in which the interference is destructive and are attracted to the regions in which the interference is constructive, resulting in the interference pattern on the detector screen.

An experiment was conducted in 2016 which demonstrated the potential validity of the de Broglie-Bohm theory via use of silicone oil droplets. In this experiment a drop of silicone oil is placed into a vibrating fluid bath, it then bounces across the bath propelled by waves produced by its own collisions, mimicking an electron’s statistical behavior with remarkable accuracy.

de Broglie, Une tentative d’interprétation causale et non linéaire de la mécanique ondulatoire (1956).
>As I had noticed, 25 years ago, we are brought back to a very old intuitive interpretation of diffraction phenomena, the same one that had been supported in the past, from Newton to Biot and Laplace, the supporters of ancient corpuscular theory of light. “If, they said, the light is deflected when it passes near the edge of a screen, it is because the edge of the screen exerts on the corpuscle of light a force which deviates it from its normally rectilinear course”. With the notion of quantum potential, we can say in a similar way: “If the light is diffracted by the edge of a screen, it is because the wave of the photon is hindered in its propagation by the edge of the screen and that a reaction on the movement of the photon results: this reaction is expressed by the intervention of the quantum potential and has the effect of bending the trajectory of the photon”. This interpretation, very attractive by its concrete nature, would naturally also be valid for the diffraction of a corpuscle other than a photon, for example for the diffraction of an electron by the edge of a screen (Börsch phenomenon).

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>We may consider a ship on automatic pilot being guided by radio waves. Here, too, the effect of the radio waves is independent of their intensity and depends only on their form. The essential point is that the ship is moving with its own energy, and that the form of the radio waves is taken up to direct the much greater energy of the ship. We may therefore propose than an electron too moves under its own energy, and that the form of the quantum wave directs the energy of the electron. This introduces several features into the movement. First of all, it means that particles moving in empty space under the action of no classical forces need not travel uniformly in straight lines. This is a radical departure from classical Newtonian theory. Moreover, since the effect of the wave does not necessarily fall off with the distance, even remote features of the environment can profoundly affect the movement. […] In this explanation of the quantum properties of the electron, the fact that the quantum potential depends only on the form and not on the amplitude of the quantum field is evidently of crucial significance. As we have already suggested, although at first sight such behaviour seems to be totally outside of our common experience, a little reflection shows that this is not so. Effects of this kind are indeed frequently encountered in ordinary experience wherever we are dealing with information. Thus in the example of the ship guided by radio waves, one may say that these waves carry information about what is in the environment of the ship and that this information enters into the movements of the ship through its being taken up in the mechanism of the automatic pilot. Similarly we explain the interference properties by saying that the quantum field contains information, for example about the slits, and that this information is taken up in the movements of the particle. In effect, we have in this way introduced a concept that is new in the context of physics – a concept that we shall call active information. The basic idea of active information is that a form having very little energy enters into and directs a much greater energy. The activity of the latter is in this way given a form similar to that of the smaller energy. It is important to distinguish our concept of active information from the more technical definition of information commonly adopted in physics in terms of, for example, Shannon’s ideas implying that there is a quantitative measure of information that represents the way in which the state of a system is uncertain to us (e.g. that we can only specify probabilities of various states). […] We are calling attention to the literal meaning of the word, i.e. to in-form, which is actively to put form into something or to imbue something with form. As a simple example of what we mean, consider a radio wave whose form carries a signal. The sound energy we here in the radio does not come directly from the radio wave itself which is too weak to be detected by our senses. It comes from the power plug or batteries which provide an essentially unformed energy that can be given form (i.e., in-formed) by the pattern carried by the radio wave. This process is evidently entirely objective and has nothing to do with our knowing the details of how this happens. The information in the radio wave is potentially active everywhere, but it is actually active, only where and when it can give form to the electrical energy which, in this case, is in the radio. An example that does not involve structures set up by human beings is the function of the DNA molecule. The DNA is said to constitute a code, that is to say, a language. The form of the DNA molecule is considered as information content for this code, while the ‘meaning’ is expressed in terms of various processes; e.g. those involving RNA molecules, which ‘read’ the DNA code, and carry out the protein-making activities that are implied by particular sections of the DNA molecule. The comparison to our notion of objective and active information is very close. Thus, in the process of cell growth it is only the form of the DNA molecule that counts, while the energy is supplied by the rest of the cell (and indeed ultimately by the environment as a whole). Moreover, at any moment, only a part of the DNA molecule is being ‘read’ and giving rise to activity. The rest is potentially active and may become actually active according to the total situation in which the cell finds itself. […] While the information in an ‘empty’ channel may not actually be active on the particle at a given moment, it may have the potentiality to become active later. Therefore one cannot say that the transmitted particle is in a state that is entirely independent of the reflected channel. […] The transition process can be treated ontologically, i.e. as having no essential relationship to measurement or observation. We showed that in such a transition the wave function effectively splits so as to define a number of separate channels. The quantum potential was determined only by the channel that the particle actually enters. The channel entered by the particle depends on the initial conditions which are distributed statistically in an ensemble. From this it follows that the probability of obtaining a certain result is the same as in the usual interpretation. In effect, everything happens as if the wave function ‘collapses’ to the final result, while no collapse actually takes place. The channels not occupied by the particles correspond to inactive information. We showed that if the particles make collisions with additional systems, there is a constant narrowing down of the domain in which the wave function can significantly affect the quantum potential at the location of the particles. From this is follows that the channel entered by the particles eventually becomes irreversibly fixed, so that it cannot be undone. […] We discuss the process of formation and dissolution of wholes (fission-fusion). This brings out the possibility of an objective ontological wholeness. […] We understand this through the formation of common pools of information. This information bring about nonlocal interaction. […] The whole is presupposed in the quantum wave function and it is the active information in this wave function that forms and dissolves wholes. […] There is no actual collapse; there is merely a process in which the information represented by the unoccupied packets effectively loses all potential for activity. […] It could be felt that the empty packets, which also satisfy Schrödinger’s equation, constitute a vast mass of ‘bits of reality’ that are, as it were, ‘floating around’ interpenetrating that part of reality which corresponds to the occupied packets. […] We may make an analogy here to human relationships in society. The most immediate and concrete reality is the collection of individual human beings. In so far as these are related by pools of information, this latter will become manifest in the behaviour of the human beings. The behaviour both of the individual and of the society depend crucially on this information (rather as happens with the particles of physics). The information itself held at some very subtle level which does not show directly and which has negligible energy compared with that involved in the physical movements of the people. To complete the analogy we might surmise that perhaps the information in the wave function is likewise contained at a more subtle level of negligible energy in a way that has not shown yet and that we have not thus far been able to study.

Hiley
>[Process and the implicate order (2005)] The quantum potential should be considered as an information potential. Not only does the quantum potential carry information about the experimental set up, but, more importantly, it induces a change of form from within the system itself. It is in this more general sense that we can regard the quantum potential as an information potential. […] In other words this information can be either active or passive. The information carried by the quantum field is clearly not information for us, it is information for the particle and as such is objective. This information has meaning for the particle. Since meaning is involved, we are not using the word “information” in the sense of Shannon (1948). […] The quantum potential always has meaning for its particle, although it might not have meaning for other particles at the same location. […] It is interesting to note that Feynman once proposed that every point of space is like a computer processing incoming information and outputting new information. For our approach to non-relativistic quantum mechanics, it is the particle that processes the information although in field theory (which we discuss later) our approach is very similar to the kind of structure Feynman had in mind. In the case of the computer, the significance of the information is decided by outside human activity both in terms of the software we use and the type of information that is stored in the chip. […] Notice that in all the examples above the additional energy does not necessarily come from the information carrier, but has some other source. In the case of the computer […], it comes from the battery or external power point. In the case of the cell, it literally comes from within the cell itself. […] In Bohm’s words, there is a continual unfolding and enfolding of orders. The proposal, then, is that this may provide a richer picture of quantum processes. There is no continuity of substance, merely an unfolding of form. The more stable the form, the more persistent it is. Suppose we apply this idea to the two-slit experiment, the slits will see this unfolding process and it will look as if a wave has gone through both slits, but the total process will manifest itself only when the energy is condensed to a small region, giving the appearance of a particle. There is no continuing particle, no continuity of substance, merely a continuity of form. Notice the order need not always be manifest. We also have a non-manifest reality which implicitly contains the order. To make a feature manifest we need a special physical process which we call observing instrument. The total process of the manifest and the non-manifest is a new order that Bohm (1980) called the implicate order.
>
>[From the Heisenberg picture to Bohm (2002)] The quantum potential gives new form to the evolution of the trajectories, in a way that is very reminiscent of the morphogenetic fields proposed by Waddington (1956) and Thom (1975) in biology. The form is provided from within but it is, of course, shaped by the environment. Thus the quantum potential reflects the experimental conditions. Close one slit and the quantum potential changes and the subsequent evolution of the particle is different. There seems to be a kind of ‘self-organization’ involved. Now self-organization requires the notion of information to be active. In the case of a biological system, this information is clearly provided by the environment, soil conditions, lack of moisture etc. In a quantum system I want to suggest that the information is provided by the experimental conditions, its environment. But this information is not passive. It is active and causes the internal energy to be redistributed between the kinetic and potential parts. Thus the quantum process is literally ‘formed from within’. […] When I use the word information here, I do not mean information for me, the experimenter, but the activity taking place in the system itself. In other words information is playing an objective and active role in all quantum processes.

Bell
>[Speakable and unspeakable in quantum mechanics (1987)] In this picture [de Broglie-Bohm] the wave always goes through both slits (as is the nature of waves) and the particle goes through only one (as is the nature of particles). […] It is vital here to put away the classical prejudice that a particle moves on a straight path in ‘field-free’ space – free, that is, from fields other than the de Broglie-Bohm!
>
>[Explained by Dürr, Goldstein and Zanghi in Bohmian mechanics as the foundation of quantum mechanics (1996)] Is it not clear from the smallness of the scintillation on the screen that we have to do with a particle? And is it not clear, from the diffraction and interference patterns, that the motion of the particle is directed by a wave? De Broglie showed in detail how the motion of a particle, passing through just one of two holes in screen, could be influenced by waves propagating through both holes. And so influenced that the particle does not go where the waves cancel out, but is attracted to where they cooperate. This idea seems to me so natural and simple, to resolve the wave-particle dilemma in such a clear and ordinary way, that it is a great mystery to me that it was so generally ignored.

According to orthodox quantum theory, the complete description of a system of particles is provided by its wave function. This statement is somewhat problematical: If “particles” is intended with its usual meaning – point-like entities whose most important feature is their position in space – the statement is clearly false, since the complete description would then have to include these positions; otherwise, the statement is, to be charitable, vague. Bohmian mechanics is the theory that emerges when we indeed insist that “particles” means particles.

### Observation: von Neumann’s cut and Schrödinger’s cat <a name="p49"></a>

In physics, the observer effect is the theory that the mere observation of a phenomenon inevitably changes that phenomenon. This is often the result of instruments that, by necessity, alter the state of what they measure in some manner. A common example is checking the pressure in an automobile tire; this is difficult to do without letting out some of the air, thus changing the pressure. Similarly, it is not possible to see any object without light hitting the object, and causing it to reflect that light. While the effects of observation are often negligible, the object still experiences a change. This effect can be found in many domains of physics, but can usually be reduced to insignificance by using different instruments or observation techniques.
An especially unusual version of the observer effect occurs in quantum mechanics, as best demonstrated by the double-slit experiment that we talked about in the previous section. Physicists have found that even passive observation of quantum phenomena (by changing the test apparatus and passively “ruling out” all but one possibility), can actually change the measured result. A particularly famous example is in 1998 when researchers at the Weizmann Institute of Science conducted a highly controlled experiment demonstrating how a beam of electrons is affected by the act of being observed. The experiment revealed that the greater the amount of “watching”, the greater the observer’s influence on what actually takes place. Despite the “observer” in this experiment being an electronic detector – possibly due to the assumption that the word “observer” implies a person – its results have led to the popular belief that a conscious mind can directly affect reality. The need for the “observer” to be conscious is a matter of interpretation. Yet, it is not supported by the majority of scientists, and has been pointed out as a misconception rooted in a poor understanding of the quantum wave function and the quantum measurement process, apparently being the generation of information at its most basic level that produces the effect.

Jordan, quoted by Bell in Speakable and unspeakable in quantum mechanics (1987).
>Jordan declared, with emphasis, that observations not only disturb what has to be measured, they produce it. In a measurement of position, for example, as performed with the gamma ray microscope:
The electron is forced to a decision. We compel it to assume a definite position; previously it was, in general, neither here nor there; it had not yet made its decision for a definite position… If by another experiment the velocity of the electron is being measured, this means: the electron is compelled to decide itself for some exactly defined value of the velocity… we ourselves produce the results of measurement.

Wigner, Remarks on the mind-body question (1995).
>The impression which one gains at an interaction, called also the result of an observation, modifies the wave function of the system. The modified wave function is, furthermore, in general unpredictable before the impression gained at the interaction has entered our consciousness: it is the entering of an impression into our consciousness which alters the wave function because it modifies our appraisal of the probabilities for different impressions which we expect to receive in the future. It is at this point that the consciousness enters the theory unavoidably and unalterably. […] It is natural to inquire about the situation if one does not make the observation oneself but lets someone else carry it out. What is the wave function if my friend looked at the place where the flash might show at time t? The answer is that the information available about the object cannot be described by a wave function. One could attribute a wave function to the joint system: friend plus object, and this joint system would have a wave function also after the interaction, that is, after my friend has looked. I can then enter into interaction with this joint system by asking my friend whether he saw a flash. If his answer gives me the impression that he did, the joint wave function of friend + object will change into one in which they even have separate wave functions (the total wave function is a product). […] However, even in this case, in which the observation was carried out by someone else, the typical change in the wave function occurred only when some information (the yes or no of my friend) entered my consciousness. It follows that the quantum description of objects is influenced by impressions entering my consciousness. Solipsism may be logically consistent with present quantum mechanics, monism in the sense of materialism is not. 

[Werner Heisenberg](/references/wernerheisenberg.html)
>A real difficulty in the understanding of this interpretation arises, however, when one asks the famous question: But what happens “really” in an atomic event? […] We cannot completely objectify the result of an observation, we cannot describe what “happens” between this observation and the next. This looks as if we had introduced an element of subjectivism into the theory, as if we meant to say: what happens depends on our way of observing it or on the fact that we observe it. Before discussing this problem of subjectivism it is necessary to explain quite clearly why one would get into hopeless difficulties if one tried to describe what happens between two consecutive observations. […] The observation plays a decisive role in the event and the reality varies, depending upon whether we observe it or not. […] In this way quantum theory reminds us, as Bohr has put it, of the old wisdom that when searching for harmony in life one must never forget that in the drama of existence we are ourselves both players and spectators.

Heisenberg points out the problem regarding objective reality. Bohmian mechanics supposes an underlying reality, independent of the observer even if the observer has an impact on it. For Bohm it makes sense to suppose that things exist on their own and he thus opposes solipsism (the external world and other minds cannot be known and might not exist outside the mind). Some other personal opinions are presented by Nauenberg.

Nauenberg, Does quantum mechanics require a conscious observer? (2011).
>I conclude with a few quotations, that are relevant to the topic addressed here, by some of the most prominent physicists in the second half of the 20th century.
>
>>Feynman (Nobel Prize, 1965): “Nature does not know what you are looking at, and she behaves the way she is going to behave whether you bother to take down the data or not”.
>
>>Gellmann (Nobel Prize, 1969): “The universe presumably couldn’t care less whether human beings evolved on some obscure planet to study its history; it goes on obeying the quantum mechanical laws of physics irrespective of observation by physicists”.
>
>>Leggett (Nobel Prize 2003): “It may be somewhat dangerous to explain something one does not understand very well [the quantum measurement process] by invoking something [consciousness] one does not understand at all!”
>
>>Wheeler: “Caution: “Consciousness” has nothing whatsoever to do with the quantum process. We are dealing with an event that makes itself known by an irreversible act of amplification, by an indelible record, an act of registration. Does that record subsequently enter into the “consciousness” of some person, some animal or some computer? Is that the first step into translating the measurement into “meaning”, meaning regarded as “the joint product of all the evidence that is available to those who communicate”? Then that is a separate part of the story, important but not to be confused with “quantum phenomena”.
>
>>Bell: “From some popular presentations the general public could get the impression that the very existence of the cosmos depends on our being here to observe the observables. I do not know that this is wrong. I am inclined to hope that we are indeed that important. But I see no evidence that it is so in the success of contemporary quantum theory. So I think that it is not right to tell the public that a central role for conscious mind is integrated into modern atomic physics. Or that ‘information’ is the real stuff of physical theory. It seems to me irresponsible to suggest that technical features of contemporary theory were anticipated by the saints of ancient religions… by introspection. The only ‘observer’ which is essential in orthodox practical quantum theory is the inanimate apparatus which amplifies the microscopic events to macroscopic consequences. Of course this apparatus, in laboratory experiments, is chosen and adjusted by the experiments. In this sense the outcomes of experiments are indeed dependent on the mental process of the experimenters! But once the apparatus is in place, and functioning untouched, it is a matter of complete indifference – according to ordinary quantum mechanics – whether the experimenters stay around to watch, or delegate such ‘observing’ to computers”.
>
>>van Kampem: “Whoever endows with more meaning than is needed for computing observable phenomena is responsible for the consequences”.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>We can see several difficulties in the attempt to bring in the direct action of the mind to give an ontological interpretation of the current physical laws of the quantum theory. Thus in a laboratory, it is hard to believe that the human mind is actually significantly affecting the results of the functioning of the instruments (which may, as we have already pointed out, be recorded on a computer that is not even examined for a long time). Moreover quantum theory is currently applied to cosmology, and it is difficult to believe that the evolution of the universe before the appearance of human beings depended fundamentally on the human mind (e.g. to make its wave function ‘collapse’ in an appropriate way). Of course one could avoid this difficulty by assuming a universal mind. But if we know little about the human mind, we know a great deal less about the universal mind. Such an assumption replaces one mystery by an even greater one. […] Like Bohr, Wheeler puts a primary emphasis on the phenomenon, in which the experimental arrangement and the experimental result constitute an indivisible whole. From his analysis of the delayed choice type of experiment, he concludes that “no phenomenon is a phenomenon until it is an observed phenomenon”, so that “the universe does not ‘exist out there’ independently of all acts of observation. It is in some strange sense a participatory universe. The present choice of the mode of observation… should influence what we say about the past… The past is undefined and undefinable without the observation”. We can agree with Wheeler that no phenomenon is a phenomenon until it is observed, because, by definition, a phenomenon is what appears. Therefore it evidently cannot be a phenomenon unless it is the content an observation. The key point about an ontological interpretation such as ours to ask the question as to whether there is an underlying reality that exists independently of observation, but that can appear to an observer when he ‘looks’ (in physics, with the aid of suitable instruments). We have proposed a model of such a reality in which we say, along with Wheeler, that the universe is essentially participatory in nature. However, unlike Wheeler, we have given an account of this participation, which we show throughout this book to be rational and orderly and in agreement with all the actual predictions of quantum theory. In doing this we assume that the underlying reality is not just the wave function, but that it also has to include the particles. As we shall see in this section, when we take this into account there is no need to say that the past is affected by our observation in the present. Nor do we imply even that what we say about the past is thus affected. Therefore the need to introduce such ideas is based on the insistence that the wave function provides a complete description of reality.

Fine, On the interpretation of Bohmian mechanics (1996).
>Although Bohmian mechanics looks more realist, insofar as it postulates an underlying reality, the role played by observation there is even more prominent than in standard quantum mechanics.

In quantum mechanics, a Heisenberg-von Neumann cut is the hypothetical interface between quantum events and an observer’s information, knowledge, or conscious awareness. Below the cut everything is governed by the wave function; above the cut a classical description is used. This cut is a theoretical construct; it is not known whether actual cuts exist, where they might be found, or how they could be detected experimentally. 

Nauenberg, Does quantum mechanics require a conscious observer? (2011).
>Early pioneers in the development of quantum mechanics like Niels Bohr (1958) assumed, however, that the measurement devices behave according to the laws of classical mechanics, but von Neumann pointed out, quite correctly, that such devices also must satisfy the principles of quantum mechanics. Hence, the wavefunction describing this device becomes entangled with the wavefunction of the object that is being measured, and the superposition of these entangled wavefunctions continues to evolve in accordance with the equations of quantum mechanics. This analysis leads to the notorious von Neumann chain, where the measuring devices are left forever in an indefinite superposition of quantum states. It is postulated that this chain can be broken, ultimately, only by the mind of a conscious observer. […] The von Neumann paradox as a dilemma: “… If the interference is destroyed, then the Schrodinger equation is incorrect for systems containing consciousness. If the interference is not destroyed, the quantum mechanical description is revealed as not wrong but certainly incomplete” (Bell and Nauenberg, 1966).

Ghirardi, The macro-objectification problem and conscious perceptions (2011).
>We must consider successive measurement procedures aiming to identify what the actual state of affairs is. […] The chain never ends but it exhibits a quite interesting feature. No matter at which point one chooses to break it, if the linear superposition is replaced by one of its terms one gets a consistent set of outcomes. […] In brief, there is a full “final” consistency provided one breaks (at a certain level) the chain. Some remarks are at order: (1) This approach leaves open the stage at which the breaking must be assumed to occur, provided it occurs at some point. In this sense, it is not surprising that von Neumann and Wigner have chosen to break it at the level in which consciousness enters into play. (2) von Neumann’s has tacitly made some quite drastic assumptions concerning the unfolding of the process (typically that the measurement has 100% efficiency, that the final states are strictly orthogonal etc., a set of assumptions which are very difficult to be verified in actual experiments) and this is why one usually refers to the just described scheme as “The Ideal von Neumann’s Measurement Process”.

Interpretations of quantum mechanics that do not recognize wave function collapse (such as De Broglie–Bohm or many-worlds interpretations) do not require Heisenberg cuts.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>[von Neumann] was led to make a distinction between the quantum and the classical levels. Between them, he said there was a ‘cut’. This is of course, purely abstract because von Neumann admitted, along with physicists in general, that the quantum and classical levels had to exist in what was basically one world. However, for the sake of analysis one could talk about these two different levels and treat them as being in interaction. The effect of this interaction was to produce at the classical level a certain observable experimental result. […] It is evident that this whole situation is unsatisfactory because the ontological process of collapse is itself highly ambiguous. Perhaps Bohr’s rather more limited ambiguity may seem preferable to von Neumann’s indefinitely proliferating ambiguity. Wigner has carried this argument further and has suggested that the above ambiguity of the collapse can be removed by assuming that this process is definitely a consequence of the interaction of matter and mind. Thus he is, in effect, placing the cut between these two and implying that mind is not limited by quantum theory. (Pauli has also felt for different reasons that mind plays a key role in this context). […] [For Bohm and Hiley, M]easurement is just a special case of a transition process in which the two systems interact and then come out in correlated states. It is this correlation that enables us, from the observed result, to attribute a corresponding property to the final state of the observed system. In the transition process that takes place in a measurement, it is clear that (as happens indeed in all transition processes) there is no need to place any ‘cuts’ or arbitrary breaks in the description of reality.

Schrödinger’s cat is a thought experiment, sometimes described as a paradox, devised by Schrödinger in 1935, though the idea originated from Einstein. It illustrates what he saw as the problem of the Copenhagen interpretation of quantum mechanics applied to everyday objects. In this interpretation a quantum system remains in superposition until it interacts with, or is observed by the external world. When this happens, the superposition collapses into one or another of the possible definite states. The scenario presents a hypothetical cat that may be simultaneously both alive and dead, a state known as a quantum superposition, as a result of being linked to a random subatomic event that may or may not occur. Since Schrödinger’s time, other interpretations of quantum mechanics have been proposed that give different answers to the questions posed by Schrödinger’s cat of how long superpositions last and when (or whether) they collapse.

Schrödinger, The present situation in quantum mechanics (1935).
>One can even set up quite ridiculous cases. A cat is penned up in a steel chamber, along with the following device (which must be secured against direct interference by the cat): in a Geiger counter, there is a tiny bit of radioactive substance, so small, that perhaps in the course of the hour one of the atoms decays, but also, with equal probability, perhaps none; if it happens, the counter tube discharges and through a relay releases a hammer that shatters a small flask of hydrocyanic acid. If one has left this entire system to itself for an hour, one would say that the cat still lives if meanwhile no atom has decayed. The first atomic decay would have poisoned it. The Ψ-function of the entire system would express this by having in it the living and dead cat (pardon the expression) mixed or smeared out in equal parts. It is typical of these cases that an indeterminacy originally restricted to the atomic domain becomes transformed into macroscopic indeterminacy, which can then be resolved by direct observation. That prevents us from so naively accepting as valid a “blurred model” for representing reality. In itself, it would not embody anything unclear or contradictory. There is a difference between a shaky or out-of-focus photograph and a snapshot of clouds and fog banks.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>Schrödinger regarded this whole situation as absurd. Firstly it did not make sense to him that observation was needed to answer the question of whether the cat was alive or dead, In addition, even before observation, one might well think that the cat should at least know more than this about its own state. Or if one does not wish to attribute consciousness to the cat, suppose instead that it had been a human being in the box. To avoid violence we could replace the gun by a device that would prick his skin. Would this person find himself in an ambiguous state in which his skin was neither clearly untouched nor clearly pricked by the device?

Carter, Classical anthropic Everett model: indeterminacy in a preordained multiverse (2011).
>According to the “Copenhagen” interpretation, the relevant “wave function” collapses either to a pure state in which the cat is unambiguously alive, or else one in which it is unambiguously dead, when a human “observer” opens its box. The trouble with the Copenhagen interpretation is that it denies “observer” status to the occupant of the box, which is questionable even in the case of a humble cat, and would clearly be quite inadmissible if the cat were replaced by another human.


### Hidden variables <a name="p410"></a>

One line of investigation in the prehistory of Bell’s Theorem is Bell’s examination of the hidden-variables program. This program involves supplementation of the quantum mechanical state of a system by further “elements of reality”, or “hidden variables”, the incompleteness of the quantum state being the explanation for the statistical character of quantum mechanical predictions concerning the system. A pioneering version of a hidden variables theory was proposed by de Broglie in 1927 and revived by Bohm.

In Bohm’s interpretation, the quantum potential constitutes an implicate (hidden) order which organizes a particle, which may itself be the result of a further implicate order called superimplicate which organizes a field. Nowadays Bohm’s theory is considered to be one of many interpretations of quantum mechanics which give a realist interpretation, and not merely a positivistic one, to quantum-mechanical calculations. A possible weakness of Bohm’s theory is that some (Einstein, Pauli, and Heisenberg) feel that it looks contrived. Indeed, Bohm thought this of his original formulation of the theory. It was deliberately designed to give predictions that are in all details identical to conventional quantum mechanics. Bohm’s original aim was not to make a serious counter proposal but simply to demonstrate that hidden-variable theories are indeed possible. It thus provided a supposed counterexample to the famous proof by von Neumann that was generally believed to demonstrate that no deterministic theory reproducing the statistical predictions of quantum mechanics is possible. 

[Werner Heisenberg](/references/wernerheisenberg.html)
>One group of counterproposals works with the idea of “hidden parameters”. Since the quantum-theoretical laws determine in general the results of an experiment only statistically, one would from the classical standpoint be inclined to think that there exist some hidden parameters which escape observation in any ordinary experiment but which determine the outcome of the experiment in the normal causal way. Therefore, some papers try to construct such parameters within the framework of quantum mechanics.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>This notion of quantum state has indeed been most systematically and extensively developed by von Neumann, who not only gave it a precise mathematical formulation, but who also attempted, in his own way, to come to grips with the philosophical issues to which this approach gave rise. It was a key part of this development to give a proof claiming to show that quantum mechanics had an intrinsic logical closure (in the sense, that no further concepts, e.g. involving ‘hidden variables’, could be introduced that would make possible a more detailed description of the state of the system than is afforded by the wave function).

de Broglie, Une tentative d’interprétation causale et non linéaire de la mécanique ondulatoire (1956).
>Mr. von Neumann concluded that it was impossible to restore classical conceptions by the introduction of hidden variables. […] This demonstration seems to definitively exclude the possibility of returning to a causal and objective theory of microscopic phenomena. […] But, as we will see, there is at least one theory, the causal theory which we will study below, which allows us to find the probability distributions of wave mechanics and which is a deterministic theory with hidden parameters. This theory may not be physically correct, but it exists and its existence alone is already at odds with von Neumann’s theorem. How is it possible? The examination of this question led me, in agreement on this point with Mr. David Bohm, to think that the demonstration of Mr. von Neumann implies a hypothesis which is not absolutely necessary and which is not fulfilled in the causal theory in question. […] As the same measuring device cannot, at the end of its interaction with the corpuscle, have “separated” the wave packets corresponding to precise values of the non-switching quantities P and Q, we see that it is not possible to measure such quantities and this makes it possible to rediscover in a new way the Heisenberg uncertainty relations as translating not a real indeterminacy of the quantities P and Q before the measurement, but only the material impossibility of obtaining simultaneously for the two quantities precise values using a single measurement operation. With the point of view adopted here, any quantity Q of the corpuscle has a well-defined value in the initial state, but this value is a “hidden variable” since, in general, any attempt to measure it will have the effect of modifying it. If, exceptionally, a measuring device has the effect of making it possible to obtain the value of Q without modifying it, then this device will modify the values of all the quantities P which do not switch with Q. We must therefore clearly distinguish the “hidden variables” which, in causal theory as in classical physics, would characterize at every instant the position and movement of the corpuscle and the “observables” in the sense of M. Dirac which are the values of these quantities likely to be obtained by a measurement operation. On this subject, Mr. Bohm rightly says: “The above means that the measurement of an observable is not really the measurement of a physical property belonging to the observed system itself. On the contrary, the value of an observable represents only a possibility, impossible to describe and to control completely, which depends as much on the measuring device as on the observed system itself”.


### Nonlocality: EPR, Bell inequalities and Aspect’s experience <a name="p411"></a>

Quantum entanglement is a label for the observed physical phenomenon that occurs when a pair or group of particles is generated, interact, or share spatial proximity in a way such that the quantum state of each particle of the pair or group cannot be described independently of the state of the others, even when the particles are separated by a large distance. As a result, nonlocality describes the apparent ability of objects to instantaneously know about each other’s state, even when separated by these large distances (potentially even billions of light years), almost as if the universe at large instantaneously arranges its particles in anticipation of future events. Thus, in the quantum world, despite what Einstein had established about the speed of light being the maximum speed for anything in the universe, instantaneous action or transfer of information does appear to be possible. This is in direct contravention of the “principle of locality” (or what Einstein called the “principle of local action”), the idea that distant objects cannot have direct influence on one another, and that an object is directly influenced only by its immediate surroundings, an idea on which almost all of physics is predicated. Nowadays, nonlocality has been experimentally verified under different physical assumptions. Yet, we now know that nonlocality does not allow for faster-than-light communication, and hence is compatible with special relativity. However, it prompts many of the foundational discussions concerning quantum theory. 

That quantum mechanics admits of entangled states was discovered by Schrödinger (1926) in one of his pioneering papers, but the significance of this discovery was not emphasized until the paper of Einstein, Podolsky, and Rosen.

Einstein, Podolsky and Rosen, Can quantum-mechanical description of physical reality be considered complete? (1935).
>In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete. […] While we have thus shown that the wave function does not provide a complete description of the physical reality, we left open the question of whether or not such a description exists. We believe, however, that such a theory is possible.

EPR examined correlations between the positions and the linear momenta of two well separated spinless particles and concluded that in order to avoid an appeal to nonlocality these correlations could only be explained by “elements of physical reality” in each particle – specifically, both definite position and definite momentum – and since this description is richer than permitted by the uncertainty principle of quantum mechanics their conclusion is effectively an argument for a hidden variables interpretation.

Bohr, Atomic physics and human knowledge (1957).
>Such an argumentation, however, would hardly seem suited to affect the soundness of quantum-mechanical description, which is based on a coherent mathematical formalism covering automatically any procedure of measurement like that indicated. The apparent contradiction in fact discloses only an essential inadequacy of the customary viewpoint of natural philosophy for a rational account of physical phenomena of the type with which we are concerned in quantum mechanics. Indeed the finite interaction between object and measuring agencies conditioned by the very existence of the quantum of action entails – because of the impossibility of controlling the reaction of the object on the measuring instruments, if these are to serve their purpose – the necessity of a final renunciation of the classical ideal of causality and a radical revision of our attitude towards the problem of physical reality. In fact, as we shall see, a criterion of reality like that proposed by the named authors contains – however cautious its formulation may appear – an essential ambiguity when it is applied to the actual problems with which we are here concerned.

Bohm noticed that “EPR did not make an explicit discussion of the question of nonlocality and focused instead on that of completeness”. Also, for him, nonlocality at large scale has insignificant effects, we do not experience it.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>If such an interaction was operating, then the criterion of EPR for an element of reality would clearly be irrelevant, since by hypothesis a disturbance of atom A can now bring about a disturbance of atom B. However, EPR did not seem to regard this sort of interaction as worthy of serious consideration. To see why, we note that there are two possibilities: (1) The force is transmitted at some finite speed less than or equal to that of light. In this case the statistical predictions of the current quantum theory would, of course, have to fail for measurements of A and B that are space-like separated. But EPR assumed, as we have pointed out, that these statistical predictions are correct, and thus they did not even envisage such a failure of the current quantum theory. (2) The interaction is transmitted instantaneously. In this case the interaction would have to be nonlocal, i.e. to operate directly and immediately between the two particles with a strong force even at very large distances. Evidently this would violate the special theory of relativity.

In 1964, Bell published a paper which eventually transformed the study of the foundation of quantum mechanics. The paper showed, under conditions that, on the assumption of certain auxiliary conditions, no physical theory that satisfies a certain locality condition, which may be called Bell locality, can fully reproduce the quantum probabilities for outcomes of experiments. Since that time, variants on the theorem, with family resemblances, have been formulated. “Bell’s Theorem” is the collective name for the entire family. The theorem has roots in Bell’s investigations into the status of the hidden-variables program, and on earlier work concerning quantum entanglement. The fact that Bell’s theorem has roots into investigations on hidden-variables theories has led to a misconception that the theorem is a no-go theorem for hidden-variables theories tout court. There could be no such theorem, since, as Bell himself repeatedly emphasized, there is a functioning hidden-variables theory, the de Broglie-Bohm theory.

It has become commonplace to say that (provided that the supplementary assumptions are accepted), the class of theories ruled out by experimental violations of Bell inequalities is the class of local realistic theories, and that the worldview to be abandoned is local realism. The terminology of “local realistic theories” as the targets of experimental tests of Bell inequalities was introduced by Clauser and Shimony (1978), intended as a synonym for what Clauser and Horne (1974) called “objective local theories”. The terminology was adopted by d’Espagnat (1979) and Mermin (1980). For Clauser and Shimony realism is “a philosophical view according to which external reality is assumed to exist and have definite properties, whether or not they are observed by someone” (1978). In a similar vein, d’Espagnat (1979) says that realism is “the doctrine that regularities in observed phenomena are caused by some physical reality whose existence is independent of human observers”.

The set-up envisaged in the proof of Bell’s theorem highlights a striking prediction of quantum theory, namely, long-distance entanglement, and experimental tests of the Bell inequalities provide convincing evidence that it is a feature of reality. Moreover, Bell’s theorem reveals that the entanglement-based correlations predicted by quantum mechanics are strikingly different from the sort of locally explicable correlations familiar in a classical context. Does Bell’s theorem show that quantum theory is incompatible with relativity? The answer, of course, depends on what one takes relativity theory to require. It can be shown that, in the absence of nonlocal interaction terms in the Hamiltonian, quantum correlations cannot be exploited to send signals superluminally. There has been a tendency in some of the literature to take this by itself to indicate compatibility with relativity. That this is insufficient can be seen from the fact that there can be theories, such as the de Broglie-Bohm theory, that require nonrelativistic spacetime structure for the formulation of their dynamical laws, while not permitting signaling (at least, as long as the usual distribution postulate for particle positions is satisfied).
There is a taxonomy of options for accepting the supplementary assumptions while also accepting Bell-inequality violating correlations:

- Reject the assumption of causal locality, and accept that there are causal relations between events that are outside of each other’s light-cones. There are two ways to do this:
	- Introduce non-relativistic spatiotemporal structure. One option, and perhaps the most straightforward one, is to reject the assumption of Lorentz invariant spatiotemporal structure at the fundamental level. This is the position that is involved in acceptance of the de Broglie-Bohm theory or any similar theory. On the de Broglie-Bohm theory, the velocity of a particle is a function not only of its own location but also of the location of any other particles it is entangled with. This means that the dynamics of the theory requires a privileged relation of distant simultaneity.
	- Retrocausality. Another option is to maintain the assumption that the past of an event is its past light cone, and thereby maintain Lorentz invariance of the spatiotemporal structure, but to deny that causes must temporally precede their effects. Typically, those who take this option accept retrocausality, maintaining that causal processes can be propagated within light cones in either temporal direction. This avenue of explaining Bell-type correlations was introduced by de Beauregard, and has been advocated by Price and Wharton (Price 1994; Price and Wharton 2015). It forms part of the Transactional Interpretation of quantum mechanics (Cramer 1986; Kastner 2013).
- Reject the Principle of Common Cause, and remove any reason there is to think that cause-effect relations between spacelike separated events are incompatible with relativistic spacetime structure. A more common view is the lesson of Bell’s theorem in that there may be correlations that are not explicable in terms of cause-effect relations. Some have suggested that correlations violating Bell inequalities are due to a common cause in their past, namely, the process that created entanglement.


Bell
>[On the problem of hidden variables in quantum mechanics (1995)] The demonstrations of von Neumann and others, that quantum mechanics does not permit a hidden variable interpretation, are reconsidered. It is shown that their essential axioms are unreasonable. It is urged that in further examination of this problem an interesting axiom would be that mutually distant systems are independent of one another. […] Whether this question is indeed interesting has been the subject of debate. The following works contain discussions of and references on the hidden variable problem: L. de Broglie (1953), W. Heisenberg in W. Pauli (1955), S. Körner (1957), N. Hansen (1963). See also the various works by D. Bohm, and Bell and Nauenberg. For the view that the possibility of hidden variables has little interest, see especially the contributions of Rosenfeld, Pauli, Heisenberg and Hansen. Einstein’s “Autobiographical Notes” and “Reply to Critics” suggest that the hidden variable problem has some interest. […] The realization that von Neumann’s proof is of limited relevance has been gaining ground since the 1952 work of Bohm. However, it is far from universal. […] In particular the analysis of Bohm seems to lack clarity, or else accuracy. […] There are features which can reasonably be desired in a hidden variable scheme. The hidden variables should surely have some spatial significance and should evolve in time according to prescribed laws. These are prejudices, but it is just this possibility of interpolating some (preferably causal) space-time picture, between preparation of and measurements on states, that makes the quest for hidden variables interesting to the unsophisticated. […] The curious feature is that the trajectory equations for the hidden variables have in general a grossly nonlocal character. […] In fact the Einstein-Podolsky-Rosen paradox is resolved in the way which Einstein would have liked least.
>
>[Speakable and unspeakable in quantum mechanics (1987)] The philosopher in the street, who has not suffered a course in quantum mechanics, is quite unimpressed by Einstein-Podolsky-Rosen correlations. He can point to many examples of similar correlations in everyday life. The case of Bertlmann’s socks is often cited. Dr. Bertlmann likes to wear two socks of different colors. Which color he will have on a given foot on a given day is quite unpredictable. But when you see that the first sock is pink you can be already sure that the second sock will not be pink. Observation of the first, and experience of Bertlmann, gives immediate information about the second. There is no accounting for tastes, but apart from that there is no mystery here. And is not the EPR business just the same? […] The EPR paper caused such a fuss, and the dust has not settled even now. It is as if we had come to deny the reality of Bertlmann’s socks, or at least of their colors, when not looked at. And as if a child has asked: How come they always choose different colors when they are looked at? How does the second sock know what the first has done? Paradox indeed! But for the others, not for EPR. EPR did not use the word ‘paradox’. They were with the man in the street in this business. For them these correlations simply showed that the quantum theorists had been hasty in dismissing the reality of the microscopic world. In particular Jordan had been wrong in supposing that nothing was real or fixed in that world before observation. For after observing only one particle the result of subsequently observing the other (possibly at a very remote place) is immediately predictable. Could it be that the first observation somehow fixes what was unfixed, or makes real what was unreal, not only for the near particle but also for the remote one? For EPR that would be an unthinkable ‘spooky action at a distance’. To avoid such action at a distance they have to attribute, to the space-time regions in question, real properties in advance of observation, correlated properties, which predetermine the outcomes of these particular observations. Since these real properties, fixed in advance of observation, are not contained in quantum formalism, that formalism for EPR is incomplete. It may be correct, as far as it goes, but the usual quantum formalism cannot be the whole story. […] It may be that we have to admit that causal influences do go faster than light. The role of Lorentz invariance in the completed theory would then be very problematic. An ‘aether’ would be the cheapest solution. But the unobservability of this aether would be disturbing. So would the impossibility of ‘messages’ faster than light, which follows from ordinary relativistic quantum mechanics in so far as it is unambiguous and adequate for procedures we can actually perform. The exact elucidation of concepts like ‘message’ and ‘we’, would be a formidable challenge.

Between 1980 and 1982, Alain Aspect (1947-, French) proposed the first quantum mechanics experiment to demonstrate the violation of Bell’s inequalities. Its irrefutable result allowed for further validation of the quantum entanglement and locality principles. It also offered an experimental answer to the EPR paradox which had been proposed about fifty years earlier. Aspect’s experiment has been reproduced, and the violation of Bell’s inequalities was systematically confirmed.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>Bohr would say that the EPR paradox was based on an inadmissible attribution of properties to a second particle solely on the basis of measurements that could be carried out on the first particle. […] Bohr is using a different notion of reality from that of EPR. For Bohr a concept represents reality only in so far as it is in unambiguous correspondence with the whole set of possible phenomena and these phenomena are necessarily such that they have to be described in terms of the concepts of classical physics. For Einstein, however, concepts are a ‘free creation of the human mind’ and their correspondence with reality is at first assumed and then tested by the phenomena they predict. Therefore there is no problem in assuming the simultaneous reality of all the properties of particle B even though these cannot be simultaneously observed. Bohr regards this as a totally inadmissible way of using concepts in the context of quantum theory in which these have to be ambiguous and mutually exclusive, but nevertheless complementary. […] We have already seen that for the one-body system the particle can depend strongly on distant features of the environment. In the two-body system we can have a similar dependence on the environment, but in addition, the two particles can also be strongly coupled at long distances. Their interaction can therefore be described as nonlocal. […] For several centuries, there has been a strong feeling that nonlocal theories are not acceptable in physics. It is well known, for example, that Newton felt very uneasy about action-at-a-distance and that Einstein regarded it as ‘spooky’. One can understand this feeling, but if one reflects deeply and seriously on this subject one can see nothing basically irrational about such an idea. […] The only serious objection we can see to nonlocality is that, at first sight, it does not seem to be compatible with relativity, because nonlocal connections in general would allow a transmission of signals faster than the speed of light. In later chapters we extend the causal interpretation to a relativistic context and show that although nonlocality is still present, it does not introduce any inconsistencies into the theory, e.g. it does not imply that we can use the quantum potential to transmit a signal faster than light. […] It has generally been felt that the very concept of nonlocality is unacceptable in a scientific theory and so people have sought an explanation of this behaviour in terms of local hidden variables that would in principle determine the results of each measurement. This leads us then to give a brief discussion of Bell’s inequality (which has to be satisfied by any such explanation), and also of the fact that Bell’s inequality is not actually satisfied, either by the predictions of the quantum theory or by the experiments that have been done to test this point. […] Nonlocality is not commonly encountered at the large scale level [since] it is generally difficult to maintain the wave functions that are needed for this except for certain systems at low temperatures (such as superconductors) and highly isolated systems that have to be produced in a rather artificial way. […] Bell’s inequality has been tested in a large number of experiments and generally speaking the inequality has been found to be violated. […] The first experiment of this kind was performed by Freedman and Clauser. The latest and perhaps the most thorough set of experiments, has been performed by Aspect et al. This implied that, independently of quantum mechanics, we have an experimental proof that if there are hidden variables they must be nonlocal. Aspect’s experiment have been criticized by some physicists. […] However, these assumptions seem rather arbitrary and acritical and, in fact, they give the impression of being contrived just to ‘save the appearances’. The least we can say is that there is a strong prima facie case for nonlocality. […] We have thus far gone extensively into quantum nonlocality and have seen that it can arise in a great many contexts. At first sight one might be concerned as to whether we will then be able to understand why nonlocality is not encountered in our common experience of the world. Basically the answer to this question is quite simple. For our ordinary experience, both in the domain of common sense and in that of classical physics, is restricted to situations in which the quantum potential is very small, so that, in this context at least, it does not produce significant EPR correlations. For, as we have seen, quantum nonlocality is entirely the product of the quantum potential.

Hiley
>[Process and the implicate order (2005)] The existence of the hologram has shown how locality can be carried as a relationship. For example in forming a hologram, the neighbourhood relations in an object are mapped into the whole domain of the hologram. If this were not so we would not see an image of the original object when only a partial region of the hologram is illuminated. The significant feature of the hologram is that local regions of the original object are mapped into every region of the hologram. Thus locality is being carried in a nonlocal or, better still, alocal way. In other words, locality can be regarded as a relationship that does not have to be represented locally. This example shows us that locality is not necessarily absolute. Thus we can, in fact, have processes which do not have to be displayed in spacetime, but nevertheless carry the spacetime properties implicitly within their structure, in this way we have the possibility that spacetime could emerge from this deeper process. These ideas tie in very nicely with the notion of the holomovement that we introduced earlier. Not only are particles to be thought of as stabilities in this movement, but space-time itself is to be an abstraction from the holomovement. This means we must not view the vacuum as ‘empty space-time’. It is not empty, but is full of undifferentiated activity. The particles are then regarded as being mere ‘ripples’ or invariant features sitting on top of this holomovement. Electrons are not little rocks, nor are quarks. We must not fall into this trap. Whitehead (1926) has warned us against the fallacy of misplaced concreteness! We must regard particles as quasi-stable, semi-autonomous forms on the background activity. They depend on the background movement and are part of the whole process. So there is no ultimate separate substance, there are only concentrations of energy. Furthermore since all quasi-invariant forms can be seen as being connected through the background activity, we now have a possible way of understanding how nonlocality can arise. The particles are not separate entities in interaction but are distinguished forms arising from a common interconnected background.
>
>[From the Heisenberg picture to Bohm (2002)] Does taking the quantum potential lead to significantly different ways of looking at quantum phenomena? I think it does. By focusing on the quantum potential we saw the significance of quantum entanglement in an entirely new way, namely its striking implication for quantum non-locality (see Bohm and Hiley 1975). The importance of this was also recognised by Bell (1987) and led him to think about the problem, which eventually led him to derive his famous inequalities. The role of the quantum potential in all this was brought out particularly clearly in the calculations of Dewdney, Holland and Kyprianidis (1987) for the case of two coupled spin-half systems in an entangled state. Their work showed how the dramatic appearance of the non-local was mediated by the quantum potential. Normally such calculations explaining an experimental result would be greeted as an indication of the success of the theory but in this case, for reasons that I fail to understand, they are regarded as meaningless. For some it is the appearance of non-locality that appears to further weaken the appeal of the approach in spite of experimentally confirmed violations of the Bell inequality. It is sometimes regarded as the Achilles heel of the theory. However for me, it is not its weakness but its strength. It offers a detailed explanation of the Einstein-Podolsky-Rosen paradox rather than leaving it all rather vague and mysterious. It is an explanation that enabled me to make much more sense of Bohr’s answer to EPR puzzle.

Maudlin, Space-time in the quantum world (1996).
>Any theory which predicts violations of Bell’s inequality for events at spacelike separation must, in this sense, be non-local. And if there are reliable violations of Bell’s inequality for events at spacelike separation, then any physically adequate theory must be non-local. This is the moral of Bell’s work, though it has taken a remarkably long time for that moral to be widely recognized. Bell’s work is not particularly about “hidden variables” theories, nor particularly about deterministic theories, nor particularly about “realistic” theories (whatever that might mean). The theorems apply to hidden variables theories and to deterministic theories and to realistic theories because they apply to all theories. But the bottom line is that locality (not hidden variables or determinism or realism) cannot be reconciled with reliable violations of the inequalities at spacelike separation. […] Experiments of Aspect et al. demonstrate violations of the inequalities at spacelike separation. […] It is not uncommon to interpret the Theory of Relativity as forbidding something from going faster than light. Elsewhere (1994), I have examined four distinct possibilities: no superluminal matter or energy transport; no superluminal signals; no superluminal causal influences; no superluminal informational exchange. I have also argued that violations of Bell’s inequality for events at spacelike separation do not demand violations of the first two constraints and do demand violations of the second two.

### Time: creation, direction and symmetry <a name="p412"></a>

Time is the indefinite continued progress of existence and events that occur in an apparently irreversible succession from the past, through the present, into the future. Time has long been an important subject of study in religion, philosophy, and science, but defining it in a manner applicable to all fields without circularity has consistently eluded scholars. The operational definition of time does not address what the fundamental nature of it is. It does not address why events can happen forward and backwards in space, whereas events only happen in the forward progress of time. Investigations into the relationship between space and time led physicists to define the spacetime continuum. General Relativity is the primary framework for understanding how spacetime works. Through advances in both theoretical and experimental investigations of space-time, it has been shown that time can be distorted, particularly at the edges of black holes.

Wheeler, Geons, black holes, and quantum foam: a life in physics (1998).
>According to a graffito that I once saw in the men’s room of the Pecan Street Cafe in Austin, Texas, “Time is nature’s way to keep everything from happening all at once”. Maybe that’s as good a definition as any. Time is, in fact, an immensely complex idea that sits at the core of critical unanswered questions about the universe and existence, questions I can’t stop pondering.

As we saw in the first chapter, philosophers of classical antiquity held that the universe had an infinite past, which caused problems for medieval Christian, Jewish and Islamic philosophers who were unable to reconcile the Aristotelian conception of the eternal with the Abrahamic view of creation. As a result, a variety of logical arguments for the universe having a finite past were developed. Kepler used the dark night sky to argue for a finite universe while Newton described large-scale motion throughout the universe. Then, Einstein’s theory of general relativity was found to admit no static cosmological solutions. A static universe is a cosmological model in which the universe is both spatially infinite and temporally infinite, and space is neither expanding nor contracting. In contrast to this model, Einstein proposed a temporally infinite but spatially finite model as his preferred cosmology during 1917, after he added a positive cosmological constant to his equations of general relativity. This constant counteract the attractive effects of gravity on ordinary matter, which would otherwise cause a static, spatially finite universe to either collapse or expand forever. Georges Lemaître (1894-1966, Belgian) proposed an expanding model for the universe in 1927 and shortly after, in 1929, Edwin Hubble (1889-1953, American) provided a comprehensive observational foundation for this theory. This led Einstein to declare that his cosmological model, and especially the introduction of the cosmological constant, was his “biggest blunder”.

Hubble’s Law suggested that the universe was expanding and that, when viewed on sufficiently large distance scales, has no preferred directions or preferred places. Hubble’s idea allowed for two opposing hypotheses to be suggested. One was Lemaître’s Big Bang, advocated and developed by George Gamow (1904-1968, Ukrainian). The other model was Fred Hoyle’s (1915-2001, English) Steady State theory, in which new matter would be created as the galaxies moved away from each other. In this model, the universe is roughly the same at any point in time. It was actually Hoyle who coined the name of Lemaître’s theory, referring to it as “this ‘big bang’ idea” in 1949. From around 1950 to 1965, the support for these theories was evenly divided. One such example can be read in de Broglie. 

de Broglie, Une tentative d’interprétation causale et non linéaire de la mécanique ondulatoire (1956).
>One could interpret by the aging of the corpuscles the apparent “recession” of the spiral nebulae (instead of invoking the hypothetical “expansion of the Universe”).

In 1964, the cosmic microwave background was discovered, which was crucial evidence in favor of the Big Bang. As a result, through the 1970s and 1980s, most cosmologists accepted it, but several puzzles remained. The theory was not very strongly confirmed. Huge advances in Big Bang cosmology were made in the 1990s and the early 21st century, as a result of major advances in telescope technology in combination with large amounts of satellite data.

One of the common misconceptions about the Big Bang model is that it fully explains the origin of the universe. However, the model does not describe how energy, time, and space were caused, but rather it describes the emergence of the present universe from an ultra-dense and high-temperature initial state. According to the theory, the universe at the beginning was very hot and very small, and since then it is expanding and cooling down. Extrapolation of the expansion of the universe backwards in time using general relativity yields an infinite density and temperature at a finite time in the past. A major actual problem is thus to solve these infinities that show the limits of the current models. In Big Bang cosmology, the Planck epoch or Planck era is the earliest stage of the Big Bang, before the time passed was equal to the Planck time, approximately 10−43 seconds. There is no currently available physical theory to describe such short times, and it is not clear in what sense the concept of time is meaningful for values smaller than the Planck time.

Wheeler, Geons, black holes, and quantum foam: a life in physics (1998).
>We really have to say that space and time came into existence, along with matter and energy and the laws of physics, at the moment of the Big Bang. If the universe expands to a maximum size, starts contracting, and eventually collapses to a fiery death – a fate that seems likely to me and to some other theorists, even though the evidence for it is still weak – then time, and space too, will end in this Big Crunch. I can reach no conclusion other than this: there was no “before” before the Big Bang, and there will be no “after” after the Big Crunch. But we don’t have to look back to the Big Bang to speak of the beginning of time or look forward to a Big Crunch to speak of the end of time. Black holes are more than likely coming into existence throughout the universe right now, and some may be evaporating. Every black hole brings an end to time and space and the laws of physics in itself as surely as the Big Crunch will bring an end to the universe as a whole. […] Theory suggests also that black holes of incredibly small size, at the scale of the so-called Planck length, are forming and dissolving all the time by the trillion, within the dimensions of every elementary particle. At that scale, with spacetime churned into quantum foam, space and time in fact lose their meaning. When we blend the two greatest theories of the twentieth century, quantum theory and general relativity, we have to conclude that time is a secondary concept, a derived concept. It has meaning only at a scale large compared with the Planck length and only well away from black holes, the Big Bang, or the Big Crunch. It is not a river that rolls inexorably forward. It is not a lake across which we glide. It is more to be compared with temperature or with entropy, concepts that take their meaning only when large numbers of particles are involved. Time, we must conclude, is of statistical origin, valid only when dimensions are large enough and when conditions are not too extreme.

Hawking, Brief answers to the big questions (2018).
>The big question in cosmology in the early 1960s was did the universe have a beginning? Many scientists were instinctively opposed to the idea, because they felt that a point of creation would be a place where science broke down. One would have to appeal to religion and the hand of God to determine how the universe would start off. […] If you accept, as I do, that the laws of nature are fixed, then it doesn’t take long to ask: what role is there for God? This is a big part of the contradiction between science and religion, and although my views have made headlines, it is actually an ancient conflict. One could define God as the embodiment of the laws of nature. […] The one remaining area that religion can now lay claim to is the origin of the universe. […] I think the universe was spontaneously created out of nothing, according to the laws of science. The basic assumption of science is scientific determinism. The laws of science determine the evolution of the universe, given its state at one time. These laws may, or may not, have been decreed by God, but he cannot intervene to break the laws, or they would not be laws. That leaves God with the freedom to choose the initial state of the universe, but even here it seems there may be laws. So God would have no freedom at all. Despite the complexity and variety of the universe, it turns out that to make one you need just three ingredients. […] The first is matter – stuff that has mass. […] The second thing you need is energy. […] The third thing we need to build a universe is space. Lots of space. […] Einstein realised something quite extraordinary: that two of the main ingredients needed to make a universe – mass and energy – are basically the same thing. […] So instead of three ingredients, we can now say that the universe has just two: energy and space. So where did all this energy and space come from? The answer was found after decades of work by scientists: space and energy were spontaneously invented in an event we now call the Big Bang. […] The great mystery at the heart of the Big Bang is to explain how an entire, fantastically enormous universe of space and energy can materialize out of nothing. The secret lies in one of the strangest facts about our cosmos. The laws of physics demand the existence of something called ‘negative energy’. To help you get your head around this weird but crucial concept, let me draw on a simple analogy. Imagine a man wants to build a hill on a flat piece of land. The hill will represent the universe. To make this hill he digs a hole in the ground and uses that soil to dig his hill. But of course he’s not just making a hill – he’s also making a hole, in effect a negative version of the hill, so it all perfectly balances out. This is the principle behind what happened at the beginning of the universe. When the Big Bang produced a massive amount of positive energy, it simultaneously produced the same amount of negative energy. In this way, the positive and the negative add up to zero, always. It’s another law of nature. So where is all this negative energy today? It’s in the third ingredient in our cosmic cookbook: it’s in space. This may sound odd, but according to the laws of nature concerning gravity and motion – laws that are among the oldest in science – space itself is a vast store of negative energy. Enough to ensure that everything adds up to zero. […] The endless web of billions upon billions of galaxies, each pulling on each other by the force of gravity, acts like a giant storage device. The universe is like an enormous battery storing negative energy. The positive side of things – the mass and energy we see today – is like the hill. The corresponding hole, or negative side of things, is spread throughout space. So what does this mean in our quest to find out if there is a God? It means that if the universe adds up to nothing, then you don’t need a God to create it. The universe is the ultimate free lunch. […] Down to the atomic level and right down to the sub-atomic level, and you enter a world where conjuring something out of nothing is possible. At least, for a short while. That’s because, at this scale, particles such as protons behave according to the laws of nature we call quantum mechanics. And they really can appear at random, stick around for a while and then vanish again, to reappear somewhere else. Since we know the universe itself was once very small – perhaps smaller than a proton – this means something quite remarkable. It means the universe itself, in all its mind-boggling vastness and complexity, could simply have popped into existence without violating the known laws of nature. From that moment on, vast amounts of energy were released as space itself expanded – a place to store all the negative energy needed to balance the books. But of course the critical question is raised again: did God create the quantum laws that allowed the Big Bang to occur? In a nutshell, do we need a God to set it up so that the Big Band could bang? I have no desire to offend anyone of faith, but I think science has a more compelling explanation than a divine creator. […] The laws of nature itself tell us that not only could the universe have popped into existence without any assistance, like a proton, and have required nothing in terms of energy, but also that it is possible that nothing caused the Big Bang. Nothing. The explanation lies back with the theories of Einstein, and his insights into how space and time in the universe are fundamentally intertwined. Something very wonderful happened to time at the instant of the Big Bang. Time itself began. To understand this mind-boggling idea, consider a black hole floating in space. A typical black hole is a star so massive that it has collapsed in on itself. It’s so massive that not even light can escape its gravity, which is why it’s almost perfectly black. Its gravitational pull is so powerful, it warps and distorts not only light but also time. […] If there were such a God, I would like to ask however did he think of anything as complicated as M-theory in eleven dimensions. […] Inside the black hole time itself doesn’t exist. And that’s exactly what happened at the start of the universe. […] The role played by time at the beginning of the universe is, I believe, the final key to removing the need for a grand designer and revealing how the universe created itself. As we travel back in time towards the moment of the Big Bang, the universe gets smaller and smaller and smaller, until it finally comes to a point where the whole universe is a space so small that it is in effect a single infinitesimally small, infinitely dense black hole. And just with modern-day black holes, floating around in space, the laws of nature dictate something quite extraordinary. They tell us that here too time itself must come to a stop. You can’t get to a time before the Big Bang because there was no time before the Big Bang. We have finally found something that doesn’t have a cause, because there was no time for a cause to exist in. For me this means that there is no possibility of a creator, because there is no time for a creator to have existed in. […] As far as we can tell, the universe goes on in space for ever and is much the same no matter how far it goes on. Although the universe seems to be much the same at each position in space, it is definitely changing in time. This was not realised until the early years of the last century. Up to then, it was thought the universe was essentially constant in time. It might have existed for an infinite time, but that seemed to lead to absurd conclusions. If stars had been radiating for an infinite time, they would have heated up the universe until it reached their own temperature. Even at night, the whole sky would be as bright as the Sun, because every line of sight would have ended either on a star or on a cloud of dust that had been heated up until it was as hot as the stars. So the observation that we have all made, that the sky at night is dark, is very important. It implies that the universe cannot have existed for ever, in the state we see today. Something must have happened in the past to make the stars turn on a finite time ago. The light from very distant stars wouldn’t have had time to reach us yet. This would explain why the sky at night isn’t glowing in every direction.

In the theory of inflation, the rapid initial expansion of the universe is caused by a hypothetical particle called the inflaton. The exact properties of this particle are not fixed by the theory but should ultimately be derived from a more fundamental theory. This is proposed by string theorists.

Greene, The elegant universe (2000).
>The Universe is expanding. We do not know if this cosmic growth will continue forever or if the expansion will one day slow down and then stop, reverse and produce a cosmic implosion. Astronomers and astrophysicists seek to answer this question by experience, because the answer involves a quantity that we should, in principle, be able to measure: the average density of matter in the Universe. If the average density of matter exceeds a so-called critical density, worth approximately 10-29 grams per cubic centimeter – roughly five atoms of hydrogen per cubic meter – then the gravitational force which permeates the cosmos will be large enough to brake and reverse expansion. If the average density of matter is below this critical value, the gravitational pull will be too weak to stop the expansion, and it will continue indefinitely. (Based on your own observations of our world, you might think that the average density of the Universe far exceeds the critical value. But keep in mind that matter – like money – tends to gather together.) […] A meticulous analysis of the distribution of galaxies has given astronomers a fairly precise idea of the average amount of visible matter in the Universe. It turns out that it is much lower than the critical value. But many clues, both theoretical and experimental, seem to indicate that the Universe is filled with invisible dark matter. This material does not participate in the nuclear fusion processes that power the stars, so it does not produce light; it is therefore undetectable by astronomers’ telescopes. No one yet knows what this invisible matter is, much less how much there is. To date, the fate of our Universe, currently expanding, remains a mystery. […] Brandenberger, Vafa and other researchers have exploited these ideas to propose a rewriting of the laws of cosmology in which the big-bang like the big-crunch, instead of involving a zero-size universe, would imply a universe of which all dimensions would reach Planck’s length. This very attractive proposition would avoid the mathematical, physical and logical puzzles of a universe which would emanate from (or collapse in) an infinitely dense point. It is certainly difficult to imagine the entire Universe compressed inside a tiny Planckian shell. But the idea of compressing it to a point, without any extension, is completely beyond comprehension. String cosmology is still in its infancy, but it’s a very promising area that may well offer an easier-to-accept alternative than the usual big-bang model. […] Gasperini and Veneziano proposed their own version of string cosmology, of which certain characteristics are common to the scenario just described, while others differ considerably from it. Like Brandenberger and Vafa, they rely on the fact that string theory has a minimum length to avoid the infinites, in temperature and energy density, of standard and inflationary theories. But, instead of concluding that the Universe was born from an extremely hot Planckian shell, Gasperini and Veneziano propose that there can be a whole prehistory of the Universe – which would start long before what we called until here time zero – which would have produced this cosmic embryo of Planck’s length. In this scenario, called the pre-big-bang, the Universe starts from a state entirely different from the big bang. The works of Gasperini and Veneziano assume that, rather than being incredibly hot and rigidly twisted into a tiny grain of space, the Universe was first cold and essentially infinite. The equations of string theory then indicate – much like the era of Guth’s inflation – that an instability has led all points of the Universe to move away from each other at full speed. Gasperini and Veneziano have shown that this has gradually curved the space, which has led to a huge increase in temperature and energy density. After a while, a millimeter-sized three-dimensional region within this vast expanse would look very much like the dense, warm fragment of Guth’s inflation. With the standard expansion of ordinary big-bang cosmology, this fragment could then account for the entire universe with which we are familiar. Furthermore, since the pre-big-bang era had its own inflationary expansion, Guth’s solution to the horizon problem was automatically part of this cosmological scenario. As Veneziano says, “string theory offers us its version of inflationary cosmology on a silver platter”.

Cosmic inflation is a theory of exponential expansion of space in the early universe. The inflationary epoch lasted from 10−36 seconds after the conjectured Big Bang singularity to some time between 10−33 and 10−32 seconds after the singularity. Following the inflationary period, the universe continued to expand, but the expansion was no longer accelerating. The detailed particle physics mechanism responsible for inflation is unknown. The basic inflationary paradigm is accepted by most physicists, as a number of inflation model predictions have been confirmed by observation; however, a substantial minority of scientists dissent from this position. Penrose advanced a model in the framework of general relativity where the universe iterates through infinite cycles, with the future of each previous iteration being identified with the Big Bang singularity of the next. The result is a new solution to Einstein’s equations, which Penrose takes to represent the entire universe, and which is composed of a sequence of sectors that Penrose calls “aeons”.

Penrose and Hameroff, Consciousness in the Universe (2011).
>The recently proposed cosmological scheme of conformal cyclic cosmology (CCC) (Penrose 2010) also has some relevance to these issues. CCC posits that what we presently regard as the entire history of our universe, from its Big-Bang origin (but without inflation) to its indefinitely expanding future, is but one aeon in an unending succession of similar such aeons, where the infinite future of each matches to the big bang of the next via an infinite change of scale. A question arises whether the dimensionless constants of the aeon prior to ours, in the CCC scheme, are the same as those in our own aeon, and this relates to the question of whether sentient life could exist in that aeon as well as in our own.

The arrow of time, also called time’s arrow, is the concept positing the “one-way direction” or “asymmetry” of time. It was developed in 1927 by Arthur Eddington (1882-1944, English), and is an unsolved general physics question. 

Hawking, A brief history of time (1988).
>According to a mathematical theorem, any theory that obeys both Quantum Mechanics and Relativity must also obey the combined CPT symmetry. In other words, the universe should behave identically if we replaced the particles with anti-particles, if we took its mirror image and if we reversed the direction of time. But Cronin and Fitch have shown that if we replaced the particles with anti-particles and that we take the mirror image without reversing the direction of time, the universe would not behave in the same way. The laws of physics would change if the direction of time were reversed (they therefore do not obey the T symmetry). The primitive universe certainly does not obey the T symmetry: as time passes, the universe expands – if it went the other way, the universe would contract. And since there are forces which do not obey the T symmetry, this allows them, while the universe is expanding, to make more anti-electrons becoming quarks than electrons becoming anti-quarks. Then, as the universe expands and cools, anti-quarks annihilate with quarks, but since there are more quarks than anti-quarks, we could see a slight excess of quarks. It is they who form the matter that we see today and of which we ourselves are made up.

Amongst the many attempts to provide an interpretation of quantum theory to account for its predictive and explanatory success, one class of interpretations hypothesizes backward-in-time causal influences – retrocausality – as the basis for constructing a convincing foundational account of quantum theory. The first proposal of retroactive influence in quantum mechanics comes from a suggestion made by Wheeler and Feynman in 1945. They were led to this idea while considering the potentially classical origins of some of the difficulties of quantum theory. Consider the following problem from classical electrodynamics: an accelerating electron emits electromagnetic radiation and, through this process, the acceleration of the electron is damped. The core of Wheeler and Feynman’s proposed “absorber theory of radiation” is a suggestion that the process of electromagnetic radiation emission and absorption should be thought of as an interaction between a source and an absorber rather than as an independent elementary process. The crucial point to note about the Wheeler-Feynman schema is that due to the advanced field of the absorber, the radiative damping field is present at the source at exactly the time of the initial acceleration. This schema of advanced and retarded waves now forms the basis for the most fully-formed retrocausal model of quantum mechanics, the transactional interpretation.

The second key idea in the historical development of retrocausality in quantum mechanics occurs around the same time as Wheeler and Feynman’s absorber theory. French physicist Costa de Beauregard, a student of de Broglie, noticed a potential objection to the reasoning found in the EPR argument. His proposal was that two distant systems could “remain correlated by means of a successively advanced and retarded wave”; that is, one system could influence, via an advanced wave, the state of the combined systems in their common past, which then, via a retarded wave, could influence the state of the distant system in a kind of “zigzag” through spacetime. When Costa de Beauregard in 1947 suggested this response to the EPR argument to his then supervisor de Broglie, de Broglie was “far from willing to accept” the proposal and forbade Costa de Beauregard to publish his unorthodox idea. However, in 1948 Feynman had developed his eponymous diagrams in which antiparticles were to be interpreted as particles moving backward-in-time along the particle trajectories, and so by 1953 de Broglie had endorsed the publication of Costa de Beauregard’s response. On the seeming craziness of the proposal, Costa de Beauregard claims: “Today, as the phenomenon of the EPR correlations is very well validated experimentally, and is in itself a ‘crazy phenomenon’, any explanation of it must be ‘crazy’”.

Wheeler, Geons, black holes, and quantum foam: a life in physics (1998).
>For complex events, time does have an arrow. Most series of events occur in one direction, but not the other. For just about everything in the world around you, you can easily tell the difference between forward-in-time and backward-in-time. But the lesson of nineteenth-century physics is that this difference is a matter of probabilities, not absolutes. One direction might be quite likely, the other direction incredibly, ridiculously unlikely. Yet unlikely is not quite the same as impossible. So Newtonian time-reversal invariance is saved, even if it doesn't seem to have much to do with the world that we experience. […] The second law of thermodynamics relates probabilities to time’s arrow. It says that any system left to itself (free of outside influences) will tend toward greater disorder. There are more ways to be disorderly than to be orderly. Disorder is therefore more probable than order. If the system is quite complex, the probability of disorder exceeds the probability of order by an enormous factor – so enormous that, for all practical purposes, there is only one direction of change (for an isolated system): from order to disorder. […] The intriguing thought that follows from these considerations is that we are aware of a one-way flow of time only because we are ourselves complex systems interacting with other complex systems. We remember the past and not the future not because there is any fundamental asymmetry in time but because of the overwhelming disparity between the likely and the unlikely in everything that we are and do and see. Back when Dick Feynman and I were talking about electrons moving with equal ease backward and forward in time, we realized that such a way of thinking made sense because of the extreme simplicity of the electron. You can tell by looking into a person’s face something about what that person has been through. You cannot tell anything about an electron’s history by looking at it. Every electron is exactly like every other electron, unscarred by its past, not blessed with a memory – of either the human or computer variety. The electron pays for its freedom to move forward and backward in time by remembering neither future nor past. We remember the past and are trapped in one-way motion through time. In speaking of nineteenth-century physics, I am speaking really of the sciences of thermodynamics and statistical mechanics. Developments in those fields showed how the statistics of large numbers can convert the time-symmetric laws that govern simple events into the time-asymmetric laws that we see governing complex events. […] For more than half of the twentieth century, it appeared that time symmetry in the small remained the rule, with time asymmetry being an artifact of complexity. As particles were discovered, as nuclei were explored, as quantum electrodynamics evolved, at first nothing marred this picture of perfect time symmetry for all basic laws. In the work that Feynman and I did on action at a distance, for instance, we found that the apparent one-way flow of radiation – forward in time, not backward – could be entirely accounted for by the large-scale distribution of absorbing mass in the universe, lots of it. It required no time asymmetry in the fundamental laws of electrodynamics. Then in 1964, James Cronin (now at the University of Chicago) and Val Fitch, with their colleagues James Christenson and René Turlay, discovered that time-reversal invariance fails for the decay of the K meson, or kaon. Here, for the first time, was an example of time asymmetry at the elementary level of single particles, not the level of complex systems.

Retrocausality has been claimed to occur in some psychic phenomena such as precognition. For example, in 2011, psychologist Daryl Bem (1938-, American) showed experimental subjects two sets of curtains and instructed them to guess which one had a picture behind it, but did not display the picture behind the curtain until after the subject made their guess. Some results showed a higher margin of success for a subset of erotic images, with subjects who identified as “stimulus-seeking” in the pre-screening questionnaire scoring even higher. However the methodology has been strongly criticized.

Stapp, Retrocausation in quantum mechanics and the effects of minds on the creation of physical reality (2017).
>The key step in the creation of quantum mechanics during 1925 by Heisenberg and his colleagues was to recognize and emphasize the essential dynamical role of mental properties in the creation of our mental empirical findings. This basic feature of quantum mechanics was cast into rigorous mathematical form by John von Neumann, and was made a central feature of contemporary relativistic quantum field theory by the work of Tomonaga and Schwinger. That theory is causally strictly forward in time. But it is explained here how it can nevertheless accommodate the seeming backward-in-time causal effects reported by D.J. Bem, and many others, by means of a slight biasing of the famous Born Rule. The purpose of this communication is to explain how those reported retrocausal findings can be explained by a strictly forward-in-time and nearly orthodox causal dynamics that, however, permits the Born Rule to be slightly biased under certain conditions. […] This proposed experiment is a variation of the well-known “erotic picture” retrocausation experiment designed and executed by Daryl J. Bem. In this experiment a human subject, often (and herein) called “Alice”, is seated in front of a screen showing two curtains, and is told that behind one of them lies an interesting picture, and behind the other lies a boring blank wall; and that she should choose (i.e., guess), on the basis of her “hunch” or “feeling”, the curtain behind which the interesting picture lies. After a delay, that chosen curtain will be removed, so that Alice can see what lies behind it. The other curtain is left in place. Soon after her choice of curtain is made and recorded, two well-tested 50-50 random number generators (RNGs) are activated. Their outcomes control mechanisms that, in each trial, place either an erotic picture or a non erotic picture, or an image of a blank wall behind each of the two curtains. Then later, at the end of the trial, Alice looks at what lies behind the curtain she has chosen. The set-up is such that there is, by virtue of the known properties of the RNGs, a 50-50 chance that if she sees a picture, not a wall, she will see an erotic picture, not a non-erotic picture. But the reported empirical finding is that Alice tends to choose an erotic picture instead of a non-erotic picture roughly 53% of the time, in a long sequence of trials. The simplest classical explanation (apart from faulty experimental design or execution) would seem to be that the RNGs are somehow producing a physical effect that acts backward-in-time and influences Alice’s earlier choice of curtain in order to tend to make that chosen curtain be the one behind which the erotic picture will later appear. Such a backward-in-time effect is appropriately called “retrocausation”. An alternative but non-retrocausal possible explanation of this unexpected correlation (of Alice’s earlier choice of screen with, effectively, the later random choices of the RNGs) is that this correlation, like everything in the quantum universe, is due to features of the underlying quantum process of the creation of the unfolding of reality. According to Tomonaga and Schwinger based relativistic quantum field theory (RQFT), the evolution of the physically described world, represented by the quantum mechanical state of the universe, is strictly forward in time. […] Thus we have an ordered sequence of space-time universes, each containing a 3D space-like surface “now”. Relative to this surface, along which a collapse occurs, there are two different “pasts”: the “actual past” that was there before the collapse, and the “effective past” that is defined as the backward-in-time continuation via a relevant Schrödinger equation, of the immediate future that exists just after the collapse. Thus the “effective past” keeps changing even though the causal dynamical process is strictly forward-in-time. This is the orthodox temporal structure defined by the relativistic quantum field theory developed by Tomonaga and Schwinger. There is no actual backward-in-time action, but merely a sequence of piecewise increments to the expanding past. In the Bem experiment each collapse can be influenced by the state of mind of the associated observer, and by the current physical state of the universe, and by the reasons behind the choice made by Nature, but not by the yet-to-be-determined future. If Charlie were to be replaced by, say, the IBM smart (but presumed mindless) computer “Watson” then, according to the orthodox theory, no quantum collapse would occur. Indeed, an experiment without conscious Charlie, but with a mindless Watson instead, is roughly equivalent to the normal Bem experiment itself. That is because the macroscopic action of the RNG and the associated recording devices is effectively already including a simple mechanical observer. So unless the high complexity of the “observer” is the essential cause of the collapse, instead of a mind per se as the orthodox theory asserts, then the usual Bem experiment is already sufficient to cover the case in which conscious Charlie is replaced by a mindless computer. According to the orthodox theory it is “mind” that matters: both the minds of conscious beings, and a capital MIND that can be gratuitously associated with Nature’s choice of response, which is not deemed by orthodox theory to be determined by purely physical causes alone.

In 1979, Benjamin Libet (1916-2007, American) performed a famous experiment meant to demonstrate that the unconscious electrical processes in the brain, called readiness potential discovered in 1964, precede conscious decisions to perform volitional, spontaneous acts, implying that unconscious neuronal processes precede and potentially cause volitional acts which are retrospectively felt to be consciously motivated by the subject. The experiment has caused controversy not only because it challenges the belief in free will, but also due to a criticism of its implicit assumptions. It has also inspired further study of the neuroscience of free will.

Penrose and Hameroff, Consciousness in the Universe (2011).
>In the 1970s neurophysiologist Benjamin Libet performed experiments on patients having brain surgery while awake, i.e. under local anesthesia (Libet 1979). Able to stimulate and record from conscious human brain, and gather patients’ subjective reports with precise timing, Libet determined that conscious perception of a stimulus required up to 500 msec of brain activity poststimulus, but that conscious awareness occurred at 30 msec post-stimulus, i.e. that subjective experience was referred ‘backward in time’. […] Measurable brain activity correlated with a stimulus often occurs several hundred msec after that stimulus, as Libet showed. Yet in activities ranging from rapid conversation to competitive athletics, we respond to a stimulus (seemingly consciously) before the above activity that would be correlated with that stimulus occurring in the brain. This is interpreted in conventional neuroscience and philosophy (e.g. Dennett 1991; Wegner 2002) to imply that in such cases we respond non-consciously, on auto-pilot, and subsequently have only an illusion of conscious response. The mainstream view is that consciousness is epiphenomenal illusion, occurring after-the-fact as a false impression of conscious control of behavior. We are merely ‘helpless spectators’ (Huxley 1986).

Libet’s experiments suggest to some that unconscious processes in the brain are the true initiator of volitional acts, and free will therefore plays no part in their initiation. If unconscious brain processes have already taken steps to initiate an action before consciousness is aware of any desire to perform it, the causal role of consciousness in volition is all but eliminated, according to this interpretation. For instance, an interpretation is that conscious experience takes some time to build up and is much too slow to be responsible for making things happen. Libet finds that conscious volition is exercised in the form of “the power of veto” (sometimes called “free won’t”). Libet’s early theory, resting on study of stimuli and sensation, was found bizarre by some commentators, including Patricia Churchland (1943-, Canadian) due to the apparent idea of backward causation. Libet argued that data suggested that we retrospectively “antedate” the beginning of a sensation to the moment of the primary neuronal response. People interpreted Libet’s work on stimulus and sensation in a number of different ways. John Eccles (1903-1997, Australian, 1963 Nobel Prize in Physiology or Medicine) presented Libet’s work as suggesting a backward step in time made by a non-physical mind. Such interpretations had impacts on non-conventional or New Age views, such as those of the Fundamental Fysiks Group in the 1970s that included Fritjof Capra, Henry Stapp and Jack Sarfatti.

Sarfatti, Retrocausality and signal nonlocality in consciousness and cosmology (2011).
>There is now ample independent reproducible evidence in many published papers (Bem 2011; Bierman 2008; Libet 1979; Radin 2004) for back-from-the-future “presponse” in human consciousness (Stapp 1996). Yakir Aharonov (1998) in his back-from-the-future version of quantum theory argues that unlike classical mechanics, in quantum mechanics the initial pre-selected and final post-selected conditions are truly independent. […] Aharonov and his students speculate that a final condition in our future retro-causally influences the past evolution of life on Earth. […] The discovery of dark energy accelerating the expansion of our observable universe is evidence for the idea that we are retro-causal back-from-the-future hologram image computations from our future cosmological event horizon (Davis 2003) whose increasing area-entropy from the moment of inflation almost trivially explains the irreversible arrow of time. […] Retrocausality was found in the physics of radiation reaction by Dirac in the 1930s and applied by Wheeler and Feynman (1945) for classical electrodynamics and to quantum theory. The imposition of a perfect total future absorber for all light ensures that the world looks as if there is only past cause and future effect.

### Light <a name="p413"></a>

The photon is a type of elementary particle. It is the quantum of the electromagnetic field including electromagnetic radiation such as light and radio waves, and the force carrier for the electromagnetic force. The invariant mass of the photon is zero; it always moves at the speed of light in a vacuum. Current commonly accepted physical theories imply or assume the photon to be strictly massless. If the photon was not a strictly massless particle, it would not move at the exact speed of light in vacuum, c. Its speed would be lower and depend on its frequency. Relativity would be unaffected by this; the so-called speed of light, c, would then not be the actual speed at which light moves, but a constant of nature which is the upper bound on speed that any object could theoretically attain in spacetime. Thus, it would still be the speed of spacetime ripples (gravitational waves and gravitons), but it would not be the speed of photons.

Light exerts physical pressure on objects in its path, a phenomenon which can be deduced by Maxwell’s equations, but can be more easily explained by the particle nature of light: photons strike and transfer their momentum. Light pressure is equal to the power of the light beam divided by c. Though, due to the magnitude of c, the effect of light pressure is negligible for everyday objects.

Photons are emitted in many natural processes, such as in an atomic transition to another energy, as explained by Bohr.

Bohr, Atomic physics and human knowledge (1957).
>Any well-defined change of an atom is an individual process consisting in a complete transition of the atom from one of its so-called stationary states to another. […] Just one light quantum is exchanged in a transition process by which light is emitted or absorbed by an atom.

One can find below the relation of photons with one another.

Wigner, Remarks on the mind-body question (1995).
>Light quanta do not influence each other directly but only by influencing material bodies which then influence other light quanta. Even in this indirect way, their interaction is appreciable only under exceptional circumstances. Similarly, consciousnesses never seem to interact with each other directly but only via the physical world. Hence, any knowledge about the consciousness of another being must be mediated by the physical world. At this point, however, the analogy stops. Light quanta can interact directly with virtually any material object but each consciousness is uniquely related to some physico-chemical structure through which alone it receives impressions. There is, apparently, a correlation between each consciousness and the physico-chemical structure of which it is a captive, which has no analogue in the inanimate world.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>It is well known that the eye can be sensitive to a few quanta at a time, but the reception of a small number of quanta gives only the vaguest sense of optical stimulation. Meaningful perception requires a large number of quanta and therefore, along the lines we have already explained this will imply an essentially classical behaviour. Meaningful communication between people also requires classically describable processes involving a large number of quanta. Thus it is not that we are assuming that the brain responds only to the states of particles and not their wave functions. Rather we are simply calling attention to the observed fact that meaningful sense perception and communication has to go through the classical level in which the effects of this wave function can be consistently left out of account.

Greene, The elegant universe (2000).
>Gauge bosons – photons, gluons, weak bosons – provide a microscopic description of the interactions they carry. Take the example of two particles with the same charge. These two particles repel each other, which can be roughly explained by using the electric field that accompanies each of them, like the halo of their electric quintessence. They repel each other because their force fields repel each other. In fact, the microscopic explanation is quite different. The electromagnetic field is like a cloud of photons whose charged particles bombard each other. Imagine two ice skaters throwing bowling balls and preventing themselves from moving forward. This is a rough analogy of how two charged particles influence each other through the exchange of small “packets” of light. An essential aspect is however lacking in the image of skaters since, for them, the exchange of bowling balls is always “repulsive” (it separates them). But the exchange of photons between particles of opposite charges results from an attractive force. It is as if the photon transmitted less the force itself than a message indicating to the recipient the behavior to adopt in the presence of this force. Between particles of the same charge, the photon delivers the message “separate”. Between particles of opposite charge, it transmits “join you”. For this reason, it is sometimes said that the photon is the intermediate particle of the electromagnetic force. The same goes for gluons and weak bosons, intermediate particles of strong and weak interactions. The strong nuclear force confines the quarks inside protons and neutrons, by exchanging gluons between quarks. Gluons are like “glue” which ensures the cohesion of subatomic particles. The reliable nuclear force is responsible for the disintegration of particles in certain forms of radioactivity; its mediators are the weak bosons.


### Limits <a name="p414"></a>

Elementary particles are subatomic (smaller than the atom) particles with no sub structure, thus not composed of other particles. Particles currently thought to be elementary by the Standard Model include the fundamental fermions (quarks, leptons, antiquarks, and antileptons), which generally are “matter particles” and “antimatter particles”, as well as the fundamental bosons (gauge bosons and the Higgs boson), which generally are “force particles” that mediate interactions among fermions. Although experimental evidence overwhelmingly confirms the predictions derived from the Standard Model, some of its parameters were added arbitrarily, not determined by a particular explanation, which remain mysterious, for instance the hierarchy problem (why, for example, the weak nuclear force is 1024 times stronger than gravity). Elementary particles are truly quantum in nature, and if they truly are elementary that means they should be point-like and not considered to have a radius or a length.

Atoms aren’t indivisible, but are made up of electrons and nuclei with a size of 10-10 m. Nuclei themselves can be further split into protons and neutrons, each with a size of 10-15 m. And if the particles inside protons and neutrons – the quarks and gluons – are bombarded with high-energy particles, they don’t show any internal structure, just like electrons. For each of the particles of the Standard Model, it has been determined that if they have a composite nature, or a physical “size” that differs from point-like, it must be less than 10-19 meters.

Wheeler, Geons, black holes, and quantum foam: a life in physics (1998).
>No, it isn’t turtles all the way down. To be sure, the observed mechanical properties of substances have been explained in terms of atoms, and atoms have been explained in terms of electrons and protons and neutrons, and protons and neutrons have been explained in terms of quarks. There are a few turtles standing on other turtles. But we have many reasons to believe that the layers of explanation do not continue indefinitely. For one reason, the bits of matter cease to be distinct as we go deeper.

If the standard model particle are truly indivisible, we should be able to keep on going to higher and higher energies, and should discover nothing that differs from point-like behavior all the way up to the Planck energy, or down to distance scales of the Planck length. This unit length (in the order of 10-35 m) is the smallest distance about which current, experimentally corroborated, models of physics can make meaningful statements. At such small distances, the conventional laws of macro-physics no longer apply, and even relativistic physics requires special treatment. Below that distance scale, physics doesn’t give sensible predictions, but we keep approaching it. Perhaps along the way, we’ll find that some (or all) of these particles can be further broken down, or perhaps that they are made up of strings or membranes, or, alternatively, that they are simply points all the way down. But all we know to date, as far as the actual sizes of particles, are the sizes of the non-fundamental ones. Everything else is just an upper limit, and the search to get to smaller and smaller scales continues.

[Werner Heisenberg](/references/wernerheisenberg.html)
>The theory of relativity is connected with a universal constant in nature, the velocity of light. This constant determines the relation between space and time and is therefore implicitly contained in any natural law which must fulfill the requirements of Lorentz invariance. Our natural language and the concepts of classical physics can apply only to phenomena for which the velocity of light can be considered as practically infinite. When we in our experiments approach the velocity of light we must be prepared for results which cannot be interpreted in these concepts. Quantum theory is connected with another universal constant of nature, Planck’s quantum of action. An objective description for events in space and time is possible only when we have to deal with objects or processes on a comparatively large scale, where Planck’s constant can be regarded as infinitely small. When our experiments approach the region where the quantum of action becomes essential we get into all those difficulties with the usual concepts that have been discussed in earlier chapter of this volume. There must exist a third universal constant in nature. This is obvious for purely dimensional reasons. The universal constants determine the scale of nature, the characteristic quantities that cannot be reduced to other quantities. One needs at least three fundamental units for a complete set of units. This is most easily seen from such conventions as the use of the c-g-s system (centimeter, gram, second system) by the physicists. A unit of length, one of time, and one of mass is sufficient to form a complete set; but one must have at least three units. One could also replace them by units of length, velocity and mass; or by units of length, velocity and energy, etc. But at least three fundamental units are necessary. Now, the velocity of light and Planck’s constant of action provide only two of these units. There must be a third one, and only a theory which contains this third unit can possibly determine the masses and other properties of the elementary particles. Judging from our present knowledge of these particles the most appropriate way of introducing the third universal constant would be by the assumption of a universal length the value of which should be roughly 10-13 cm, that is, somewhat smaller than the radii of the lighter atomic nuclei. When from such three units one forms an expression which in its dimension corresponds to a mass, its value has the order of magnitude of the masses of the elementary particles. If we assume that the laws of nature do contain a third universal constant of the dimension of a length and of the order of 10-13 cm, then we would again expect our usual concepts to apply only to regions in space and time that are large as compared to the universal constant. We should again be prepared for phenomena of a qualitatively new character when we in our experiments approach regions in space and time smaller than the nuclear radii.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>The idea is that there will be a stochastic sub-quantum and sub¬-relativistic level in which the current laws of physics will fail. This will probably first be encountered near the Planck length of 10-33 cm. However, over longer distances our stochastic interpretation of relativistic quantum theory will be recovered as a limiting case, but as we have suggested earlier, experiments involving shorter times could reveal significant differences from the predictions of the current relativistic quantum theory. […] The fact that the particle is moving under its own energy, but being guided by the information in the quantum field, suggest that an electron or any other elementary particle has a complex and subtle inner structure (e.g. perhaps even comparable to that of a radio). This notion goes against the whole tradition of modern physics which assumes that as we analyse matter into smaller and smaller parts its behaviour always grows more and more elementary. But our interpretation of the quantum theory indicates that nature is far more subtle and strange than previously thought. However, this sort of inner complexity is perhaps not as implausible as may appear at first sight. For example, a large crowd of people can be treated by simple statistical laws, whereas individually, their behaviour is immensely more subtle and complex. Similarly, large masses of matter reduce approximately to a simple Newtonian behaviour, whereas the molecules and atoms out of which matter is built have a more complex inner structure. To make this suggestion yet more plausible, we note that between the shortest distances now measurable in physics (of the order of 10-16 cm) and the shortest distances in which current notions of space-time probably have meaning which is the order of 10-33 cm, there is a vast range of scale in which an immense amount of yet undiscovered structure could be between our own size and that of the elementary particle. Moreover, since the vacuum is generally regarded as full with an immense energy of fluctuation, revealed for example in the Casimir effect, it may be further suggested that ultimately the energy of this particle comes from this source. (Some of it may also come from the rest energy of the particle.) It should be added here that (as happens with the radio wave) the quantum information field may also have some energy. However, as has been made clear in the many analogies given here, this must be negligible in comparison to the energy of the particle which it guides. […] It should be clear that we do not expect to come to the end of this process of discovery (for example, in a form that is currently called the “Theory of Everything”). Rather our view is that nature in its total reality is unlimited, not merely quantitatively, but also qualitatively in its depth and subtlety of laws and processes. Our knowledge at any stage is an abstraction from this total reality and therefore cannot be expected to hold indefinitely when extended into new domains. In any case it is evident that there is no way to prove that any particular aspect of our knowledge is absolutely correct. Indeed no matter how long it may have demonstrated its validity, it is always possible that later it may be found to have limits. For example, Newtonians physics, which held sway for several centuries, was found to be limited and gave way to quantum theory and relativity. Yet just before this revolution took place at the end of the nineteenth century, Lord Kelvin, one of the leading physicists at the time, advised young people not to go into physics because it was finished and only subject to refinement in the next decimal places. Nevertheless he did point to two small clouds on the horizon, the negative results of the Michelson-Morley experiment and the problems of black body radiation. It must be admitted that he chose his clouds correctly, though he totally underestimated their importance. Nowadays physicists are similarly talking about an ultimate unified theory including the four forces and perhaps strings, supersymmetry, etc. that would explain everything. However, it can hardly be said that the clouds in this picture are only two and restricted to being small. […] At present further progress only seems to be possible through the use of the imagination to suggest new concepts that might permit a more precise formulation of these ideas. In this regard the situation is not very different from what it is in string theory, which is at present guided by speculative use of mathematical concepts, that also have little or no contact with experiment.

Greene, The elegant universe (2000).
>Absolutely everything is subject to the fluctuations inherent in the principle of uncertainty. Even the gravitational field. If it is classically zero, it is on the quantum scale only on average; its value fluctuates. In addition, still under the principle of uncertainty, it is expected that the value of the field will oscillate more than it is observed on a smaller scale. Quantum mechanics has taught us the hazards of confinement: if we reduce the scale further, the ripples will be even greater. It is the curvature of space that reveals the presence of the gravitational field. Its fluctuations therefore lead to increasingly violent distortions of space. […] Let’s reduce the scale further to the last level: quantum fluctuations in the gravitational field produce terrible distortions. So much so that space is no longer anything like a softly curved object like a flat membrane. It takes on a turbulent and quirky form. An ultra-microscopic examination of space (and time) reveals a terrible facet of the Universe: the conventional notions of right and left, front and back, up and down (and even before and after) lose their meaning. John Wheeler coined the term quantum foam to describe this foamy effervescence. Here the fundamental incompatibility between quantum mechanics and general relativity is revealed: on microscopic scales, the violence of the fluctuations of the quantum world invalidates the central hypothesis of general relativity since space is no longer smooth. The keystone of quantum theory – uncertainty relationships – comes here into direct conflict with that of general relativity – geometrically smooth space(-time). […] The founding principles of general relativity and quantum mechanics allow us to assess the scale from which these pernicious phenomena would become perceptible. […] This scale, called Planck length, is the result of the union of the constant of gravitation (which measures the intensity of the gravitational force) and the constant of Planck (which measures the intensity of quantum effects). The very low value of these two constants leads, for the length of Planck, to a result which goes beyond comprehension: 10-33 centimeters! To understand what this means, imagine that the atoms are the size of the known Universe; Planck’s length would then correspond to the height of an ordinary tree! You now understand that the incompatibility between general relativity and quantum mechanics only occurs for an almost esoteric domain of space-time. […] According to string theory, the elementary ingredients of nature are not point particles. Rather, they are tiny, one-dimensional filaments, much like tiny rubber bands, extremely thin, that vibrate tirelessly. However, do not be fooled by their name: unlike an ordinary piece of rope, itself composed of molecules and atoms, the strings of string theory would be the very foundations of matter, the ultramicroscopic constituents of particles that make up the atoms themselves. The strings of string theory are so infinitesimal – on average, their length is roughly equal to Planck’s length - that they seem like a point, even when examined with our most powerful instruments. […] Planck’s length is some seventeen orders of magnitude below what we can see today. So, with current technologies, we would need an accelerator the size of the galaxy to see the strings individually. In fact, Shmuel Nussinov of Tel Aviv University has shown that this rough estimate, based on simple proportionality calculations, is probably too optimistic; according to his more precise analysis, it would require an accelerator the size of the Universe. (The energy required to probe the material at Planck’s length is roughly equal to a thousand kilowatts/hour, i.e. the energy required to operate an electric radiator for a hundred hours, which is not particularly extravagant. The seemingly insurmountable technological challenge is to concentrate all this energy on a single particle, that is to say a single string.)

String theory is an ambitious research program in the framework of quantum field theory. According to string theory, all fundamental particles can be considered to be excitations of underlying non-pointlike entities in a multi-dimensional space. These one-dimensional objects are called strings. The particles’ intrinsic charge, mass and spin may then arise as nonseparable features of the world at the deepest level. String theory and its upgrades, such as M-theory, are trying to reach the Theory of Everything; a hypothetical single, all-encompassing, coherent theoretical framework of physics that fully explains and links together all physical aspects of the universe. 

Over the past few centuries, two theoretical frameworks have been developed that, together, most closely resemble a theory of everything. These two theories upon which all modern physics rests are general relativity and quantum field theory. Physicists have experimentally confirmed virtually every prediction made by these two theories when in their appropriate domains of applicability. Nevertheless, they are mutually incompatible. To resolve the incompatibility, a theoretical framework revealing a deeper underlying reality, unifying gravity with the other three interactions (strong nuclear, weak nuclear, and electromagnetic), must be discovered. At present, most particle physicists expect that the outcome of the ongoing experiments – the search for new particles at the large particle accelerators and for dark matter – are needed in order to provide further input for a theory of everything.

Rosenfeld, explained by Bell in Speakable and unspeakable in quantum mechanics.
>Rosenfeld has gone so far as to suggest that a quantum theory of gravitation may be unnecessary. The only gravitational phenomena we actually know are of macroscopic scale and involve very many atoms. So we only need the concept of gravitation on this classical level, whose separate logical status is anyway fundamental in the traditional view.

Bell, Speakable and unspeakable in quantum mechanics (1987).
>I think that most contemporary physicists would regard any purely classical theory of gravitation as provisional, and hold that any really adequate theory must be applicable, in principle, also on the microscopic level – even if its effects there are negligibly small. Many of these same contemporary physicists are perfectly complacent about the vague division of the world into classical macroscopic and quantum microscopic inherent in contemporary (i.e., traditional) quantum theory. This mixture of concern on the one hand and complacency on the other is in my opinion less admirable than the clear headed and systematic complacency of Rosenfeld.

Everett, explained by Bell in Speakable and unspeakable in quantum mechanics.
>Everett was complacent neither about gravitation nor quantum theory. As a preliminary to a synthesis of the two he sought to interpret the notion of a wave function for the world. This world certainly contains instruments that can detect, and record macroscopically, microscopic and other phenomena.

Aharonov and Vaidman, About position measurements (1996).
>I [Vaidman] remember discussing with David [Bohm] my passion for the many-worlds interpretation (Everett 1957), and then, what I find most attractive in the Bohm theory: that it is unambiguous, deterministic and complete. I see it as the best candidate for the final theory of the world for a physicist (such as Aharonov) who does not want to accept the existence of many worlds. David said to me that what I liked in his theory did not have much value for him. He had a strong belief that a man cannot find the final theory of the world. All that we can do is to look for a better and better approximation to the correct theory which is intrinsically unattainable to us. His vision was that the causal interpretation should suggest a way for generalization to the next-level theory which also, by no means, will be the final theory of the world.

Hawking, A brief history of time (1988).
>Can there really be such a theory? Or are we just chasing mirage? There seem to be three possibilities: (1) There is indeed a completely unified theory, which we will one day discover if we show clever enough. (2) There is no ultimate theory of the universe, an infinite series of theories which describe the universe more or less precisely. (3) There is no theory of the universe; events cannot be predicted beyond a certain point and happen randomly and arbitrarily. Some will argue the third possibility based on the fact that if there were a complete set of laws, it would infringe on God’s freedom to change your mind and intervene in the world. It is a bit like the old paradox: can God create a stone so heavy that he cannot lift it? But this idea that God may want to change his mind is an example of a fallacy, noticed by Saint Augustine, of imagining God as a being existing in time: time is only a property of the universe that God created. It is likely that he knew what he wanted to do when he created it! With the advent of quantum mechanics, we are led to recognize that events cannot be predicted with complete accuracy and that there is always a degree of uncertainty. If you want, you can describe this chance of God’s intervention, but it would be a very strange kind of intervention: there is no evidence that it obeys a design. If that were really the case, it would not be by chance, by definition. In modern times, we have effectively erased the third possibility set out above by redefining the aim of science: our wish is to formulate a set of laws which are capable of predicting events only within the limits of the principle of uncertainty. The second possibility, that which evokes an infinite series of more and more refined theories, is in agreement with our experience. On several occasions, we have increased the sensitivity of our measurements or conducted a new class of observations only to discover new phenomena which were not predicted by existing theory, and to take these into account, we have had to develop a more advanced theory. It would therefore not be very surprising that the current generation of great unification theories is wrong in asserting that nothing essential can ever happen between the electroweak unification energy of around 100 GeV and the energy of great unification about one million billion GeV. We should really expect to find several new layers of more fundamental structures than quarks and electrons that we currently call “elementary” particles. However, it seems that gravity can provide a limit to this sequence of “Russian dolls”. If we had a particle with an energy greater than Planck’s energy, ten billion billion GeV (a 1 followed by 19 zeros), its mass would be so concentrated that it would subtract itself from the rest of the universe and that it would form a black hole. So it really seems that the sequence of more and more refined theories must know some limit as we increase the energies and that there should be an ultimate theory of the universe. Of course, Planck’s energy is a long way from the energies of around one hundred GeV which are the best we can produce in the laboratory today. We will not fill this gap with particle accelerators in the years to come! The very primitive stages of the universe, however, are an arena where such energies may have been deployed. I think there is a good chance that the study of the primitive universe and the demands of mathematical logic will lead us to a completely unified theory during the life of some of those around us today, provided however that we did not explode before. […] Even if we discover a completely unified theory, this would not mean that we would be able to predict events in general, for two reasons. The first is the limitation that the uncertainty principle of quantum mechanics gives to our predictive powers. In practice, however, this first limitation is less restrictive than the second. This is because we cannot solve the equations of the theory exactly, except in very simple situations. (We cannot even exactly solve the motion of three bodies in Newtonian theory of gravitation, and the difficulty increases with the number of bodies and the complexity of the theory.) We already know the laws that govern the behavior of matter in almost the most extreme conditions. In particular, we know the fundamental laws that underlie all chemistry and biology. But we have certainly not reduced these subjects to the status of resolved problems; we have, as always, little success when we predict human behavior from mathematical equations! Also, even if we find a set of fundamental laws, there will always be in the years which follow an intellectual concern to develop better methods of approximation so that we can make better predictions concerning the probable consequences of complex situations and real. A complete, logical and unified theory is only the first step: our goal is a complete understanding of the events around us and of our own existence.

Greene, The elegant universe (2000).
>There is general agreement that the discovery of the “theory of everything” would not solve (nor simplify) psychology or biology, geology or chemistry, or even physics. The universe is so fabulously diverse and complex that breaking through to the final theory, as it is described here, would in no way put an end to science. Quite the contrary. This theory, the ultimate description of the Universe in its most microscopic detail, would be independent of any deeper explanation. It would provide the strongest foundation for building our understanding of the Universe. Its discovery would mark a beginning, not an end. The ultimate theory would provide a firm and consistent foundation that would assure us that our Universe is an intelligible place. […] Shortly before his death, Richard Feynman made it clear that he could not believe that string theory was the unique remedy for the problems – in particular, the pernicious infinities – that hinder the harmonious fusion between gravitation and quantum theory. “My feeling has been – and I could be wrong – that there’s more than one way to skin a cat. I don’t think that there’s only one way to get rid of the infinities. The fact that a theory gets rid of infinities is to me not a sufficient reason to believe its uniqueness”. […] As is often the case with debates of great importance, each of these detractors opposes a fervent defender. Witten says that when he learned how string theory included gravity and quantum theory, it was “the greatest intellectual thrill” of his life. Cumrun Vafa, of Harvard, one of the leaders in string theory, says that “string theory really lets you see the deepest understanding of the Universe you’ve ever had”. For Nobel Prize winner Murray Gell-Mann, string theory is “a fabulous thing” and he expects that one version of string theory will one day be the theory of the whole world. […] In his long quest for a unified theory, Einstein wondered if “God could have made the Universe in a different way; that is, if the constraint of logical simplicity leaves some freedom or not at all”. With this remark, Einstein formulated for the first time a point of view shared today by many physicists: if there exists an ultimate theory of nature, then, one of the most convincing arguments in favor of its precise form would be that the theory cannot have another form. The ultimate theory would necessarily have the form it has because it would represent the only explanatory framework capable of describing the Universe without internal inconsistencies or logical aberrations. Such a theory would proclaim that things are what they are because they have to be like that. […] We see that the five string theories were considered to be completely separate. Like the five arms of the starfish, thanks to recent discoveries, all string theories are now understood to be part of a single framework. (In fact, we will see that a sixth theory – a sixth arm – will even be added to this union.) This unifying framework has been provisionally called M-theory. […] Although much remains to be done, physicists have already revealed two fundamental aspects of the M-theory. First, it lives in eleven dimensions (ten spatial and one temporal). […] The second property of the M-theory that we discovered is that in addition to the strings it contains other objects: two-dimensional membranes, three-dimensional bubbles (called three-branes) and a whole bunch of other ingredients. […] Even if we leave aside the problem of initial conditions and their impact on cosmic evolution, recent, highly speculative hypotheses suggest other potential limitations to the explanatory power of any ultimate theory. No one knows if these ideas are viable and, for the moment, they are developing alongside the main scientific trends. However, they throw an interesting and unexpected light on an obstacle which any supposedly final theory could come up against. The basic idea is as follows. Imagine that what is called the Universe is in fact only a tiny part of an immensely larger cosmological expanse. Imagine it to be just one of the many universe-islets scattered across a gigantic cosmic archipelago. It might sound a bit far-fetched to you – and it may be – but Andreï Linde has come up with a mechanism that could produce such a gigantic universe. Linde discovered that the surge in inflationary expansion may not have been a single, isolated event. On the contrary, he argues, the conditions favorable to inflation could have been repeated in various regions scattered throughout the cosmos, which would then have undergone their own expansion, developing into new, separate universes. And, in each of these universes, the process continues, with the eruption of new universes in the most remote regions of the old ones, thus generating an interminable network of cosmic expansion. The terms are lacking, but let us go with the flow and baptize with the name of multiverse this new concept of universe, very generalized, of which we would call universe each of its constituent parts. All that is known indicates that physics remains coherent and uniform throughout the Universe. But this may not be true of the physical attributes of these other universes as long as we are separate from them, or at least far enough apart, that their light has not had time to reach us. We can then imagine that physics varies from one universe to another. […] Lee Smolin of Pennsylvania State University came up with even more radical ideas. Drawing on the similarity between the conditions of the big bang and the center of the black holes, characterized in both cases by a colossal density of matter, he suggested that each black hole constitutes the seed of a new universe, which would erupt by an explosion similar to that of the big bang, but which would remain forever hidden from our eyes by the horizon of the black hole. Not content with proposing only another mechanism for the creation of multiuniverses, Smolin introduced a new element, a cosmic version of the genetic mutation, which makes it possible to dodge the scientific limitations associated with the anthropic principle. Imagine, he argues, that when a universe emerges from the core of a black hole, its physical attributes, such as the mass of particles or forces, are close to those of the parent universe, but not identical. Since black holes result from the extinction of stars, and since the formation of these same stars depends on the precise values of the masses of elementary components and the intensities of their interactions, the fertility of a given universe – the offspring of black holes that it can generate – it also depends significantly on these parameters. Small variations in parameters. Descending universes will therefore lead to the generation of more optimal ones, for the production of black holes, than their parent universes, whose own descendants in the universe will be much more abundant. After many “generations”, the descendants of the universes optimized to produce black holes will be so numerous that they will represent the bulk of the population of these multiuniverses. Thus, rather than invoking the anthropic principle, Smolin’s idea provides a dynamic mechanism which, on average, channels the parameters of each successive generation of universes towards particular values, those which are optimal for the creation of black holes.

The Universe is all of space and time and their contents, including planets, stars, galaxies, and all other forms of matter and energy. While the spatial size of the entire Universe is unknown, it is possible to measure the size of the observable universe, which is currently estimated to be 1026 m in diameter. In various multiverse hypotheses, a universe is one of many causally disconnected constituent parts of a larger multiverse, which itself comprises all of space and time and its contents; as a consequence, “the Universe” and “the multiverse” are synonymous in such theories. According to the general theory of relativity, far regions of space may never interact with ours even in the lifetime of the Universe due to the finite speed of light and the ongoing expansion of space.

[Werner Heisenberg](/references/wernerheisenberg.html)
>From the present astronomical knowledge one cannot definitely distinguish between several possible models. It may be that the space filled by the universe is finite. This would not mean that there is an end of the universe at some place. It would only mean that by proceeding farther and farther in one direction in the universe one would finally come back to the point from which one had started. The situation would be similar as in the two-dimensional geometry on the surface of the earth where we, when starting from a point in an answered direction, finally come back to this point from the west.

The Universe could have more dimensions than thought, as pointed out by Greene with string theory, but certainly not less, as explained by Hawking with the anthropic principle.

Greene, The elegant universe (2000).
>In an article he wrote to Einstein in 1919, Kaluza made a surprising suggestion. He proposed that the spatial structure of the Universe could have more dimensions than the usual three. The reason he had come to formulate such a radical thesis was that he had discovered that it provided an elegant and attractive framework to unify Einstein’s theory of general relativity and Maxwell’s theory of electromagnetism in a unique conceptual model. But, first of all, how can this proposition be compatible with the blatant fact that we see three dimensions? The answer to this question, implicit in Kaluza’s work, then clarified and refined by the Swedish mathematician Oskar Klein in 1926, is that the spatial structure of the Universe can have both extended and circular compact dimensions. […] Thus, we discover with surprise that, even if we see only three spatial dimensions, the reasoning of Kaluza and Klein shows that this does not exclude the existence of additional dimensions, insofar as these are ultrasmall. The Universe could very well have hidden dimensions. […] In 1926, Klein combined Kaluza’s initial suggestion with ideas from the emerging field of quantum theory. His calculations indicated that an additional circular dimension could be as small as Planck’s length, which greatly exceeds our detection capabilities. […] It turns out that string theory requires that our Universe have additional dimensions. […] Since the strings are so small, not only can they vibrate in large and extended dimensions, but they can also oscillate in other dimensions, compact and circular. And we can thus satisfy, in our Universe, the requirement of nine spatial dimensions of string theory, assuming – Kaluza-Klein-style – that in addition to our three spatial dimensions there are six others, wrapped around themselves. […] This raises a number of questions. First, why does string theory require this precise number of nine spatial dimensions to get rid of absurd probabilities? This is probably the question whose answer is the most difficult to formulate without recourse to mathematical formalism. A direct calculation answers it, but nobody has an intuitive interpretation, nor an explanation without technical details, of the number which results from it. […] Second, if the equations of string theory (or, more precisely, the approximate equations which underlie our preliminary explanations) show that the Universe has nine spatial dimensions and one temporal dimension, how is it that three spatial dimensions (and one temporal) are large and extended, while all the others are tiny and twisted? Why are they not all stretched out, or all twisted, or whatever? At the moment, no one knows the answer to this question. […] In their founding article, Candelas, Horowitz, Strominger and Witten had taken a first step in this direction. They discovered not only that the additional dimensions of string theory must be rolled up into a Calabi-Yau form, but they also deduced certain consequences for the possible vibrational configurations of the strings.

Hawking, Brief answers to the big questions (2018).
>Given the nature of the observations we can make now, all we can do is assign a probability to a particular history of the universe. Thus the universe must have many possible histories, each with its own probability. […] This idea that the universe has multiple histories may sound like science fiction, but it is now accepted as science fact. It is due to Richard Feynman, who worked at the eminently respectable California Institute of Technology. […] Feynman’s approach to understanding how things works is to assign to each possible history a particular probability, and then use this idea to make predictions. It works spectacularly well to predict the future. So we presume it works to retrodict the past too. Scientists are now working to combine Einstein’s general theory of relativity and Feynman’s idea of multiple histories into a complete unified theory that will describe everything that happens in the universe. This unified theory will enable us to calculate how the universe will evolve, if we know its state at one time. But the unified theory will not in itself tell us what happens at the frontiers of the universe, the edges of space and time. But if the frontier of the universe was just at a normal point of space and time we could go past it and claim the territory beyond as part of the universe. On the other hand, if the boundary of the universe was at a jagged edge where space and time were scrunched up, and the density was infinite, it would be very difficult to define meaningful boundary conditions. So it is not clear what boundary conditions are needed. It seems there is no logical basis for picking one set of boundary conditions over another. However, Jim Hartle of the University of California, Santa Barbara, and I realised there was a third possibility. Maybe the universe has no boundary in space and time. At first sight, this seems to be in direct contradiction to the geometrical theorems that I mentioned earlier. These showed that the universe must have had a beginning, a boundary in time. However, in order to make Feynman’s techniques mathematically well defined, the mathematicians developed a concept called imaginary time. It isn’t anything to do with the real time that we experience. It is a mathematical trick to make the calculations work and it replaces the real time we experience. Our idea was to say that there was no boundary in imaginary time. That did away with trying to invent boundary conditions. We called this the no-boundary proposal. If the boundary condition of the universe is that it has no boundary in imaginary time, it won’t have just a single history. There are many histories in imaginary time and each of them will determine a history in real time. Thus we have a superabundance of histories for the universe. What picks out the particular history, or set of histories that we live in, from the set of all possible histories of the universe? One point that we can quickly notice is that many of these possible histories of the universe won’t go through the sequence of forming galaxies and stars, something that was essential to our development. It may be that intelligent beings can evolve without galaxies and stars, but it seems unlikely. Thus the very fact that we exist as beings that can ask the question ‘Why is the universe the way it is?’ is a restriction on the history we live in. It implies it is one of the minority of histories that have galaxies and stars. This is an example of what is called the Anthropic Principle. The Anthropic Principle says that the universe has to be more or less as we see it, because if it were different there wouldn’t be anyone here to observe it. Many scientists dislike the Anthropic Principle, because it seems little more than hand waving, and not to have much predictive power. But the Anthropic Principle can be given a precise formulation, and it seems to be essential when dealing with the origin of the universe. M-theory, which is our best candidate for a complete unified theory, allows a very large number of possible histories for the universe. Most of these histories are quite unsuitable for the development of intelligent life. Either they are empty, or too short lasting, or too highly curved, or wrong in some other way. Yet, according to Richard Feynman’s multiple-histories idea, these uninhabited histories might have quite a high probability. We really don’t care how many histories there may be that don’t contain intelligent beings. We are interested only in the subset of histories in which intelligent life develops. This intelligent life need not be anything like humans. […] As an example of the power of the Anthropic Principle, consider the number of directions in space. It is a matter of common experience that we live in three-dimensional space. […] Two flat directions are not enough for anything as complicated as intelligent life. There is something special about three space dimensions. In three dimensions, planets can have stable orbits around stars. This is a consequence of gravitation obeying the inverse square law, as discovered by Robert Hooke in 1665 and elaborated on by Isaac Newton.


### Beyond: universal field and order <a name="p415"></a>

Quantum field theory (QFT) has already been mentioned in previous sections and it now needs some clarification. It is a theoretical framework that combines classical field theory, special relativity, and quantum mechanics (but notably not general relativity’s description of gravity) and is used to construct physical models of subatomic particles. QFT treats particles as excited states (also called quanta) of their underlying fields, which are more fundamental than the particles. As a successful theoretical framework today, quantum field theory emerged from the work of generations of theoretical physicists spanning much of the 20th century. Its development began in the 1920s with the description of interactions between light and electrons, culminating in the first quantum field theory – quantum electrodynamics. A major theoretical obstacle soon followed with the appearance and persistence of various infinities in perturbative calculations, a problem only resolved in the 1950s. A second major barrier came with QFT’s apparent inability to describe the weak and strong interactions, to the point where some theorists called for the abandonment of the field theoretic approach. The development of gauge theory and the completion of the Standard Model in the 1970s led to a renaissance of quantum field theory. Even without referring to a test particle, a field occupies space, contains energy, and its presence precludes a classical “true vacuum”. This has led physicists to consider electromagnetic fields to be a physical entity, making the field concept a supporting paradigm of the edifice of modern physics. 

Feynman, The Feynman lectures on physics, vol. I (1970).
>The fact that the electromagnetic field can possess momentum and energy makes that field very real, and so, for better understanding, the original idea that there are just the forces between particles has to be modified to the idea that a particle makes a field, and a field acts on another particle, and the field itself has such familiar properties as energy content and momentum, just as particles can have.

An example of such quantum field is the Higgs field. Without the Higgs mechanism, all bosons (one of the two classes of particles, the other being fermions) would be considered massless. The Higgs field resolves this conundrum. The simplest description of the mechanism adds a quantum field (the Higgs field) that permeates all space to the Standard Model. Below some extremely high temperature, the field causes spontaneous symmetry breaking during interactions. The breaking of symmetry triggers the Higgs mechanism, causing the bosons it interacts with to have mass. The Large Hadron Collider at CERN announced results consistent with the Higgs particle in 2013, making it extremely likely that the field, or one like it, exists, and explaining how the Higgs mechanism takes place in nature.

Miller, A quasi-political Explanation of the Higgs Boson (1993).
>Imagine a cocktail party of political party workers who are uniformly distributed across the floor, all talking to their nearest neighbors. The ex-Prime Minister enters and crosses the room. All of the workers in her neighbourhood are strongly attracted to her and cluster round her. As she moves she attracts the people she comes close to, while the ones she has left return to their even spacing. Because of the knot of people always clustered around her she acquires a greater mass than normal, that is, she has more momentum for the same speed of movement across the room. Once moving she is harder to stop, and once stopped she is harder to get moving again because the clustering process has to be restarted. In three dimensions, and with the complications of relativity, this is the Higgs mechanism. In order to give particles mass, a background field is invented which becomes locally distorted whenever a particle moves through it. The distortion – the clustering of the field around the particle – generates the particle’s mass. The idea comes directly from the Physics of Solids. Instead of a field spread throughout all space a solid contains a lattice of positively charged crystal atoms. When an electron moves through the lattice the atoms are attracted to it, causing the electron’s effective mass to be as much as 40 times bigger than the mass of a free electron. The postulated Higgs field in the vacuum is a sort of hypothetical lattice which fills our Universe. We need it because otherwise we cannot explain why the Z and W particles which carry the Weak Interactions are so heavy while the photon which carries Electromagnetic forces is massless. […] We will find it much easier to believe that the field exists, and that the mechanism for giving other particles mass is true, if we actually see the Higgs particle itself. Again, there are analogies in the Physics of Solids. A crystal lattice can carry waves of clustering without needing an electron to move and attract the atoms. These waves can behave as if they are particles. They are called phonons, and they too are bosons. There could be a Higgs mechanism, and a Higgs field throughout our Universe, without there being a Higgs boson. The next generation of colliders will sort this out.

Thus, fields such as the Higgs field seem to pervade the entire universe. Particles, to be understood as fields as well, can interact with it to gain some properties, such as mass in this example. Greene explains the idea of a unique field that splits under some conditions to create the fields we experience daily.

Greene, The elegant universe (2000).
>Sheldon Glashow, Abdus Salam and Steven Weinberg received the Nobel Prize for unifying weak and electromagnetic interactions. Their work demonstrates that these quantum field theories unite naturally, although they seem very different. The fields of the weak nuclear force diminish very quickly to cancel themselves beyond the subatomic scales, while the electromagnetic fields – visible light, X-rays, radio or television signals – manifest themselves on a macroscopic scale. Glashow, Salam and Weinberg have, however, established that at sufficiently high temperatures and energies – for example within a few seconds of the big bang – the fields “merge”. Nothing distinguishes them anymore and they are called, more aptly, electroweak fields. As soon as the temperature drops, which it does from the big bang, our two theories “crystallize” by differentiating. […] The three non-gravitational forces unite in the extremely hot environment of the Primordial Universe. Calculations by physicists giving the variation of the intensity of these forces with energy and temperature show that before about 10-35 seconds after the explosion the strong, weak and electromagnetic forces were only a “super”-force, of “great unification”. At that time, the Universe was much more symmetrical than today. Like the homogeneity achieved by founding a disparate collection of different metals, the extreme energy and temperature of the Original Universe erased the fundamental differences between forces as we see them today.

The Aharonov-Bohm effect is a quantum phenomenon in which a particle is affected by electromagnetic fields even when traveling through a region of space in which both electric and magnetic field are zero. The effect was analyzed in 1959 and then regularly confirmed experimentally with better precision. The Aharonov–Bohm effect shows that the local electric and magnetic fields do not contain full information about the electromagnetic field. Bohm and Hiley describe a field linked to instantaneous nonlocal effects.

Aharonov, Cohen and Rohrlich, Nonlocality of the Aharonov-Bohm effect (2016).
>Although the Aharonov-Bohm and related effects are familiar in solid state and high energy physics, the nonlocality of these effects has been questioned. Here we show, for the first time, that the Aharonov-Bohm effect has two very different aspects. One aspect is instantaneous and nonlocal; the other aspect, which depends on entanglement, unfolds continuously over time. While local, gauge-invariant variables may occasionally suffice for explaining the continuous aspect, we argue that they cannot explain the instantaneous aspect. Thus the Aharonov-Bohm effect is, in general, nonlocal.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>One could suppose that in addition to the known types of field there was a new kind of field which would determine a space-like surface along which nonlocal effects would be propagated instantaneously. At present we can say very little about this field, but one could surmise that this space-like surface would be close to a hyperplane of constant time as determined in a certain Lorentz frame. A good candidate for such a frame could be obtained by considering at each point in space-time, the line connecting it to the presumed origin of the universe. This would determine a unique time order for the neighbourhood of that point around which one would expect isotropic properties in space. We may plausibly conjecture that this frame would be the one in which the 3K background radiation in space has an isotropic distribution. This unique frame would not only make possible a coherent account of nonlocal connections, but could also be significant in other ways. For example in this frame there will evidently be no limit to the possible speed of the particles. Thus, in the stochastic form of the ontological interpretation of the Dirac equation it is possible … to have random movements that are faster than light. In the unique frame that we have introduced, there evidently is no reason why invariant process of this kind can give a Lorentz invariant equilibrium distribution. This brings us to the second objection, i.e. that the absolute frame is unobservable. Certainly if we restrict ourselves to the assumptions that have been made thus far, this would be a valid criticism. However, in terms of our ontological approach it is possible to alter these assumptions in such a way that the absolute frame would be observable, while negligible changes would be produced in the domain of experiments that have been possible thus far. […] With regard to the special frame implied by ‘nonlocal’ quantum connections and the deeper fields that would propagate these connections locally, we do not propose that these are intrinsically unobservable. We merely say that in the statistical and manifest domain in which the current quantum theory and relativity are valid, these new properties cannot be observed. Just as the observation of atoms became possible where continuity broke down, so the observation of the new properties would become possible where quantum theory and relativity break down.

It is possible to conceive mind-matter relations indirectly, via a third category. This third category is often regarded as being neutral with respect to the distinction between material and mental, i.e., psychophysically neutral. In this case, issues of reduction and emergence concern the relation between the unseparated “background reality” and the distinguished aspects material and mental. Such “dual aspect” frameworks of thinking have received increasing attention in contemporary discussion, and they have a long tradition reaching back as far as to Spinoza. In the early days of psychophysics, Whitehead, the modern pioneer of process philosophy, referred to mental and physical poles of “actual occasions”, which themselves transcend their bipolar. Other variants of this idea have been suggested in 1955 by Jung and Pauli, involving Jung’s conception of a psychophysically neutral, archetypal order, or by Bohm and Hiley referring to an implicate order which unfolds into the different explicate domains of the mental and the material. For Russell and the neo-Russellians the compositional arrangements of psychophysically neutral elements decide how they differ with respect to mental or physical properties. As a consequence, the mental and the physical are reducible to the neutral domain. Chalmers’ 1996 ideas on “consciousness and information” fall into this class. No quantum structures are involved in this work. The other class of dual-aspect thinking is decompositional rather than compositional. Here the basic metaphysics of the psychophysically neutral domain is holistic, and the mental and the physical (neither reducible to one another nor to the neutral) emerge by breaking the holistic symmetry or, in other words, by making distinctions. This framework is guided by the analogy to quantum holism, and the predominant versions of this picture are quantum theoretically inspired as, for instance, proposed by Pauli and Jung and by Bohm and Hiley. They are based on speculations that clearly exceed the scope of contemporary quantum theory. In Bohm’s and Hiley’s approach, the notions of implicate and explicate order mirror the distinction between ontic and epistemic domains. Mental and physical states emerge by explication, or unfoldment, from an ultimately undivided and psychophysically neutral implicate, enfolded order. This order is called holomovement because it is not static but rather dynamic, as in Whitehead’s process philosophy. At the level of the implicate order, the term active information expresses that this level is capable of “informing” the epistemically distinguished, explicate domains of mind and matter.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>The ordinary Cartesian order applied to separate points, finds one of its strongest supports in the function of a lens. What a lens does is to produce an approximate correspondence of points on an object to points on its image. The perception of this correspondence strongly brings out attention to the separate points. But as is well known, there is a new instrument used to making images called the hologram which does not do this. Rather each region of the hologram makes possible an image of the whole object. When we put all these regions together, we still obtain an image of the whole object, but one that is more sharply defined as well as containing more points of view. The hologram does not look like the object at all, but gives rise to an image only when it is suitably illuminated. The hologram seems, on cursory inspection, to have no significant order in it, and yet there must somehow be in it an order that determines the order of points that will appear in the image when it is illuminated. We may call this order implicit, but the basic root of the word implicit means ‘enfolded’. So in some sense, the whole object is enfolded in each part of the hologram rather than being in point-to-point correspondence. We may therefore say that each part of the hologram contains an enfolded order essentially similar to that of the object and yet obviously different in form. As we develop this idea, we shall see that this notion of enfoldment is not merely a metaphor, but that is has to be taken fairly literally. To emphasis this point, we shall therefore say that the order in the hologram is implicate. The order in the object, as well as in the image, will then be unfolded, and we shall call it explicate. The process, in this case wave movement, in which this order is conveyed from the object to the hologram will be called enfoldment or implication. The process in which the order in the hologram becomes manifest to the viewer in an image will be called unfoldment or explication. […] First of all we note that basically all the laws of movement in quantum mechanics do correspond to enfoldment and unfoldment. […] We may suppose that the universe, which includes the whole of existence, contains not only all the fields that are now known, but also an indefinitely large set of further fields that are unknown and indeed may never be known in their totality. Recalling that the essential qualities of fields exist only in their movement we propose to call this ground the holomovement. It follows that ultimately everything in the explicate order of a common experience arises from the holomovement. Whatever persists with a constant form is sustained as the unfoldment of a recurrent and stable pattern which is constantly being renewed by enfoldment and dissolved by unfoldment. When the renewable ceases the form vanishes. The notion of a permanently extant entity with a given identity, whether this be a particle or anything else, is therefore at best an approximation holding only in suitable limiting cases. […] The classical field, which generally obeys a linear equation, has already been understood in terms of the implicate order, e.g. as in the example of the hologram. But when this field is quantized, a further kind of implicate order is introduced. We shall call this the super implicate order. The super implicate order is related to the implicate order as the implicate order, in particle theories, is related to the particles. […] The whole idea of implicate order could be extended in a natural way. For if there are two levels of implicate order, why should there not be more? Thus if we regard the super implicate order as the second level, then we might consider a third level which was related to the second as the second is to the first. […] Evidently we could go on indefinitely to high levels of implicate order. […] In other words we have an order of implicate orders. […] Thus far we have considered only a one-way connection of implicate orders in which the higher affects the lower but not the other way round. However, we could quite readily extend the idea to allow lower orders to affecter the higher ones.

The holomovement brings together the holistic principle of “undivided wholeness” with the idea that everything is in a state of process or becoming (Bohm calls it the “universal flux”). Bohm went further and said that “the implicate order has its ground in the holomovement which is, as we have seen, vast, rich, and in a state of unending flux of enfoldment and unfoldment, with laws most of which are only vaguely known. As such, the holomovement includes not just physical reality, but life, consciousness and cosmology”.

Hiley, Process and the implicate order (2005).
>It is important to realize that when we use the term ‘holomovement’ we are not just referring to objects moving through space. We are referring to much more subtle orders of change, development and evolution of every kind. Bohm (1976) illustrates this generality by referring to the movement of a symphony. Here there are the individual notes which are carried by oscillating air molecules but the movement of the symphony cannot be understood by the motion of the air molecules. Here the term movement is referring to something much more abstract, but nevertheless it is real, carrying meaning for our emotions. It is clear that the movement of a symphony involves a total ordering, where the past and future actively intermingle, creating an order which transcends the temporal order of the notes. Thus one can apprehend the whole symphony at any moment. […] To help understand this coordinated movement we have likened the group behaviour to ballet dancers whose movements are coordinated, not by direct mechanical forces, but rather by each individual responding to a common theme. In the case of the ballet each dancer responds to the meaning provided by the musical score as it develops in time. Thus in the analogy the wave function provides the “score” to which the particles respond. The two independent wave functions correspond in the analogy to two sets of dancers following their own theme. Here the form of the movement in each group can be regarded as unfolding from within and the energy that is needed to bring about these changes is provided by the individuals themselves. Although the analogy has obvious weaknesses, it nevertheless highlights the radical difference between classical forces and the type of effect generated by the quantum potential. […] Since the group behaviour is something that is intrinsic to that particular group of particles and to no others, it seems, once again, as if there were some kind of self-organization is involved, but a self-organization that is shaped by the environment and mediated by the quantum potential. Thus the system behaves as a whole or a totality in such a way that the particles appear to be nonlocally linked.

Einstein’s most famous equation says that energy and matter are two sides of the same coin. As he showed, light and matter and just aspects of the same thing. Matter is just frozen light. And light is matter on the move. How does one become the other? We don’t really know. We only know that it does. The same effect occurs with quantum particles, and not just with light. A neutron, for example, can decay into a proton, electron and anti-neutrino. The mass of these three particles is less than the mass of a neutron, so they each get some energy as well. So energy and matter are really the same thing. Completely interchangeable. And finally, although energy and mass are related through special relativity, mass and space are related through general relativity. One can define any mass by a distance known as a radius given by Karl Schwarzschild (1873-1916, German), which is the radius of a black hole of that mass. So in a way, energy, matter, space and time are all aspects of the same thing.

[Werner Heisenberg](/references/wernerheisenberg.html)
>When one compares this order with older classifications that belong to earlier stages of natural science one sees that one has now divided the world not into different groups of objects but into different groups of connections. In an earlier period of science one distinguished, for instance, as different groups minerals, plants, animals, men. These objects were taken according to their group as of different natures, made of different materials, and determined in their behavior by different forces. Now we know that it is always the same matter, the same various chemical compounds that may belong to any object, to minerals as well as animals or plants; also the forces that act between the different parts of matter are ultimately the same in every kind of object. What can be distinguished is the kind of connection which is primarily important in a certain phenomenon. For instance, when we speak about the action of chemical forces we mean a kind of connection which is more complicated or in any case different from that expressed in Newtonian mechanics. The world thus appears as a complicated tissue of events, in which connections of different kinds alternate or overlap or combine and thereby determine the texture of the whole. […] The experiments carried out by means of cosmic radiation or of the big accelerators have revealed new interesting features of matter. Besides the three fundamental building stones of matter – electron, proton and neutron – new elementary particles have been found which can be created in these processes of highest energies and disappear again after a short time. The new particles have similar properties as the old ones except for their instability. Even the most stable ones have lifetimes of roughly only a millionth part of a second, and the lifetimes of others are even a thousand times smaller. At the present time about twenty-five different new elementary particles are known; the most recent one is the negative proton. These results seem at first sight to lead away from the idea of the unity of matter, since the number of fundamental units of matter seems to have again increased to values comparable to the number of different chemical elements. But this would not be a proper interpretation. The experiments have at the same time shown that the particles can be created from other particles or simply from the kinetic energy of such particles, and they can again disintegrate into other particles. Actually the experiments have shown the complete mutability of matter. All the elementary particles can, at sufficiently high energies, be transmuted into other particles, or they can simply be created from kinetic energy and can be annihilated into energy, for instance into radiation. Therefore, we have here actually the final proof for the unity of matter. All the elementary particles are made of the same substance, which we may call energy or universal matter; they are just different forms in which matter can appear.


## Life and consciousness <a name="p5"></a>

### Introduction <a name="p51"></a>

It is (for some purposes) interesting to know that particular brain areas are activated during particular mental activities; but this does, of course, not explain why they are. Thus, it would be premature to talk about mind-matter interactions in the sense of causal relations. For instance, there is the influential stance of strong reduction, stating that all mental states and properties can be reduced to the material domain or even to physics (physicalism). This point of view claims that it is both necessary and sufficient to explore and understand the material domain, e.g., the brain, in order to understand the mental domain, e.g., consciousness. It leads to a monistic picture, in which any need to discuss mental states is eliminated right away or at least considered as epiphenomenal.

Much discussed counterarguments against the validity of such strong reductionist approaches are qualia (defined as individual instances of subjective, conscious experience) arguments, which emphasize the impossibility for physicalist accounts to properly incorporate the quality of the subjective experience of a mental state, the “what it is like to be” (Nagel 1974) in that state. This leads to an explanatory gap between third-person and first-person accounts for which Chalmers (1996) has coined the notion of the “hard problem of consciousness”. Dennett suggested that qualia was “an unfamiliar term for something that could not be more familiar to each of us: the ways things seem to us”. Much of the debate over their importance hinges on the definition of the term, and various philosophers emphasize or deny the existence of certain features of qualia. Consequently, the nature and existence of various definitions of qualia remain controversial because they are not verifiable. An argument holds that it is conceivable (or not inconceivable) that there could be physical duplicates of people, called “philosophical zombies”, without any qualia at all. These “zombies” would demonstrate outward behavior precisely similar to that of a normal human, but would not have a subjective phenomenology. It is worth noting that a necessary condition for the possibility of philosophical zombies is that there be no specific part(s) of the brain that directly give rise to qualia – the zombie can only exist if subjective consciousness is causally separate from the physical brain. For Dennett, zombies are “not just possible, they’re actual. We’re all zombies. Nobody is conscious”.

Another discussed counterargument is that the physical domain itself is not causally closed. Any solution of equations of motion (be it experimentally, numerically, or analytically) requires to fix boundary conditions and initial conditions which are not given by the fundamental laws of nature (Primas 2002). This causal gap applies to classical physics as well as quantum physics, where a basic indeterminacy due to collapse makes it even more challenging.

### Metaphysical ideas <a name="p52"></a>

Perhaps no aspect of mind is more familiar or more puzzling than consciousness and our conscious experience of self and world. The problem of consciousness is arguably the central issue in current theorizing about the mind. Despite the lack of any agreed upon theory of consciousness, there is a widespread, if less than universal, consensus that an adequate account of mind requires a clear understanding of it and its place in nature. We need to understand both what consciousness is and how it relates to other, nonconscious, aspects of reality.

It may be that no other classical philosophical tradition, East or West, offers a more complex and counter-intuitive account of mind and mental phenomena than Buddhism. While Buddhists share with other Indian philosophers the view that the domain of the mental encompasses a set of interrelated faculties and processes, they do not associate mental phenomena with the activity of a substantial, independent, and enduring self or agent. Rather, Buddhist theories of mind center on the doctrine of not-self, which postulates that human beings are reducible to the physical and psychological constituents and processes which comprise them.

Philosophers of the Abhidharma traditions had argued that our cognitive propensities are beginningless, each thought being merely the continuation of an endless series of previous thoughts, which constantly inform, influence, and direct the cognitive process. They maintained that these cognitive propensities are most vividly manifest in traces of memory and in the activity of conceptual thought. The Buddhist epistemologists, however, came to reject both memory and conceptual elaboration as reliable sources of knowledge. These cognitive modalities were completely dissociated from direct perception, the only type of cognitive awareness that some regarded as warranted.
	
Kamalasila, Stages of meditation, explained by the current Dalai Lama (2001).
>The cycle of existence … is born by participating in the essential nature of its causes. The two root causes of birth in the cycle of existence are karma and disturbing emotions, the latter being predominant. Ignorance, which is the misconception of real existence, is the most serious of the three main disruptive emotions. And this ignorance which is mistaken about existence is not something which would have fallen from the sky, but a creation of consciousness. […] Consciousness … is in the nature of things. However, ignorance, which is both the source of all other disturbing emotions and a cause of birth in the cycle of existence, arises at the same time as consciousness. And consciousness has no beginning.

One of the pressing questions in seventeenth century philosophy, and perhaps the most celebrated legacy of Descartes’s dualism, is the problem of how two radically different substances such as mind and body enter into a union in a human being and cause effects in each other. How can the extended body causally engage the unextended mind, which is incapable of contact or motion, and “move” it, that is, cause mental effects such as pains, sensations and perceptions. Spinoza denies that the human being is a union of two substances. The human mind and the human body are two different expressions of one and the same thing. Because there is no causal interaction between the mind and the body, the so-called mind-body problem does not, technically speaking, arise. For Spinoza, the mind, like any other idea, is simply one particular mode of God’s attribute, Thought. Whatever happens in the body is reflected or expressed in the mind. In this way, the mind perceives, more or less obscurely, what is taking place in its body. And through its body’s interactions with other bodies, the mind is aware of what is happening in the physical world around it. 

Spinoza, explained by Misrahi in commentaries to Spinoza’s Ethics (2005).
>Spinozism is the outrageous and simple claim that man has no soul and that he is not immortal. Man is first of all a being of nature, originally constituted as a conscious body and inseparably unified as such. […] Desire is the whole of human consciousness, as the idea of the body was the whole of man. Essential founder of the individual being in the deployment of its activity and its passivity, the Desire is not one of the psychic elements which would constitute among others the individual personality. […] Man is Desire from side to side, and not a set of faculties: to know, to want and to desire. By defining man as Spirit and not as Soul, Spinoza made possible a radically unitary description of human nature: all the rich aspects of his activity are the expression of his unified reality. Self and body awareness, desire to be and to act, are one and the same reality in which man is fully committed.

At the outset of modern scientific psychology in the mid-nineteenth century, the mind was still largely equated with consciousness, and introspective methods dominated the field as in the work of William James. However, the relation of consciousness to brain remained very much a mystery. The dynamics of consciousness are evident in the coherent order of its ever changing process of flow and self-transformation, what James called the “stream of consciousness”, as opposed to a succession of “ideas”. Consciousness is a dynamic process. James praises Gustav Fechner (1801-1887, German) for holding that “the whole universe in its different spans and wave-lengths, exclusions and developments, is everywhere alive and conscious”.

James, Does ‘consciousness’ exist? (1904).
>I believe that ‘consciousness’, when once it has evaporated to this state of pure diaphaneity, is on the point of disappearing altogether. It is the name of a nonentity, and has no right to a place among first principles. Those who still cling to it are clinging to a mere echo, the faint rumor left behind by the disappearing ‘soul’ upon the air of philosophy. […] To deny plumply that ‘consciousness’ exists seems so absurd on the face of it – for undeniably ‘thoughts’ do exist – that I fear some readers will follow me no farther. Let me then immediately explain that I mean only to deny that the word stands for an entity, but to insist most emphatically that it does stand for a function. […] ‘Consciousness’ is supposed necessary to explain the fact that things not only are, but get reported, are known. Whoever blots out the notion of consciousness from his list of first principles must still provide in some way for that function’s being carried on. […] Consciousness connotes a kind of external relation, and does not denote a special stuff or way of being. The peculiarity of our experiences, that they not only are, but are known, which their ‘conscious’ quality is invoked to explain, is better explained by their relations – these relations themselves being experiences – to oneanother.

Jung viewed the ego as the center of the field of consciousness, the part of the psyche where our conscious sense of identity and existence resides. In his theory the persona appears as a consciously created personality or identity, made of part of the collective psyche through experience. Both self and others believe in that identity, even if it is really no more than a “well-played role”.

Jung, Synchronicity – An acausal connecting principle (1952).
>I am only too conscious that synchronicity is a highly abstract and “irrepresentable” quantity. It ascribes to the moving body a certain psychoid property which, like space, time, and causality, forms a criterion of its behaviour. We must completely give up the idea of the psyche’s being somehow connected with the brain, and remember instead the “meaningful” or “intelligent” behaviour of the lower organisms, which are without a brain. Here we find ourselves much closer to the formal factor which, as I have said, has nothing to do with brain activity. […] The “absolute knowledge” which is characteristic of synchronistic phenomena, a knowledge not mediated by the sense organs, supports the hypothesis of a self-subsistent meaning, or even expresses its existence. Such a form of existence can only be transcendental, since, as the knowledge of future or spatially distant events shows, it is contained in a psychically relative space and time, that is to say in an irrepresentable space-time continuum. […] We must ask ourselves whether there is some other nervous substrate in us, apart from the cerebrum, that can think and perceive, or whether the psychic processes that go in us during loss of consciousness are synchronistic phenomena, i.e., events which have no causal connection with organic processes. […] On the organic level it might be possible to regard biological morphogenesis in the light of the synchronistic factor. Professor A. M. Dalcq (of Brussels) understands form, despite its tie with matter, as a “continuity that is superordinate to the living organism”.

On the question of mind or life arising from matter and laws of physics, here is the point of view of three physicists.

Wigner, Remarks on the mind-body question (1995).
>One may well wonder how materialism, the doctrine that “life could be explained by sophisticated combinations of physical and chemical laws”, could so long be accepted by the majority of scientists. […] Everybody believes that the phenomenon of sensations is widely shared by organisms which we consider to be living. It is very likely that, if certain physico-chemical conditions are satisfied, a consciousness, that is, the property of having sensations, arises. […] The physico-chemical conditions and properties of the substrate not only create the consciousness, they also influence its sensations most profoundly. Does, conversely, the consciousness influence the physico-chemical conditions? In other words, does the human body deviate from the laws of physics, as gleaned from the study of inanimate nature? The traditional answer to this question is, “No”: the body influences the mind but the mind does not influence the body. Yet at least two reasons can be given to support the opposite thesis. […] The second argument to support the existence of an influence of the consciousness on the physical world is based on the observation that we do not know of any phenomenon in which one subject is influenced by another without exerting an influence thereupon. This appears convincing to this writer.

Wheeler, Geons, black holes, and quantum foam: a life in physics (1998).
>Life, we have every reason to believe, arose from nonlife. Why did it take the forms it has taken and evolve as it has? There is surely not one path only that it could follow. Chance played a major role.

Hiley, Process and the implicate order (2005).
>In the history of Western philosophy at least since Berkeley and Hume it has by no means always been taken for granted that mind and consciousness should be explained in terms of matter (of course, even Descartes, let alone many of the Greek philosophers, as dualists, did not try to explain mind in terms of matter). On the contrary, for many philosophers the existence of a mind-independent physical world has been the central problem of philosophy and some have opted towards antirealism, by questioning the idea that the existence of reality is independent of the human mind. But because of the current prevalence of common sense physicalism especially in the Anglo-American cognitive science and analytical philosophy community, the antirealist direction is largely ignored and the problem of consciousness tends to be the problem of explaining how consciousness arises from the physical world and how it is related to it. However, the lack of progress in this issue is beginning to alert researchers to the possibility that some form of antirealism is also a possibility when trying to understand human experience in a naturalistic fashion (see Varela 1991; Globus 1995; Pylkkö 1995).

### Memory and the ego <a name="p53"></a>

In 1700, Locke characterized a person as “a thinking intelligent Being, that has reason and reflection, and can consider it self as it self, the same thinking thing in different times and places”. On such a view, self-consciousness is essential to personhood. In particular, on Locke’s view it is the capacity to reidentify oneself at different times that is important, a claim which is in keeping with the central role of memory in his account of personal identity. Mach’s theory and life, was sometimes compared with Buddhism, namely by Heinrich Gomperz (1873-1942, Australian) who addressed Mach as the “Buddha of Science” due to his phenomenalist approach of the ego.

Mach, The analysis of sensations and the relation of the physical to the psychical (1902).
>That complex of memories, moods, and feelings, joined to a particular body (the human body), which is called the “I” or “Ego”, manifests itself as relatively permanent. I may be engaged upon this or that subject, I may be quiet and cheerful, excited and ill-humored. Yet, pathological cases apart, enough durable features remain to identify the ego. Of course, the ego also is only of relative permanency. The apparent permanency of the ego consists chiefly in the single fact of its continuity, in the slowness of its changes. […] When I recall today my early youth, I should take the boy that I then was, with the exception of a few individual features, for a different person, were it not for the existence of the chain of memories. […] The very gradual character of the changes of the body also contributes to the stability of the ego. […] Continuity alone is important. […] But continuity is only a means of preparing and conserving what is contained in the ego. This content, and not the ego, is the principal thing. This content, however, is not confined to the individual. With the exception of some insignificant and valueless personal memories, it remains presented in others even after the death of the individual. The elements that make up the consciousness of a given individual are firmly connected with one another, but with those of another individual they are only feebly connected, and the connection is only casually apparent. Contents of consciousness, however, that are of universal significance, break through these limits of the individual, and, attached of course to individuals again, can enjoy a continued existence of an impersonal, superpersonal kind, independently of the personality by means of which they were developed.
	
Hiley also mentions the effect of memory on the creation of an identity. Transience of thoughts as well as their necessary constant movement is analogous to that of matter.

Hiley, Process and the implicate order (2005).
>The implicate order seems to offer a way to encompass our thought processes. For example, if you try to hold an idea in the ‘working store’ of your mind and attempt to keep it there, you know what inevitably happens, it disappears back into the general process of thought. Indeed in the mind, thought structures are continually appearing and dissolving. However there are relatively stable forms which can be re-captured easily and these are called ‘memories’. But we know that on a longer time scale memories fade and become modified. There is some similarity with ordinary matter to which we have already referred, namely, that particles are quasi-stable forms in an underlying process. One essential difference is that familiar matter is far more stable. It should not be forgotten however that some elementary particles decay within 10−24 seconds after being created! In large scale matter, the stability is, of course, much stronger still, but even mountains move! This principle of stability is very important because without it, there would be no classical world. Thus as the particles begin to form out of the cosmic primordial Big Bang, the seeds of the classical world are sown and this world begins to dominate as more structures form and stabilize. But underlying these explicate features of matter and mind is the deeper order, namely, the implicate order. […] We lift thought into immediate attention and hold the thought as a quasi-stable structure which we can reflect on, forming a temporary Cartesian theatre. In this process it is as if thought polarizes into two distinct aspects. There is what can be made manifest, and there is the process that is producing the manifestation. These two aspects actually form one totality, but one aspect is unfolded in the relative stability of the other. One can regard the process that produces the manifestation is what we identify as self. Thus the self is built on the more stable structures that have been laid down in the brain or in the neural nets by the type of processes envisaged by Edelman (1992). These are relative stabilities built into the neuron structures. They are not permanent, they can change but it can require a lot of energy to restructure them. It must be remembered that those features that constitute the manifestation are unstable and as we know too well can easily sink into the background and requires effort to re-create them. But in thought this lack of stability is not conceived as a problem as it is when we consider “particles” as being quasi-stable forms on a background of process. Furthermore to organize any thought process we need information. We need to give form to our thought and we do that using information stored in memories or new information coming in from outside. This information does not seem to be located in any specific area of the brain. Rather it seems to be stored in a dynamic form and Pribram (1991) has argued that it may even be stored as some form of dynamic hologram. Memories are then re-created through activity in the brain. Since memory is an essential feature of self, the self is not localized at some point in the brain. The ‘homunculus’ is a global dynamical process sustained by activity itself and stabilized by active information. Thus active information is essential to develop meaningful structures in thought. Much of the information that we have available to us is not relevant except in particular circumstances. This means that much of our information is passive. Again we are always forgetting bits of information as well as being unaware of much more. In other words there is also plenty of inactive information in the world of thought! Therefore at a very general level, our proposals for the interpretation of quantum mechanics seem to suggest that there may not be such a great difference between matter and thought as the dualist supposes.

Sigmund Freud (1856-1939, German) initially considered the ego to be a sense organ for perception of both external and internal stimuli. He thought of the ego as synonymous with consciousness and contrasted it with the repressed unconscious. Nietzsche’s impact on Freud’s conception of the ego is notable. Nietzsche was sometimes tempted by skepticism about a self which can stand back from the solicitations of inclination and control them.

Nietzsche
>[Thus spoke Zarathustra, explained by Goldschmidt (1983)] If all the “values” are denounced in turn, it is not to substitute others for them, but to affirm that nothing is ever stopped, definitive, certain; everything is only a provisional stage. 
>
>[Thus spoke Zarathustra (1883)] “I” you say, and you are proud of that word. But the greater thing – in which you are unwilling to believe – is your body with its great intelligence; it does not say “I”, but performs it. What the senses feel, what the spirit perceives, is never an end in itself. But senses and spirit would like to persuade you that they are the end of all things: that is the extent of their vanity. Tools and playthings are the senses and the spirit: behind them there still lies the Self. The Self seeks with the eyes of the senses, it listens also with the ears of the spirit. The Self is ever listening and seeking; it compares, compels, conquers, destroys. It rules and is also the I’s ruler. Behind your thoughts and feelings, my brother, there stands a mighty commander, an unknown wise man – he is called Self. He lives in your body, he is your body. There is more reason in your body than in your best wisdom. And who knows to what end your body requires precisely this your best wisdom? Your Self laughs at your ego and its proud leaps. “What are these leaps and flights of fancy to me?” it says to itself. “A detour to my purpose. I am the leading reins of the I and the prompter of its conceptions”. The Self says to the I: “Feel pain here!” And then it suffers and reflects on how it might suffer no more – and this is the very purpose for which it is meant to think.

### Single/many/splitting-mind(s)/worlds/awareness <a name="p54"></a>

In 1952 Erwin Schrödinger gave a lecture in which at one point he jocularly warned his audience that what he was about to say might “seem lunatic”. He went on to assert that what the equation that won him a Nobel prize seems to be describing is several different histories, they are “not alternatives but all really happen simultaneously”. This is the earliest known reference to the many-worlds. Let us start with an explanation given by Vaidman about what is the Many-World Interpretation (MWI), as originating in 1957 in Everett’s PhD thesis. The phrase “many-worlds” is due to Bryce DeWitt (1923-2004, American) who was responsible for the wider popularization of Everett’s theory, which had been largely ignored for the first decade after publication. DeWitt’s phrase “many-worlds” has become much more popular than Everett’s “Universal Wavefunction” that many forget that this is only a difference of terminology.

Vaidman, Many-Worlds Interpretation of quantum mechanics (2018).
>The fundamental idea of the MWI, going back to Everett 1957, is that there are myriads of worlds in the Universe in addition to the world we are aware of. In particular, every time a quantum experiment with different possible outcomes is performed, all outcomes are obtained, each in a different world, even if we are only aware of the world with the outcome we have seen. […] The reason for adopting the MWI is that it avoids the collapse of the quantum wave. (Other no-collapse theories are not better than MWI for various reasons, e.g., nonlocality of Bohmian mechanics; and the disadvantage of all of them is that they have some additional structure.) The collapse postulate is a physical law that differs from all known physics in two aspects: it is genuinely random and it involves some kind of action at a distance. According to the collapse postulate the outcome of a quantum experiment is not determined by the initial conditions of the Universe prior to the experiment: only the probabilities are governed by the initial state. There is no experimental evidence in favor of collapse and against the MWI. We need not assume that Nature plays dice: science has stronger explanatory power. The MWI is a deterministic theory for a physical Universe and it explains why a world appears to be indeterministic for human observers. The MWI allows for a local explanation of our Universe. […] The MWI resolves most, if not all, paradoxes of quantum mechanics (e.g., Schrödinger cat), see Vaidman 1994. A physical paradox is a phenomenon contradicting our intuition. The laws of physics govern the Universe incorporating all the worlds and this is why, when we limit ourselves to a single world, we may run into a paradox.	

In the following, the original ideas of Everett are exposed, as well as the point of view and criticisms of Bohm, Bell and Carter.

Everett
>[“Relative state” formulation of quantum mechanics (1957)] How is one to apply the conventional formulation of quantum mechanics to the space-time geometry itself? The issue becomes especially acute in the case of a closed universe. There is no place to stand outside the system to observe it. There is nothing outside it to produce transitions from one state to another. Even the familiar concept of a proper state of the energy is completely inapplicable. In the derivation of the law of conservation of energy, one defines the total energy by way of an integral extended over a surface large enough to include all parts of the system and their interactions. But in a closed space, when a surface is made to include more and more of the volume, it ultimately disappears into nothingness. Attempts to define the total energy for a closed space collapse to the vacuous statement, zero equals zero. […] Throughout all of a sequence of observation processes there is only one physical system representing the observer, yet there is no single unique state of the observer (which follows from the representations of interacting systems). Nevertheless, there is a representation in terms of a superposition, each element of which contains a definite observer state and a corresponding system state. Thus with each succeeding observation (or interaction), the observer state “branches” into a number of different states. Each branch represents a different outcome of the measurement and the corresponding eigenstate for the object-system state. All branches exist simultaneously in the superposition after any given sequence of observations. […] The whole issue of the transition from “possible” to “actual” is taken care of in the theory in a very simple way – there is no such transition, nor is such a transition necessary for the theory to be in accord with our experience. From the viewpoint of the theory all elements of a superposition (all “branches”) are “actual”, none any more “real” than the rest. It is unnecessary to suppose that all but one are somehow destroyed, since all the separate elements of a superposition individually obey the wave equation with complete indifference to the presence or absence (“actuality” or not) of any other elements. This total lack of effect of one branch on another also implies that no observer will ever be aware of any “splitting” process. […] Arguments that the world picture presented by this theory is contradicted by experience, because we are unaware of any branching process, are like the criticism of the Copernican theory that the mobility of the earth as a real physical fact is incompatible with the common sense interpretation of nature because we feel no such motion. In both cases the argument fails when it is shown that the theory itself predicts that our experience will be what it in fact is. […] The “trajectory” of the memory configuration of an observer performing a sequence of measurements is thus not a linear sequence of memory configurations, but a branching tree, with all possible outcomes existing simultaneously in a final superposition with various coefficients in the mathematical model. In any familiar memory device the branching does not continue indefinitely, but must stop at a point limited by the capacity of the memory. In order to establish quantitative results, we must put some sort of measure (weighting) on the elements of a final superposition. This is necessary to be able to make assertions which hold for almost all of the observer states described by elements of a superposition.
>
>[Explained by Bohm in The undivided universe: an ontological interpretation of quantum theory] Everett not only assumes that the physical universe can be described completely in terms of Hilbert space, but he seems to imply that the same is true for mind, which he regards as being in essence just awareness and memory as he has defined them. It must be emphasised however that this in itself is a highly speculative assumption with very little evidence behind it. Everyone agrees that we know very little about mind or conscious awareness, but many would claim that it must include much more than just memory. Even if we accept the as yet unproven assumption that memory can be explained in the way that Everett does, it does not follow that this could be done for the whole of mind which includes attention, sensitivity to incoherence, all sorts of subtle feeling and thoughts and creative imagination as well as much more. […] Everett assumes that the whole universe including all observers exists objectively and is described completely and perfectly by a vector in Hilbert space. One may explain what this means loosely by saying that in some sense the universe is regarded as a multi-dimensional reality. But what Everett is doing, is to make a theory that relates the universe to various points of view that are contained within it. What we experience of the universe is a single subsequence of these points of view. Or, as Squires has put it, what we are dealing with is not a many-universe theory, but a theory of one universe with many viewpoints. Each point of view establishes a relationship between a state of awareness and the state of some other part of the universe containing the observing instrument and the observed object. […] We repeat again what Squires has said, i.e. it is not a theory of many universes, but a theory of many viewpoints of one universe. Everett’s aim is thus not mainly to explain the universe, but rather to explain our perceptions of the universe. Everett revealed his attitude to this question in commenting on some views on the quantum theory expressed by Einstein. In the von Neumann interpretation, observation induces a ‘collapse’ of the wave function that may affect the whole universe. Einstein objected to that as implausible and unsatisfactory. At another time Einstein put this feeling colorfully by saying that he could not believe that a mouse could bring about drastic changes in the universe by simply looking at it. In answer to this, Everett says that from the standpoint of his theory, “it is not so much the system that is affected by an observation as it is the observer who becomes correlated to the system”. Furthermore he adds “Only the observer’s state has changed, to become correlated… with the system. The mouse does not affect the universe – only the mouse is affected”. It is clear from the above that Everett did not contemplate the splitting of the universe. […] Everett’s concept of relative state could be applied to a single hydrogen atom […]. If we were to apply Everett’s theory at this level, we would have to say that the electron is aware of the proton and that the proton is aware of the electron. It would then indeed follow that the overall awareness associated with the hydrogen atom as a whole would split into many correlated states of mutual awareness of the electron and the proton, which states are, however, not aware of each other. This would imply that everything was aware of everything else and that human awareness (or indeed machine awareness) were only special cases of this.
>
>[Explained by Bell in Speakable and unspeakable in quantum mechanics (1987)] In 1957 H. Everett published a paper setting out what seemed to be a radically new interpretation of quantum mechanics. His approach has recently received increasing attention. He did not refer to the ideas of de Broglie of thirty years before nor to the intervening elaboration of those ideas by Bohm. Yet it will be argued here that the elimination of arbitrary and inessential elements from Everett’s theory leads back to, and throws new light on, the concepts of de Broglie. Everett was motivated by the notion of a quantum theory of gravitation and cosmology. In a thoroughly quantum cosmology, a quantum mechanics of the whole world, the wave function of the world could not be interpreted in the usual way. For this usual interpretation refers only to the statistics of measurement results for an observer intervening from outside the quantum system. When that system is the whole world, there is nothing outside. This situation presents no particular difficulty for the traditional (or ‘Copenhagen’) philosophy, which holds that a classical conception of the macroscopic world is logically prior to the quantum conception of the microscopic. The microscopic world is described by wave functions which are determined by and have implications for macroscopic phenomena in experimental set-ups. These macroscopic phenomena are described in a perfectly classical way (in the language of ‘be-ables’ rather than ‘observables’, so that there is no question of an endless chain of observers observing observers observing…). There is of course no sharply defined boundary between what is to be treated as microscopic and what as macroscopic, and this introduces a basic vagueness into fundamental physical theory. But this vagueness, because of the immense difference of scale between the atomic level where quantum concepts are essential and the macroscopic level where classical concepts are adequate, is quantitatively insignificant in any situation hitherto envisaged. So, it is quite acceptable to many people. […] Everett disposes of this vaguely defined suspension of the linear Schrödinger equation with the following bold proposal: it is just an illusion that the physical world makes a particular choice among the many macroscopic possibilities contained in the expansion; they are all realized, and no reduction of the wave function occurs. He seems to envisage the world as a multiplicity of ‘branch’ worlds, one corresponding to each term in the expansion. Each observer has representatives in many branches, but the representative in any particular branch is aware only of the corresponding particular memory state. So he will remember a more or less continuous sequence of past ‘events’, just as if he were living in a more or less well defined single branch world, and have no awareness of other branches. Everett actually goes further than this, and tries to associate each particular branch at the present time with some particular branch at any past time in a tree-like structure, in such a way that each representative of an observer has actually lived through the particular past that he remembers. In my opinion this attempt does not succeed and is in any case against the spirit of Everett’s emphasis on memory contents as the important thing. We have no access to the past, but only to present memories. A present memory of a correct experiment having been performed should be associated with a present memory of a correct result having been obtained. If physical theory can account for such correlations in present memories it has done enough – at least in the spirit of Everett. […] There are infinitely many different expansions […], corresponding to the infinitely many complete sets. Is there then an additional multiplicity of universes corresponding to the infinitely many terms in each expansion? I think (I am not sure) that the answer is no, and that Everett confines his interpretation to a particular expansion. […] This preference for a particular set of operators is not dictated by the mathematical structure of the wave function. It is just added (only tacitly by Everett, and only if I have not misunderstood) to make the model reflect human experience. The existence of such a preferred set of variables is one of the elements in the close correspondence between Everett’s theory and de Broglie’s – where the positions of particles have a particular role. […] Whereas Everett assumes that all configurations of his special variables are realized at any time, each in the appropriate branch universe, the de Broglie world has a particular configuration. I do not myself see that anything useful is achieved by the assumed existence of the other branches of which I am not aware. But let he who finds this assumption inspiring make it.
>
>[Explained by Carter in Classical anthropic Everett model (2012)] His attempt to provide a positive interpretation of the meaning of the “wave function” was not entirely successful. Part of the trouble arose merely from misunderstanding, due to injudicious choice of wording, whereby what I would prefer to refer to as alternative “channels” were called “branches”, thereby conveying the misleading idea of a continual multiplication of worlds (Leslie 1996), whereas (since Everett’s idea was that evolution remains strictly unitary) the “worlds” in question are strictly conserved, having neither beginning nor end: what changes is only the resolution of distinction between different “channels”, which may become finer (or coarser!) as observational information is acquired (or lost!). A more serious – since not merely semantic – problem by which many people have been puzzled is what Graham (1973) has called the “dilemma” posed by Everett’s declaration that the alternative possible outcomes of an observation are all “equally real” though not (if their quantum amplitudes are different) “equally probable”.

DeWitt and Wheeler formulated the Wheeler-DeWitt equation for the wavefunction of the Universe and advanced the formulation of Everett’s many-worlds interpretation in the 1970s. 

DeWitt
>[Explained by Bohm in The undivided universe: an ontological interpretation of quantum theory] DeWitt asserts that the universe itself is splitting into a stupendous number of branches, all resulting from the measurement-like interactions between its myriads of components. Such interactions take place, not only in measurement, but also in natural processes of all kinds which likewise bring about a continual splitting of the universe. […] The key difference between DeWitt’s approach and that of Everett is that DeWitt’s theory is purely objective, while Everett’s theory depends crucially on the inclusion of the subjective experiences of the observer. […] For DeWitt, the universe actually splits as might happen to material objects. This assumption creates a number of problems. Firstly we can ask just when does it split. […] One usually appeals to some feature such as the complex and chaotic phase relations of the wave function to make such interference unlikely to have significant effects. From DeWitt’s emphasis on ‘complexity’, one might suppose that he assumes that when the wave function reaches a certain degree of complication, the universe begins to split.
>
>[Explained by Albert and Loewer in Interpreting the many worlds interpretation (1996)] One interpretation, which we call “the splitting worlds view”, (SWV) when a quantum measurement occurs, the measuring device and indeed the whole literally splits into two or more (depending on the number possible outcomes of the measurement) worlds. […] The following passage by DeWitt suggests the SWV:
>
>>The universe is constantly splitting into a stupendous number of branches, all resulting from the measurement like interactions between its myriads of components. Moreover, every quantum transition taking place on every star, in every galaxy, in every remote corner of the universe is splitting our local world on earth into myriads of copies itself.
>
>The worlds of the SWV may remind one of the “possible worlds” which are discussed in philosophical logic. They differ at least in two respects. All the worlds of the SWV are equally real and none are abstract. In this it differs from the common view that possible worlds are abstract entities of some kind, sets of propositions or properties. It is more like David Lewis’ realism about possible worlds except that on the SWV worlds split. Second, their worlds of the SWV are “quantum” worlds in that some observables are in superpositions at a world. DeWitt says of this view:
>
>>I still recall vividly the shock I experienced on first encountering this multiworld concept. The idea of 10100+ slightly imperfect copies of oneself all constantly splitting into further copies, which ultimately become unrecognizable, is not easy to reconcile with common sense.
>
>DeWitt attempts to reconcile the SWV with common sense by arguing that “to the extent to which we can be regarded simply as automata and hence on a par with ordinary measuring apparatuses, the laws of quantum mechanics do not allow us to feel the splits”.

There are many attempts to provide an explanation of what we see based on the MWI or its variants. Albert and Loewer are proposing the Single-Mind View.

Albert and Loewer, Interpreting the many worlds interpretation (1988).
>There is simply no need, within this interpretation, to find an “explanation” of the collapse. One of the attractions of the many worlds interpretation is that since it does not require a division between measured system and measuring apparatus it is in principle possible to characterize the entire universe at any given time by a state function, a “universal wave function”, the time evolution of which is governed by a Schrödinger equation. Thus the account has special appeal to cosmologists who may want to consider the quantum state of the universe. […] It isn’t entirely clear just what the many-worlds interpretation is. In an illuminating article, Richard Healy remarked that “the interpretation itself needs interpreting”. We very much agree with him. […] Everett and DeWitt discovered, surprisingly, that even though the Schrödinger equation is inconsistent with collapses, it predicts that it will appear to observers (in the sense just explained) that collapses occur. On the Splitting World View (SWV), splitting plays the role of a collapse by producing successors of measuring devices (including observers) which record unique values of measurements. It is this which leads one, and indeed led Everett in the first place, to think that there is something to the many worlds idea. […] The SWV is actually inconsistent with the dynamical equations. For example, according to the Schrödinger equation the total mass-energy before and after the interaction are the same. […] But on the SWV a measurement literally results in an astronomical increase of the number of particles and of the total mass-energy. Of course it will be the case that this increase in the number of particles (or mass-energy) will go undetected by observers. But this in no way alleviates the main point that the splitting process is literally inconsistent with the dynamical equations and so cannot be taken as an interpretation in which all physical processes are described by them. No adequate many-worlds interpretation can countenance any such incompatibility with the dynamical equations. […] What we want is an “interpretation” which explains how it is that we always “see” (mistakenly so, if the many worlds interpretation is correct) macroscopic objects as not being in superpositions and never experience ourselves as in superpositions. […] This view, which we call the single mind view (SMV), solves some of the problems that confront the SWV. On the SMV the entire physical world is a thoroughly quantum mechanical system. […] Principle (IV) is satisfied by interpreting quantum mechanical probabilities as the probabilities that an observer (a mind) will have certain beliefs after making a measurement. The world does not split upon measurement; rather, the mind associated with a brain ends up being in the mental state associated with one of the brain states in the superposition of B states that describes her brain. The probability that the mind will end up in a particular state is completely determined by the physical state of the observer + system measured. Everett’s and DeWitt’s original argument to explain why systems appear to collapse on measurement is easily adapted by the SWV. […] Further measurements by the observer will result in “confirmation” since the brain states that are part of the overall state all perceive the same value. There is no worry about the fact that we never feel ourselves to be in or introspect superpositions since mental states are never in superpositions. […] Of course the startling feature of the SMV is its non-physicalism. On the SMV, all but one of the elements of a superposition represent, as it were, mindless brains. […] The individual minds, as on the SMV, are not quantum mechanical systems; they are never in superpositions. This is what is meant by saying that they are non-physical. The time evolution of each of the minds on the MMV, just as on the SMV, is probabilistic. However, unlike the SMV, there are enough minds associated with the brain initially so that minds will end up associated with each of the elements of the final superposition. An infinity of minds is required since a measurement or a sequence of measurements may have an infinite number of outcomes. Furthermore, although the evolution of individual minds is probabilistic, the evolution of the sets of minds … is deterministic since the evolution of the measurement process is deterministic and we can read off from the final state the proportions of the minds in various mental states. […] We have purchased supervenience of the mental on the physical at the cost of postulating an infinity of minds associated with each sentient being. No doubt this talk of infinitely many minds sounds crazy. […] The account is realist in the sense that it entails that there is a uniquely correct state for the whole universe and in the sense that it does not suppose that the state of the universe in any way depends on a consciousness or on what observables an observer decides to measure. In this it contrasts markedly with some “idealist” interpretations which entail that consciousness, by bringing about a collapse or in choosing to measure certain observables, in some mysterious way makes reality (perhaps different realities for different observers). […] One may wonder whether it may be possible to get away with less dualist commitments. One assumption, perhaps the most disturbing aspect of the dualism, is that there is a matter of fact concerning whether or not a mind associated with a brain at one time is the same mind as one associated with the brain at different time. This assumption violates the supervenience of the mental on the physical since the evolution of the physical state of the universe does not determine such identities. […] We could effect a partial retreat from this dualism by postulating that associated with a brain at any one time is a set of “momentary minds” while not postulating that there is a matter of fact that a mind (associated with a particular brain) at one moment is trans-temporal identical with a mind (associated with the same brain) at another moment. However, the cost of surrendering the “trans-temporal identity of minds” would seem to be that we can no longer make sense of statements like “the probability that I will observe spin up on measurement is p” since such statements seem to presuppose that it makes to talk of a single mind persisting through time. Perhaps, however, it could be argued (though we will not do so here) that the conception of a mind persisting through time is an illusion; one that results from the fact that “most” momentary minds will be associated with brain states which record a “personal” history and so it will seem to such a mind that it has existed as a persisting entity.

Bohm and Hiley are critical not only about Everett’s proposition, but about the theories that try to account for the behavior of the mind with physical laws.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>It should be pointed out here that the many-minds interpretation does not imply nonlocality. This comes about because reality is assumed to be in perfect correspondence with the total wave function of the universe and because the whole of this reality is covered completely by the perceptions of the many minds which have no effect on the wave function. Because Schrödinger’s equation contains only local interactions, its development is entirely local even though it has to be expressed in configuration space. Each mind becomes aware of only one facet of this multidimensional reality. The perceived classical world is then a result of the fact that each mind is only aware of a small part of the whole, the perceived classical world is in a sense an illusion. What is illusory is that this part is the whole. It if were seen to be merely a facet then there would be no illusion. To conclude, the many-minds interpretation does remove many of the difficulties with the interpretations of Everett and DeWitt, but requires making a theory of mind basically to account for the phenomena of physics. At present we have no foundations for such a theory. Indeed even to consider the mere possibility of this sort of assumption, we have further to suppose that there are new kinds of laws uniquely relating real physical properties such as awareness, to parts or components of the total vector in Hilbert space. As we have pointed out … this is equivalent to assuming that the state of awareness is not limited by anything like the uncertainty principle – clearly a gigantic step beyond present day physics. Thus even if we succeed in developing such a theory, it is clear that we will require new principles, new concepts and new equations. This means once again, that the claim of the many-worlds interpretation to avoid the addition of speculative new assumptions will fail. One of the first objections that tends to arise against the many-worlds interpretation is that its assumption of an infinity of universe or minds, almost all of them being unobservable, violates the principle of Occam’s razor, i.e. that we should not introduce a multiplicity of unnecessary assumptions. However, the proponents of this interpretation answer such objections by saying, e.g. with d’Espagnat, that multiplication of worlds entitles one to economize on principles. Or as Primas has said, “The Everett interpretation is superior in logical economy”. As we have already indicated, they would criticize our interpretation by saying that in addition to postulating the reality of the wave function, it brings in the further concept of particles with their equations of motion. It is thus clear that they feel that the many-worlds interpretation is superior because it does not do this sort of thing.

The construction of the quantum state of the Universe in terms of the quantum states of parts or objects presented above is only approximate; it is good only for all practical purposes (FAPP). Indeed, as Vaidman put it “the concept of an object itself has no rigorous definition: should a mouse that a cat just swallowed be considered as a part of the cat”. Bell claimed that “ordinary quantum mechanics is just fine FAPP” and was looking for “precise mechanics”.

Bell, Speakable and unspeakable in quantum mechanics (1987).
>Does this final synthesis, omitting de Broglie’s trajectories and Everett’s other branches, make a satisfactory formulation of fundamental physical theory? Or rather would some variation of it based on a relativistic field theory? It is logically coherent, and does not need to supplement mathematical equations with vague recipes. But I do not like it. Emotionally, I would like to take more seriously the past of the world (and of myself) than this theory would permit. More professionally, I am uneasy about the possibility of incorporating relativity in a profound way. No doubt it would be possible to ensure memory of a null result for the Michelson-Morley experiment and so on. But could the basic reality be other than the state of world, or at least a memory, extended in space at a single time – defining a preferred Lorentz frame? To try to elaborate on this would only be to try to share my confusion.

For Page, quantum mechanics may be formulated so that it contains nothing probabilistic, except conscious perceptions.

Page, Consciousness and the quantum (2011).
>Sensible Quantum Mechanics (SQM) or Mindless Sensationalism (MS) is a framework for relating consciousness to a quantum universe. It states that each conscious perception has a measure that is given by the expectation value of a corresponding quantum “awareness operator” in a fixed quantum state of the universe. […] The measures are not propensities for potentialities to be actualized, so there is nothing indeterministic in this framework, and no free will in the incompatibilistic sense. As conscious perceptions are determined by the awareness operators and the quantum state, they are epiphenomena. No fundamental relation is postulated between different perceptions (each being the entirety of a single conscious experience and thus not in direct contact with any other), so SQM or MS, a variant of Everett’s “many-worlds” framework, is a “many-perceptions” framework but not a “many-minds” framework. […] Because I regard the basic conscious entities to be the conscious experiences themselves, which might crudely be called sensations if one does not restrict the meaning of this word to be the conscious responses only to external stimuli, and because I doubt that these conscious experiences are arranged in any strictly defined sequences that one might define to be minds if they did exist, my framework has sensations without minds and hence may be labeled Mindless Sensationalism. […] I should also emphasize that by a conscious experience, I mean the phenomenal, first-person, “internal” subjective experience, and not the unconscious “external” physical processes in the brain that accompany these subjective phenomena. 

Pylkkänen reviews the difference about the relation of consciousness to quantum mechanics in the work of Stapp and Bohm.

Pylkkänen, Henry Stapp vs. David Bohm on mind, matter, and QM (2019).
>While Henry Stapp and David Bohm interacted constructively in various contexts (see, e.g., Griffin 1986), Stapp tends to be rather dismissive of Bohm’s attempts to discuss the relationship of mind and matter in the context of quantum mechanics (see, e.g., Stapp 2007). Stapp, of course, acknowledges the value of Bohm’s 1952 “causal” or “pilot wave” interpretation of QM, and Bohm and Hiley’s later development of it under the label “ontological interpretation”. He has also considered Bohm’s more general implicate order approach and even made contributions that are useful for it (see Bohm 1986; Stapp 1986). However, Stapp is critical of Bohm’s proposal that the way to bring in mind to Bohm and Hiley’s ontological interpretation is to extend the quantum ontology by adding higher level fields. […] A popular suggestion about the relationship between consciousness and quantum mechanics has to do with the role of the consciousness of the observer or experimenter in measurements (for example, such a role is crucial in Henry Stapp’s (2007) view). Bohm’s 1952 theory seems to eliminate any need for the consciousness of the observer in quantum dynamics or measurement-like interactions. Yet there is a sense in which the Bohm theory, too, opens up a new way for understanding the mind-matter relationship. […] When Bohm reflected on the mathematical form of the quantum potential Q in the late 1970s, he noticed that Q … depends only on the form or shape of the quantum field. This form typically reflects the form of the environment (e.g., whether one or two slits are open in a two-slit experiment). This suggests that the particle is not being pushed and pulled mechanically by the quantum field, but rather that the particle is able to respond to the form of the field, or is literally IN-FORMED actively by the information contained in the field. Note that this is information for the electron, not information for us. Thus, Bohm called this type of information active information. There exists potentially active information everywhere where the quantum potential is non-zero, while the information is actually active where the particle is (see Bohm and Hiley 1993). […] How is this notion of active information relevant to the mind-matter relationship? Bohm noted that what is typical of mental phenomena is activity of form (as opposed to activity of substance). When we are reading a newspaper, we are abstracting the forms of the letters; we do not need to eat the paper. These forms are taken up by the nervous system and eventually give rise to an experience of meaning. The meaning, in turn, can be active (e.g., we may visit a certain store later on that day, if there was an interesting advertisement in the newspaper). Also, when we are reading a map, an information content builds up in the mind and guides our activities in the territory. Thus, there is at least an analogy between active information at the quantum level and active information in human subjective experience. Bohm did not want to reduce the human mind and consciousness to the quantum level, but he also wanted to avoid dualism. He suggested that the quantum ontology can be extended to include higher level fields, each influencing and being influenced by levels below, and that the human mind could be a part of such a hierarchy of levels of information associated with certain neural processes (Bohm and Hiley 1993). In this way, say, when I move my hand, the information content in my thought could act down the hierarchy all the way to the level of the quantum field, which latter could then control particles (e.g., in synapses or some other relevant “quantum sites” in the brain). Such effects could then be amplified to control macroscopic neural processes. Henry Stapp has been critical of postulating such an “infinite tower” of pilot waves to explain the mind: “Bohm certainly appreciated the need to deal more substantively with the problem of consciousness. He wrote a paper on the subject (Bohm 1990) which ended up associating consciousness with an infinite tower of pilot waves, each one piloting the wave below. But the great virtue of the original pilot-wave model namely the fact that it was simple and deterministic with cleanly specified solvable equations, became lost in this infinite tower” (Stapp 2007). Bohm and Hiley acknowledged the difficulty and wrote: “… how can we understand this theory if the subtle levels are carried to infinity. Does the goal of comprehension constantly recede as we try to do this? We suggest that the appearance of such a recession is in essence just a feature of our language, which tends to give too much emphasis to the analytic side of our thought processes” (Bohm and Hiley 1993). They further suggested that the infinity of levels is analogous to the poles of a magnet, which are a feature of linguistic and intellectual analysis, and have no independent existence outside of such analysis. At every part of a magnet there is a potential pair of north and south poles that overlap each other. But such potential poles are convenient abstractions that we introduce in order to think about the actuality, namely an unbroken magnetic field.


### Physics and the origin of life <a name="p55"></a>

Life is often defined in basic biology textbooks in terms of a list of distinctive properties that distinguish living systems from non-living. Although there is some overlap, these lists are often different, depending upon the interests of the authors. Each attempt at a definition are inextricably linked to a theory from which it derives its meaning. Some biologists and philosophers even reject the whole idea of there being a need for a definition, since life for them is an irreducible fact about the natural world. Others see life simply as that which biologists study. There have been three main philosophical approaches to the problem of defining life that remain relevant today: Aristotle’s view of life as animation, a fundamental, irreducible property of nature; Descartes’s view of life as mechanism; and Kant’s view of life as organization, to which we need to add Darwin’s concept of variation and evolution through natural selection. In addition we may add the idea of defining life as an emergent property of particular kinds of complex systems.

Descartes radically reconceptualized the views of classical antiquity on emergence of life by his dualism of matter and mind; life was a problem for which an explanation was to be sought in the mechanistic interactions of matter, and there was the question of how mind was related to the matter in living beings. As chemistry developed as a discipline in the eighteenth and nineteenth centuries the goal of most advanced thinkers was to develop explanatory theories of living things in terms of chemical matter and mechanisms. Such attempts at what must be admitted to be premature reduction were resisted by critics, including some vitalists, whose positions covered a wide range from romantic anti-materialists, through chemists seeking a new type of Newtonian force (“vital force”) in nature, to materialists who had an intuition of the importance of the organized whole. The debate between the “mechanists” and the “vitalists” about the relationship of matter and life as well as matter and mind, spilled over into the twentieth century, especially during the time that biochemists were defining their field as a separate discipline from chemistry or physiology. In 1943 Erwin Schrödinger gave a series of lectures at the Dublin Institute for Advanced Studies, which were published as What is Life? in 1944. This book had a major impact on the development of twentieth century biology. Schrödinger wondered if there could be sustained order in the molecules responsible for heredity when it was well known that statistical ensembles of molecules quickly became disordered (with increased entropy). Also, how could living things generate order from disorder through their metabolism? It was through answering these two specific questions from the perspective of a physicist that Schrödinger sought to answer the big question, what is life?

Bohr, Atomic physics and human knowledge (1957).
>No proper understanding of the essential aspects of life is possible in purely physical terms. On the other hand, the view known as vitalism can hardly be given an unambiguous expression by the assumption that a peculiar vital force, unknown to physics, governs all organic life. Indeed, I think we all agree with Newton that the ultimate basis of science is the expectation that nature will exhibit the same effects under the same conditions. If, therefore, we were able to push the analysis of the mechanism of living organisms as far as that of atomic phenomena, we should not expect to find any features foreign to inorganic matter. […] We must realize that every experimental arrangement with which we could study the behaviour of the atoms constituting an organism to the extent to which this can be done for single atoms in the fundamental experiments of atomic physics will exclude the possibility of maintaining the organism alive. The incessant exchange of matter which is inseparably connected with life will even imply the impossibility of regarding an organism as a well-defined system of material particles like the systems considered in any account of the ordinary physical and chemical properties of matter. […] The sensitivity of perceptive organs, so important for the integrity of the organisms, has been found to approach the level of individual quantum processes, and amplification mechanisms play an important part especially in the transmission of nervous messages.

[Werner Heisenberg](/references/wernerheisenberg.html)
>Just as in the case of chemistry, one learns from simple biological experience that the living organisms display a degree of stability which general complicated structures consisting of many different types of molecules could certainly not have on the basis of the physical and chemical laws alone. Therefore, something has to be added to the laws of physics and chemistry before the biological phenomena can be completely understood. With regard to this question two distinctly different views have frequently been discussed in the biological literature. The one view refers to Darwin’s theory of evolution in its connection with modern genetics. According to this theory, the only concept which has to be added to those of physics and chemistry in order to understand life is the concept of history. The enormous time interval of roughly four thousand million years that has elapsed since the formation of the earth has given nature the possibility of trying an almost unlimited variety of structures of groups of molecules. Among these structures there have finally been some that could reduplicate themselves by using smaller groups from the surrounding matter, and such structures therefore could be created in great numbers. Accidental changes in the structures provided a still larger variety of the existing structures. Different structures had to compete for the material drawn from the surrounding matter and in this way, through the “survival of the fittest”, the evolution of living organisms finally took place. There can be no doubt that this theory contains a very large amount of truth, and many biologists claim that the addition of the concepts of history and evolution to the coherent set of concepts of physics and chemistry will be amply sufficient to account for all biological phenomena. One of the arguments frequently used in favor of this theory emphasizes that wherever the laws of physics and chemistry have been checked in living organisms they have always been found to be correct; there seems definitively be no place at which some “vital force” different from the forces in physics could enter. […] This second view can perhaps be stated in the following terms: It is very difficult to see how concepts like perception, function of an organ, affection could be a part of the coherent set of the concepts of quantum theory combined with the concept of history. On the other hand, these concepts are necessary for a complete description of life, even if for the moment we exclude mankind as presenting new problems beyond biology. Therefore, it will probably be necessary for an understanding of life to go beyond quantum theory and to construct a new coherent set of concepts, to which physics and chemistry may belong as “limiting cases”; History may be an essential part of it, and concepts like perception, adaptation, affection also will belong to it. If this view is correct, the combination of Darwin’s theory with physics and chemistry would not be sufficient to explain organic life; but still it would be true that living organisms can to a large extent be considered as physicochemical systems – as machines, as Descartes and Laplace have put it – and would, if treated as such, also react as such. One could at the same time assume, as Bohr has suggested, that our knowledge of a cell being alive may be complementary to the complete knowledge of its molecular structure. […] The first of these two views is more common among modern biologists than the second; but the experience available at present is certainly not sufficient to decide between the two views.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>Consider a tree, for example, which grows from a seed. Actually the seed makes a negligible contribution to the material substance of the plant and to the energy needed to make it grow. The substance comes from the air, water and soil, while the energy comes from the sun. In the absence of a seed, all these move in an implicate order of a relatively low level of subtlety (based, for example, on the order of the constituent particles). This brings about the constant re-creation of inanimate matter in various forms. However, through the information contained in the DNA molecule, the overall process is subtly altered so that it produces a living tree instead. All the streams of matter and energy which had hitherto developed in an unorganized way, now begin to bring substance and energy to the plant, which grows and eventually decays to fall back into inanimate matter. At no stage is there a break in this process. For example one would not wish to say that an ‘intimate’ molecule of carbon dioxide enters the tree and suddenly becomes ‘alive’, or that the oxygen molecule suddenly ‘dies’ when it leaves the tree. Rather, life is eternally enfolded in matter and more deeply in the underlying ground of a generalized holomovement as is mind and consciousness. Under suitable conditions, all of these unfold and evolve to become manifest in ever higher stages of organization. The notion of a separate organism is clearly an abstraction, as is also its boundary.

Frederik Hopkins (1861-1947, English, 1929 Nobel Prize in Physiology or Medicine) had a belief that although living things did not disobey any physical or chemical laws they instantiated them in ways that required understanding of biological phenomena, constraints, and functional organization. In 1913, Hopkins rejected both the reductionism of organic chemists who sought to deduce in vitro what had to happen in vivo and the crypto-vitalism of many physiologists who viewed the protoplasm of living cells as itself alive and irreducible to chemical analysis. What Hopkins offered instead was a view of the cell as a chemical machine, obeying the laws of thermodynamics and physical chemistry generally, but having organized molecular structures and functions. For Hopkins life is “a property of the cell as a whole, because it depends upon the organization of processes”. Indeed, Hopkins was impressed with the philosophy of Whitehead with its part/whole relationships and emphasis on processes rather than entities.

Whitehead
>[Explained by Hiley in Process and the implicate order] Systems form wholes or better still totalities where the whole is not the sum of the separate parts. The whole gives form to the parts, it organizes the parts so one can say there is a kind of organic process involved. Here we are using the term organic in the sense of Whitehead (1938). In other words the holomovement is a basis for an approach which we could term, following Whitehead, organic realism. […] Take life for example. Here we have another form of movement in which all the various functions of the life form are organized to work together to create and maintain the whole organism. Whitehead (1938) draws our attention to this feature when he writes:
>
>>An electron within a living body is different from an electron outside it, by reason of the plan of the body.
>
>We can think of life as an organizing energy that is working from within through the movements of its organs, its cells and indeed every molecule and atom, ultimately merging with the universal field of movement, the holomovement. This movement clearly goes beyond the motions of the individual atoms and molecules. These motions are merely the explicate orders that science has excelled at untangling and describing. What has not been appreciated is the deeper implicate order, the order that lies behind the entangled states of quantum mechanics, that lies behind the symphony and, indeed, behind life itself. It is the overarching order that organizes the parts, not through external forces but through new organizing principles that are only just beginning to be articulated. […] Whiteheadians emphasise the fundamental role played by the notion of ‘process’. We feel that our use of the word ‘movement’ is more appropriate for the subtle orders we want to include in science. The word process cannot be used, for example, in the context of a symphony. It is clearly inappropriate to talk about the process of a symphony. The word ‘to proceed’ means literally ‘to step forward’ thus refers to a special kind of movement where one step follows another. The movement of a symphony, however, involves a total ordering which involves the whole movement, past and anticipated, at any one moment. In spite of this difference, the two approaches have a basic similarity in the way they treat of the subject/object division. The way Whitehead puts it:
>
>>The philosophy of organism is the inversion of Kant’s philosophy. For Kant, the world emerges from the subject; for the philosophy of organism, the subject emerges from the world. […]
>
>The difference between mind and matter is one of complexity, but the principle is the same, so that the difference is not as great as we might expect. Processes involving mind are merely much more subtle. In this sense the emergence of consciousness is much closer to the views suggested by Searle (1992). Our proposal is that in the brain there is a manifest (or physical) side and a subtle (or mental) side acting at various levels. At each level, we can regard one side as the manifest or material side, while the other is regarded as the subtle or mental side. The material side involves electrochemical processes of various kinds, it involves neuron activity and so on. The mental side involves the subtle or virtual activities that can be actualized by active information mediating between the two sides. Thus it is the active information that provides the link between the two sides. This approach is reminiscent of Whitehead’s division of the process of concrescence into three stages. Stage I is regarded as the “physical pole” which he calls the responsive stage and is the seat of causal efficacy. Stage II is the supplementary stage which Whitehead calls the “mental pole”. Whitehead also adds a third stage which he calls “satisfaction”. This is a curious term which Epperson (2004) describes as meaning “the actualization of one of the many potential integrations generated in the first two stages”. Whether Stage III can be regarded as the mediation of some form of active information to generate the actualization is not clear to me. Indeed I find Whitehead’s language very difficult to penetrate. I would certainly not use the term “supplementary” to describe the subtle or mental side and I would certainly not use the word “satisfaction” to describe the linkage. Nevertheless the fact that Whitehead uses the terms “mental” and “physical” suggests that there is some similarity between the two approaches.

Quantum theory introduced an element of randomness standing out against the previous deterministic worldview preceding it, in which randomness expresses our ignorance of a more detailed description (as in statistical mechanics). In sharp contrast to such epistemic randomness, quantum randomness in processes such as the spontaneous emission of light, radioactive decay, or other examples has been considered a fundamental feature of nature, independent of our ignorance or knowledge. As mental activity is in some way correlated to the behavior of the material brain and since quantum theory is the most fundamental theory of matter that is currently available, it is a legitimate question to ask whether quantum theory can help us to understand consciousness. Several approaches answering this question affirmatively were proposed in recent decades. There are three basic types of corresponding approaches: (1) consciousness is a manifestation of quantum processes in the brain, (2) quantum concepts are used to understand consciousness without referring to brain activity, and (3) matter and consciousness are regarded as dual aspects of one underlying reality. Major contemporary variants of these quantum-inspired approaches make different epistemological assumptions and use quantum theory in different ways. 

Penrose and Hameroff, Consciousness in the Universe (2011).
>Consciousness implies awareness: subjective experience of internal and external phenomenal worlds. Consciousness is central also to understanding, meaning and volitional choice with the experience of free will. Our views of reality, of the universe, of ourselves depend on consciousness. Consciousness defines our existence. Three general possibilities regarding the origin and place of consciousness in the universe have been commonly expressed. (A) Consciousness is not an independent quality but arose as a natural evolutionary consequence of the biological adaptation of brains and nervous systems. The most popular scientific view is that consciousness emerged as a property of complex biological computation during the course of evolution. Opinions vary as to when, where and how consciousness appeared, e.g. only recently in humans, or earlier in lower organisms. Consciousness as evolutionary adaptation is commonly assumed to be epiphenomenal (i.e. a secondary effect without independent influence), though it is frequently argued to confer beneficial advantages to conscious species (Dennett, 1991; 1995; Wegner, 2002). (B) Consciousness is a quality that has always been in the universe. Spiritual and religious approaches assume consciousness has been in the universe all along, e.g. as the ‘ground of being’, ‘creator’ or component of an omnipresent ‘God’. Panpsychists attribute consciousness to all matter. Idealists contend consciousness is all that exists, the material world is an illusion (Kant, 1781). (C) Precursors of consciousness have always been in the universe; biology evolved a mechanism to convert conscious precursors to actual consciousness. This is the view implied by Whitehead (1929; 1933) and taken in the Penrose-Hameroff theory of ‘orchestrated objective reduction’ (‘Orch OR’). Precursors of consciousness, presumably with proto-experiential qualities, are proposed to exist as the potential ingredients of actual consciousness, the physical basis of these proto-conscious elements not necessarily being part of our current theories of the laws of the universe (Penrose and Hameroff, 1995).

Weinberg
>[Reductionism Redux (1995)] Of course everything is ultimately quantum mechanical: the question is whether quantum mechanics will appear directly in the theory of the mind, and not just in the deeper level theories like chemistry on which the theories of the mind will be based. […] Penrose may be right about that, but I doubt it.
>
>[Explained by Greene in The elegant universe (2000)] The reductionist philosophy provokes impassioned debates. Many find it revolting, or simply stupid, to claim that the wonders of life and the Universe are only the fruit of this vain waltz which animates small microscopic particles, punctuated by the laws of physics alone. Are joy, despair or boredom really just the result of chemical reactions inside our brains – reactions between molecules and atoms, which in fact only result in reactions, on a smaller scale, between the particles, themselves being only small vibrating strings? Faced with this type of criticism, the words of Nobel Prize winner Steven Weinberg, in The dream of an ultimate theory, remain cautious:
>
>>On the other side of the spectrum are the opponents of reductionism. They are horrified by what they interpret as the arid rigor of modern science. Whether their universe or themselves can be reduced, to any extent, to a story of particles, fields and their interactions, and they feel diminished … I will not attempt to respond to such criticism with a passionate talk about the beauties of contemporary science. Admittedly, the reductionist point of view sends shivers down to your spine. But we have accepted it as it is, not because we like it, but because that is how the world works. 


### Physics and the origin of human consciousness <a name="p56"></a>

The words “conscious” and “consciousness” are umbrella terms that cover a wide variety of mental phenomena. Both are used with a diversity of meanings, and the adjective “conscious” is heterogeneous in its range, being applied both to whole organisms and to particular mental states and processes. Understanding consciousness involves three major questions: What is consciousness? How does consciousness come to exist? Why does consciousness exist?

The How question focuses on explanation rather than description. It asks us to explain the basic status of consciousness and its place in nature. Is it a fundamental feature of reality in its own right, or does its existence depend upon other nonconscious items, be they physical, biological, neural or computational? And if the latter, can we explain or understand how the relevant nonconscious items could cause or realize consciousness? Put simply, can we explain how to make something conscious out of things that are not conscious?

The functional or Why question asks about the value or role or consciousness and thus indirectly about its origin. Does it have a function, and if so what is it? Does it make a difference to the operation of systems in which it is present, and if so why and how? If consciousness exists as a complex feature of biological systems, then its adaptive value is likely relevant to explaining its evolutionary origin, though of course its present function, if it has one, need not be the same as that it may have had when it first arose.
 
Chomsky, Language and nature (1995).
>We also should be able now to adopt an attitude towards the mind-body problem formulated in the wake of Newton’s demolition of materialism and the “mechanical philosophy”: for example, by Joseph Priestley, whose conclusion was “not that all reduces to matter, but rather that the kind of matter on which the two-substance view is based does not exist”, and “with the altered concept of matter, the more traditional ways of posing the question of the nature of thought and of its relations to the brain do not fit. We have to think of a complex organized biological system with properties the traditional doctrine would have called mental and physical” (Yolton 1983). […] The basic contention of Priestley and other 18th century figures seems uncontroversial: thought and language are properties of organized matter – in this case, mostly the brain, not the kidney or the foot. It is unclear why the conclusion should be resurrected centuries later as an audacious and innovative proposal: “the bold assertion that mental phenomena are entirely natural and caused by the neurophysiological activities of the brain” (Paul Churchland 1994), the hypothesis “that capacities of the human mind are in fact capacities of the human brain” (Patricia Churchland 1994); or that “consciousness is a higher-level or emergent property of the brain”, “as much of the natural biological order as … photosynthesis, digestion, or mitosis” (Searle 1992), nor why Nagel should describe this last as the “metaphysical heart” of a “radical thesis” that “would be a major addition to the possible answers to the mind-body problem” if properly clarified (as he considers unlikely: Nagel 1993). Every year or two a book appears by some distinguished scientist with the “startling conclusion” or “astonishing hypothesis” that thought in humans “is a property of the nervous system, or rather of the brain”, the “necessary result of a particular organization” of matter, as Priestley put the matter long ago. […] The current situation is that we have good and improving theories of some aspects of language and mind, but only rudimentary ideas about the relation of any of this to the brain. […] Is there a problem with internalist (or individualist) approaches to other domains of psychology? So it is widely claimed, but on dubious grounds, I think. Take the study of hearing. One long-standing question is how the auditory cortex determines the location of a sound. There does not seem to be any “auditory map”, as there is a visual and somatosensory map. Some recent work suggests that the auditory cortex registers sound location not by spatial arrangement of neurons, but by a temporal pattern of firing in a kind of “Morse code” (Barinaga 1994, Middlebrooks 1994). The discussion is worded in the usual mixture of technical and informal discourse. Someone reading it might be misled into thinking that the theory of auditory perception is externalist, making crucial reference to “solving problems” posed by the external world of sounds. But that is an illusion. The auditory system doesn’t “solve problems” in any technical sense of this term, and if they knew how to do so, the researchers might choose to stimulate the receptors directly instead of using loudspeakers – much as they did in the computer model which, in fact, provided the main evidence for their theory of sound localization, which would work as well for a brain in a vat as for an owl turning its head to face a mouse in the bush. The same considerations apply to the study of visual perception along lines pioneered by David Marr, which has been much discussed in this connection. This work is mostly concerned with operations carried out by the retina; loosely put, the mapping of retinal images to the visual cortex. Marr’s famous three levels of analysis – computational, algorithmic, and implementation – have to do with ways of construing such mappings. Again, the theory applied to a brain in a vat exactly as it does to a person seeing an object in motion. […] If Ullman could have stimulated the retina directly, he would have done that; or the optic nerve. The investigation, Ullman writes, “concerns the nature of the internal representations used by the visual system and the processes by which they are derived” (Ullman 1979). The account is completely internalist. […] No notion like “content”, or “representation of”, figures within the theory, so there are no answers to be given as to their nature. The same is true when Marr writes that he is studying vision as “a mapping from one representation to another, and in the case of human vision, the initial representation is in no doubt – it consists of arrays of image intensity values as detected by the photoreceptors in the retina” (Marr 1982) – where “representation” is not to be understood relationally, as “representation of”.

Dummett, explained by Chomsky in Language and nature (1995).
>Methodological dualism has sometimes apparently been explicitly advocated. Consider Dummett’s thesis that scientific accounts fall short of philosophical explanation for conceptual reasons. To take his example, suppose that a naturalistic approach to language succeeds beyond our wildest dreams. Suppose it provides a precise account of what happens when sound waves hit the ear and are processed, is fully integrated into a scientific theory of action, and solves the unification problem, integrating the theories of cells and computational processes. We should then have a successful theory of what Jones knows when he has acquired a language: what he know about rhyme, entailment, usage appropriate to situations, and so on. But no matter how successful, Dummett writes, these discoveries would “contribute nothing to philosophy”, which requires an answer to a different question: not how knowledge is stored or used, but “how it is delivered”. The naturalistic account would be a “psychological hypothesis”, but not a “philosophical explanation”, because it does not tell us “the form in which [the body of knowledge] is delivered” (Dummett, 1993). For the sciences, the account tells everything that can be asked about the form in which knowledge is delivered, but philosophy calls for a kind of explanation unknown in naturalistic inquiry. […] If I understand Dummett, philosophical explanation crucially involves access to consciousness.

Neural theories of consciousness come in many forms, though most in some way concern the so called “neural correlates of consciousness”. In each case the aim is to explain how organization and activity at the relevant neural level could underlie one or another major type or feature of consciousness. Some physical theories have gone beyond the neural and placed the natural locus of consciousness at a far more fundamental level, in particular at the micro-physical level of quantum phenomena. According to such theories, the nature and basis of consciousness cannot be adequately understood within the framework of classical physics but must be sought within the alternative picture of physical reality provided by quantum mechanics. The proponents of the quantum consciousness approach regard the radically alternative and often counterintuitive nature of quantum physics as just what is needed to overcome the supposed explanatory obstacles that confront more standard attempts to bridge the psycho-physical gap. There are a wide range of specific theories and models that have been proposed, appealing to a variety of quantum phenomena to explain a diversity of features of consciousness. 

Some such as Stapp have taken quantum mechanics to indicate that consciousness is an absolutely fundamental property of physical reality, one that needs to be brought in at the very most basic level. They have appealed especially to the role of the observer in the collapse of the wave function, i.e., the collapse of quantum reality from a superposition of possible states to a single definite state when a measurement is made. Such models may or may not embrace a form of quasi-idealism, in which the very existence of physical reality depends upon its being consciously observed.

[Werner Heisenberg](/references/wernerheisenberg.html)
>The existence of quantum theory has changed our attitude from what was believed in the nineteenth century. During that period some scientists were incline to think that the psychological phenomena could ultimately be explained on the basis of physics and chemistry of the brain. From the quantum-theoretical point of view there is no reason for such an assumption. We would, in spite of the fact that the physical events in the brain belong to the psychic phenomena, not expect that these could be sufficient to explain them. We would never doubt that the brain acts as a physico-chemical mechanism if treated as such; but for an understanding of psychic phenomena we would start from the fact that the human mind enters as object and subject into the scientific process of psychology.

Pylkkänen, Henry Stapp vs. David Bohm on mind, matter, and QM (2019).
>David Bohm made many suggestions about how quantum theory might be relevant to understanding the mind and its relationship to matter. Already in his 1951 textbook Quantum Theory, he proposed that there are striking analogies between quantum processes and thought. For example, a kind of uncertainty principle applies for the thought process in the sense that an attempt to define the content of a stream of thought precisely will make the “direction” of the stream of thought indeterminate. He even suggested (inspired by Niels Bohr’s writings) that these analogies could be explained if there were certain points controlling brain mechanisms so sensitive and delicately balanced that they must be described in an essentially quantum-mechanical way (he speculated that such points could exist at certain types of nerve junctions). In this way, Bohm anticipated the various “quantum mind” hypotheses that have appeared during recent decades.

Bohm and Hiley, The undivided universe: an ontological interpretation of quantum theory.
>Some neuroscientists, notably Eccles, have suggested that quantum processes may be important in understanding the more subtle activities of the brain. For example as has already been point out, we know that retinal cells respond to a few quanta at a time and that this response leads to a multiplication of their effects to a classical level of intensity. But the retina is just an extension of the brain. There could evidently be other parts of the brain in which such a sensitivity may exist, e.g. in certain kinds of synapses. If this were the case, then the brain would be a system that could, like a measuring apparatus, manifest and reveal aspects of the quantum world in the overall processes. Such quantum sensitivity would imply that in more subtle possibilities of behaviour of the brain, a classical analysis would break down. All this means that as the processes of perception unfolds into the brain, it may as it were connect to the subtle quantum domain which latter may in turn reconnect to the classical domain, as outgoing action is determined through amplification of quantum effects. […] Another possible area in which the super implicate order may be relevant is in consciousness. […] Throughout this book it has been our position that the quantum theory itself can be understood without bringing in consciousness and that as far as research in physics is concerned, at least in the present general period, this is probably the best approach. However, the intuition that consciousness and quantum theory are in some sense related seems to be a good one. […] Our proposal in this regard is that the basic relationship of quantum theory and consciousness is that they have the implicate order in common. The essential features of the implicate order are, as we have seen, that the whole universe is in some way enfolded in everything and that each thing is enfolded in the whole. […] The explicate order, which dominates ordinary ‘common sense’ experience as well as classical physics, appears to stand by itself. But actually this is only an approximation and it cannot be properly understood apart from its ground in the primary reality of the implicate order, i.e. the holomovement. All things found in the explicate order emerge from the holomovement and ultimately fall back into it. They endure only for some time, and while they last, their existence is sustained in a constant process of unfoldment and re-enfoldment, which gives rise to their relatively stable and independent forms in the explicate order. It takes only a little reflection to see that a similar sort of description will apply even more directly and obviously to consciousness, with its constant flow of evanescent thoughts, feelings, desires, urges and impulses. All of these flow into and out of each other and, in a certain sense, enfold each other (as, for example, we may say that one thought is implicit in another, nothing that this word literally means enfolded). A very clear illustrative example of this enfoldment can be seen by considering what takes place when one is listening to music. At a given moment, a certain note is being played, but a number of the previous notes are still ‘reverberating’ in consciousness. Close attention will show that it is the simultaneous presence and activity of all these related reverberations that is responsible for the direct and immediately felt sense of movement, flow and continuity, as well as for the apprehension of the general meaning of the music. To hear a set of notes so far apart in time that there is no consciousness of such reverberation will destroy altogether the sense of a whole unbroken living movement that gives meaning and force to what is being heard. The sense of order in the above experience is very similar to what is implied in our model of a particle as a sequence of successive incoming and outgoing waves. We saw how a particle trajectory could be expressed at a given moment in terms of an order of spherical waves that are present at that moment. Here the outgoing waves would correspond to ‘reverberations’, while the incoming waves would correspond to anticipations (which are also evidently present as we listen to the music). The essential point is that the whole movement is contained in this way at any given moment. Of course not all of it is conscious at any given moment and a great deal may be unconscious. But even that which is unconscious and vaguely anticipatory may contain a whole movement. The most striking example of this is Mozart saying that the whole composition came to him almost in a flash and that from there on he was simply able to play it or to write it. Thus the whole was unfolded rather as the sequence of spheres is unfolded in our model. […] The implicate order is not only the ground of perception, but also of the actual process of thought. For thought is based on information contained in the memory. We do not know a great deal of how memory works at present, but it does not seem that it operates through a one to one correspondence between, for example, an object and an image stored somewhere in the brain. Rather what is relevant for thought is the storage of many abstracted items of information, for example, about shape, color, the uses of the object, the various things with which it is associated, etc. Indeed in our customary use of language we say that a certain content is re-collected, thus suggesting that this content is collected together from the whole of memory. […] It is clear moreover that this information is basically active rather than passive. We may easily verify this in our subjective experience. Suppose, for example, that on a dark night one encounters some shadows. If the memory contains information that there may be assailants in the neighbourhood this can give rise immediately to a sense of danger, with a whole range of possibilities (fight, flight, etc.). This is not what could be called purely a mental process, but includes an involuntary and essentially unconscious process of hormones, heartbeat and neuro-chemicals of various kinds, as well as physical tensions and movements. However, if we look again and we see that it is only a shadow that confronts us, this thought has a calming effect and all this activity ceases. Such a response to information is extremely common (e.g. information that ‘X’ is a friend or enemy, good or bad etc.). More generally in consciousness, information is seen to be active in all these ways physically, chemically, electrically etc. This sort of activity is evidently similar to that which was described in connection with quantum processes involving particles such as electrons, for which the information in the wave function was also active in the movement of the particle. At first sight, however, there may still seem to be a significant difference between these two cases. Thus, in our subjective experience, action can, in some case at least, be mediated by reflection in conscious thought. On the other hand, in the case of the electron, the action of information is immediate. But actually, even if this happens, the difference is not as great as it might appear to be. For such reflection follows on the suspension of physical action. This gives rise to a train of thought. However, both a suspension of physical action and the resulting train of thought follow immediately from a further kind of active information implying the need to do this. […] It is convenient to introduce the notion that consciousness shows or manifests on two sides which may be called the physical and the mental. Active information can serve as a kind of link or ‘bridge’ between these two sides. These latter are however inseparable, in the sense, for example, that information contained in thought, which we feel to be on the mental side, is at the same time a related neurophysiological, chemical and physical activity (which is clearly what is meant by the ‘material’ side of thought). […] For the human being, all of this implied a thoroughgoing wholeness, in which mental and physical sides participate very closely in each other. Likewise, intellect, emotion, and the whole state of the body are in a similar flux of fundamental participation. Thus, there is no real division between mind and matter, psyche and soma. The common term psychosomatic is in this way seen misleading, as it suggests the Cartesian notion of two distinct substances in some kind of interaction. […] One may consider the analogy of the poles of a magnet, which are likewise a feature of linguistic and intellectual analysis, and have no independent existence outside such analysis. At every part of a magnet there is a potential pair of north and south poles that overlap each other. But these magnetic poles are actually abstractions, introduced for convenience of thinking about what is going on, while the whole process is a deeper reality – an unbroken magnetic field that is present over all space. Similarly, we may for the sake of thinking about the subject, abstract any given level of subtlety out of the unbroken whole of reality and focus our attention on it. At each such level, there will be a ‘mental pole’ and a ‘physical pole’. Thus as we have already implied, even an electron has at least a rudimentary mental pole, represented mathematically by the quantum potential. Vice versa, as we have seen, even subtle mental processes have a physical pole. But the deeper reality is something beyond either mind or matter, both of which are only aspects that serve as terms for analysis. […] Quantum physics interpreted ontologically and extended in a natural way makes possible a reflection even of the subtle mental aspects of the observer. Thus through this mirror the observer sees ‘himself’ both physically and mentally in the larger setting of the universe as a whole. There is no need, therefore, to regard the observer as basically separate from what he sees, nor to reduce him to an epiphenomenon of the objective process. Indeed the notion of separateness is an abstraction and an approximation valid for only certain limited purposes. More broadly one could say that through the human being, the universe is making a mirror to observe itself. Or vice versa the universe could be regarded as continuous with the body of the human being. After all, this latter, like the plant, gets all its substance and energy from the universe and eventually falls back into it. Evidently the human being could not exist without this context (which has very misleadingly been called an environment).

In 1946 Dennis Gabor (1900-1979, Hungarian, 1971 Nobel Prize in Physics) invented the hologram mathematically, describing a system where an image can be reconstructed through information that is stored throughout the hologram. He demonstrated that the information pattern of a three-dimensional object can be encoded in a beam of light, which is more-or-less two-dimensional. 
Holonomic brain theory is a specific theory of quantum consciousness that was developed by neuroscientist Karl Pribram (1919-2015, Austrian) initially in collaboration with Bohm. Pribram became aware of Bohm’s work in 1975 and realized that, since a hologram could store information within patterns of interference and then recreate that information when activated, it could serve as a strong metaphor for brain function. Holonomic brain theory describes human cognition by modeling the brain as a holographic storage network. Pribram suggests these processes involve electric oscillations in the brain’s fine-fibered dendritic webs, which are different from the more commonly known action potentials involving axons and synapses. These oscillations are waves and create wave interference patterns in which memory is encoded naturally, and the waves may be analyzed by a Fourier transform. Gabor, Pribram and others noted the similarities between these brain processes and the storage of information in a hologram, which can also be analyzed with a Fourier transform. In a hologram, any part of the hologram with sufficient size contains the whole of the stored information. In this theory, a piece of a long-term memory is similarly distributed over a dendritic arbor so that each part of the dendritic network contains all the information stored over the entire network. This model allows for important aspects of human consciousness, including the fast associative memory that allows for connections between different pieces of stored information and the non-locality of memory storage (a specific memory is not stored in a specific location, i.e. a certain cluster of neurons). 

Sarfatti, Retrocausality and signal nonlocality in consciousness and cosmology (2011).
>I conjecture that the intrinsic conscious mental field is analogous to a hologram […]. Therefore, different parts of the brain can locally decode messages from other distant parts without having to wait for classical electrical and chemical signal keys to move slower than light. The coherent order parameter landscape is phase and amplitude locked into the distributed pattern of the electrical and chemical signals both for sensory input and motor output in a set of creative feedback-control loops. The imprinting action of the electrical and chemical signal patterns on the intrinsically mental landscape excites our inner conscious qualia and explains the “binding problem” of how the conscious mind has an undivided wholeness. The piloting reaction of those excitations in the mental landscape back on the electrical and chemical signal pattern is our volition in which thought is expressed in our motor behavior.

Hiley, Process and the implicate order (2005).
>One of the most important questions in the mind/matter relationship is whether mental aspects can be explained in terms of physical properties of the brain. Many philosophers have argued against such a possibility. For example, Brentano and Husserl have concluded that intentionality (which has to do with the directedness of mental states towards other beings or things and more generally the meaning relation) cannot be reduced to a physical property. […] This raises the question as to whether we must necessarily have some sort of fundamental dualism in our world view. The difficulties with such a view are well known and one of the most important aims of contemporary analytical philosophers of mind is to understand how this dualism can be overcome by introducing some form of materialism. Thus it is not surprising that they have tried to accommodate intentionality and meaning within a naturalistic framework. This project is known as “naturalizing intentionality” (e.g. Fodor 1990). Part of this program involves identifying psychological features with specific physical processes occurring in the brain. These physical processes can be treated in two distinct ways. There are those who argue that there is no need to call on quantum mechanics or any other “exotic” physics in order to make progress and those who feel that it is essential to introduce some aspects of quantum theory or even “new” physics in order to capture the essential features of the relationship between mind and matter. Indeed, as Stapp (1993) has argued that without the quantum side, it will not be possible to make a satisfactory and coherent theory. […] Even if one tries to develop ontological theories of quantum phenomena such as those proposed by Bohm (1952) which is specifically designed to account for what happens between measurements, we find the observer still plays an active but subtle role, a role that is different from the role played in classical physics. The observer becomes a participator by proxy through his instruments. Even in the Bohm approach it is not possible to provide a unique and total individual view of the world that is independent of how the observer attempts to view the world. What the Bohm interpretation shows is that it is always possible to provide an ensemble of views (i.e., an ensemble of individual trajectories), each view being contingent on uncontrollable initial conditions. Any attempt to reduce the number of members of the ensemble fails and merely changes the very nature of the ensemble in an irreducible way producing another, different ensemble with an equal number of members. Thus although we can produce an ensemble of possible views consisting of actual well defined individual views, we have no way of defining a single world unambiguously. […] If we are right in identifying the quantum potential with active information and this active information is of a similar kind operating at the molecular level then this might be of direct significance to the brain, particularly if we follow the view articulated by Edelman (1992). He suggests that the key process going on in the brain is the selection and modification of groups of neurons produced by information entering through external stimuli. These groups then produce an appropriate outward response, which, if successful, will produce a stabilizing effect on the member of the species responding to the external environment in a sensible way. […] Certainly thoughts are displayed from within and can take on the appearance of permanent entities (or least semi-permanent entities) which are displayed before our intellectual gaze. We could try to liken these aspects of thought to some kind of particle-like features such as the psychons suggested by Eccles (1994), or the corticons introduced by Stuart, Takahashi and Umezawa (1978), or indeed the protophenomena postulated by MacLennan (1996). Some aspects of these thoughts could be regarded as changing in a mechanical way, but there are other more subtle features that could be regarded as being changed in a manner that is analogous to the way the quantum potential operates. […] One drawback to all this is that the particles appear to be permanent entities, whereas the semi-permanent aspects of thought can easily fade away and even disappear completely! The analogy becomes much more compelling if particles are not thought of as rock-like entities but as quasi-stable excitations of a deeper processes. Such a view is much closer to the way particles are treated in quantum field theory. Indeed the use of fields as basic entities is much more appropriate to the functioning of mind. The notions such as corticons and protophenomena can be considered as collective features of groups of neurons or as excitations of the dendritic fields or the action potentias. In this way we could provide a mathematical basis for Pribram’s (1991) holoscape. (For a detailed discussion of this approach see Jibu and Yasue 1991).

Penrose and Hameroff have developed a model according to which consciousness arises through quantum effects occurring within subcellular structures internal to neurons known as microtubules. The model posits “objective collapses” which involve the quantum system moving from a superposition of multiple possible states to a single definite state, but without the intervention of an observer or measurement as in most quantum mechanical models. The theory is called “Orchestrated Objective Reduction” (Orch OR) since the mechanism is held to be a quantum process called objective reduction that is orchestrated by microtubules. It is proposed that the theory may answer the hard problem of consciousness and provide a mechanism for free will. While mainstream theories assert that consciousness emerges as the complexity of the computations performed by cerebral neurons increases, Orch OR posits that consciousness is based on non-computable quantum processing performed by qubits (basic unit of quantum information) formed collectively on cellular microtubules, a process significantly amplified in the neurons.
In this theory, elementary acts of consciousness are neurophysiologically realized as gravitation-induced reductions of coherent superposition states in microtubules. The respective quantum states are assumed to be coherent superpositions of tubulin states, ultimately extending over many neurons. Their simultaneous gravitation-induced collapse is interpreted as an individual elementary act of consciousness. The proposed mechanism by which such superpositions are established includes a number of involved details that remain to be confirmed or disproven. According to Penrose and Hameroff, the environment internal to the microtubules is especially suitable for such objective collapses, and the resulting self-collapses produce a coherent flow regulating neuronal activity and making non-algorithmic mental processes possible. The idea of focusing on microtubules is partly motivated by the argument that special locations are required to ensure that quantum states can live long enough to become reduced by gravitational influence rather than by interactions with the warm and wet environment within the brain.

Penrose and Hameroff, Consciousness in the Universe (2011).
>How does the brain produce consciousness? An enormous amount of detailed knowledge about brain function has accrued; however the mechanism by which the brain produces consciousness remains mysterious (Koch 2004). The prevalent scientific view is that consciousness somehow emerges from complex computation among simple neurons which each receive and integrate synaptic inputs to a threshold for bit-like firing. […] The ‘hard problem’ (Chalmers 1996) is the question of how cognitive processes are accompanied or driven by phenomenal conscious experience and subjective feelings, referred to by philosophers as ‘qualia’. […] To account for the distinction between conscious activities and non-conscious ‘auto-pilot’ activities, and the fact that consciousness can occur in various brain regions, Hameroff (2009) developed the ‘Conscious pilot’ model. […] The model suggests consciousness literally moves around the brain in a mobile synchronized zone, within which isolated, entangled microtubules carry out quantum computations and Orch OR. Taken together, Orch OR and the conscious pilot distinguish conscious from non-conscious functional processes in the brain. […] Certain aspects of human consciousness, such as understanding, must be beyond the scope of any computational system, i.e. ‘non-computable’. […] The non-computable ingredient required for human consciousness and understanding, Penrose suggested, would have to lie in an area where our current physical theories are fundamentally incomplete, though of important relevance to the scales that are pertinent to the operation of our brains. The only serious possibility was the incompleteness of quantum theory – an incompleteness that both Einstein and Schrödinger had recognized. […] This incompleteness is the unresolved issue referred to as the ‘measurement problem’. […] One way to resolve it would be to provide an extension of the standard framework of quantum mechanics by introducing an objective form of quantum state reduction – termed ‘OR’ (objective reduction). […] Unitary evolution, denoted here by U, is the continuous deterministic evolution of the quantum state (i.e. of the wavefunction of the entire system) according to the fundamental Schrödinger equation, The other is the procedure that is adopted whenever a measurement of the system – or observation – is deemed to have taken place, where the quantum state is discontinuously and probabilistically replaced by another quantum state (referred to, technically, as an eigenstate of a mathematical operator that is taken to describe the measurement). This discontinuous jumping of the state is referred to as the reduction of the state (or the ‘collapse of the wavefunction’), and will be denoted here by the letter R. […] Orch OR depends, indeed, upon a particular OR extension of current quantum mechanics, taking the bridge between quantum- and classical-level physics as a ‘quantum-gravitational’ phenomenon. This is in contrast with the various conventional viewpoints, whereby this bridge is claimed to result, somehow, from ‘environmental decoherence’, or from ‘observation by a conscious observer’, or from a ‘choice between alternative worlds’, or some other interpretation of how the classical world of one actual alternative may be taken to arise out of fundamentally quantum-superposed ingredients. […] In the DP (Diosi–Penrose) scheme for OR, the superposition reduces to one of the alternatives in a time scale τ that can be estimated (for a superposition of two states each of which can be taken to be stationary on its own) according to the formula τ = h/E. Here h is Dirac’s form of Planck’s constant and E is the gravitational self-energy of the difference between the two mass distributions of the superposition. (For a superposition for which each mass distribution is a rigid translation of the other, E is the energy it would cost to displace one component of the superposition in the gravitational field of the other, in moving it from coincidence to the quantum-displaced location). According to Orch OR, the (objective) reduction is not the entirely random process of standard theory, but acts according to some non-computational new physics. The idea is that consciousness is associated with this (gravitational) OR process, but occurs significantly only when the alternatives are part of some highly organized structure, so that such occurrences of OR occur in an extremely orchestrated form. Only then does a recognizably conscious event take place. On the other hand, we may consider that any individual occurrence of OR would be an element of proto-consciousness. The OR process is considered to occur when quantum superpositions between slightly differing space-times take place, differing from one another by an integrated space-time measure which compares with the fundamental and extremely tiny Planck (4-volume) scale of space-time geometry. Since this is a 4-volume Planck measure, involving both time and space, we find that the time measure would be particularly tiny when the space-difference measure is relatively large (as with Schrödinger’s cat), but for extremely tiny space-difference measures, the time measure might be fairly long, such as some significant fraction of a second. […] In any case, we recognize that the elements of proto-consciousness would be intimately tied in with the most primitive Planck-level ingredients of space-time geometry, these presumed ‘ingredients’ being taken to be at the absurdly tiny level of 10-35 m and 10-43 s, a distance and a time some 20 orders of magnitude smaller than those of normal particle-physics scales and their most rapid processes. These scales refer only to the normally extremely tiny differences in spacetime geometry between different states in superposition, and OR is deemed to take place when such space-time differences reach the Planck level. Owing to the extreme weakness of gravitational forces as compared with those of the chemical and electric forces of biology, the energy E is liable to be far smaller than any energy that arises directly from biological processes. However, E is not to be thought of as being in direct competition with any of the usual biological energies, as it plays a completely different role, supplying a needed energy uncertainty that then allows a choice to be made between the separated space-time geometries. It is the key ingredient of the computation of the reduction time τ. Nevertheless, the extreme weakness of gravity tells us there must be a considerable amount of material involved in the coherent mass displacement between superposed structures in order that τ can be small enough to be playing its necessary role in the relevant OR processes in the brain. These superposed structures should also process information and regulate neuronal physiology. According to Orch OR, microtubules are central to these structures, and some form of biological quantum computation in microtubules (most probably primarily in the more symmetrical A-lattice microtubules) would have to have evolved to provide a subtle yet direct connection to Planck-scale geometry, leading eventually to discrete moments of actual conscious experience. […] As yet, no experiment has been refined enough to determine whether this (DP) OR proposal is actually respected by Nature, but the experimental testing of the scheme is fairly close to the borderline of what can be achieved with present-day technology (see, for example, Marshall 2003). […] Orch OR can have causal efficacy in conscious actions and behavior, as well as providing conscious experience and memory. In the absence of Orch OR, non-conscious neuronal activities might proceed by classical neuronal and microtubule-based computation. In addition there could be quantum computations in microtubules that do not reach the Orch OR level, and thereby also remain unconscious.

Most specific theories of consciousness – whether cognitive, neural or quantum mechanical – aim to explain or model consciousness as a natural feature of the physical world. However, those who reject a physicalist ontology of consciousness must find ways of modeling it as a nonphysical aspect of reality. Thus those who adopt a dualist or anti-physicalist metaphysical view must in the end provide specific models of consciousness. Both substance dualists and property dualists must develop the details of their theories in ways that articulate the specific natures of the relevant non-physical features of reality with which they equate consciousness or to which they appeal in order to explain it.

A variety of such non-physical models have been proposed. Chalmers argues for an “explanatory gap” from the objective to the subjective, and criticizes physicalist explanations of mental experience, making him a dualist. Chalmers characterizes his view as “naturalistic dualism”: naturalistic because he believes mental states are caused by physical systems (such as brains); dualist because he believes mental states are ontologically distinct from and not reducible to physical systems. This view could also be characterized by more traditional formulations such as property dualism.

Chalmers has offered an admittedly speculative version of panpsychism which calls upon the notion of information not only to explain psycho-physical invariances between phenomenal and physically realized information spaces but also to possibly explain the ontology of the physical as itself derived from the informational (a version of “it from bit” theory). Some quantum theorists such as Stapp treat consciousness as a fundamental feature of reality, and insofar as they do so, they might be plausibly classified as non-physical theories as well. 

Chalmers, explained by Hiley in Process and the implicate order (2005).
>There is the problem of experience vs. objective physical process. Why should an objective physical process, such as a neurophysiological process, give rise to or be accompanied by experience at all? This is what Chalmers (1996) calls the “hard problem of consciousness”. There are at least two different ways in which one could approach the hard problem in light of the quantum theory, via the suggestion of active information and Bohm’s (1980) deeper suggestions involving the implicate order. It is interesting to note that Chalmers (1996) himself has approached the hard problem by first postulating that information is a fundamental property of the universe and then by further suggesting that information has two aspects, the physical and the phenomenal. By saying, in effect, that something phenomenal is fundamental in the universe, he hopes to make the appearance of conscious experience with physical processes more intelligible. In this regard the ontological interpretation provides a very concrete example of how information plays a fundamental role in physical processes, perhaps more so than Chalmers’ discussion has been able to establish so far. If we now interpret active information in the spirit of Chalmers by assuming that it has both a phenomenal and a physical aspect, we have a concept of information which is both concrete and fundamental and also may help to approach the hard problem of consciousness in a more satisfactory way. This discussion is very tentative and general indeed. Most importantly one needs to note that Chalmers uses a Shannon-type notion of information which is different from Bohm’s notion of active information.

Though, Chalmers argued against quantum consciousness. He instead discussed how quantum mechanics may relate to dualistic consciousness but is skeptical of the ability of any new physics to resolve the hard problem of consciousness. Predicate dualism is a view espoused by nonreductive physicalists such as Jerry Fodor (1935-2017, American). Following the path paved by linguist Chomsky, Fodor developed a strong commitment to the idea of psychological nativism, that postulates the innateness (at least some knowledge about language exists in humans at birth) of many cognitive functions and concepts. Fodor strongly opposed reductive accounts of the mind. He argued that mental states are multiple realizable and that there is a hierarchy of explanatory levels in science such that the generalizations and laws of a higher-level theory of psychology or linguistics, for example, cannot be captured by the low-level explanations of the behavior of neurons and synapses. Fodor thus maintained that while there is only one ontological category of substances and properties of substances (usually physical), the predicates that we use to describe mental events cannot be redescribed in terms of (or reduced to) physical predicates of natural languages. Fodor was also a prominent critic of the “ill-grounded Darwinian and neo-Darwinian theories of natural selection”.

A close view of information as a fundamental property of the universe presented by Chalmers is also expressed by Dyson, who puts a deeper word: “mind”. As he wrote in 2002, “mind and intelligence are woven into the fabric of our universe in a way that altogether surpasses our comprehension”.

Dyson, explained by Hiley in Process and the implicate order (2011).
>If we turn to quantum mechanics … All that we can describe are relationships between successive observations and no attempt is made to describe what is happening between such observations. This is not merely a form of logical positivism, it goes much deeper. For example Dyson (1979) writes:.
>
>>I cannot help thinking that our awareness of our own brains has something to do with the process of “observation” in atomic physics. That is to say, I think our consciousness is not a passive epiphenomenon carried along by the chemical events in our brains, but is an active agent forcing the molecular complexes to make choices between one quantum state and another. In other words, mind is already inherent in every electron, and the processes of human consciousness differ only in degree but not in kind from the processes of choice between quantum states which we call “chance” when made by electrons.
>
>One could then further postulate that some primitive sort of intentionality is a fundamental property of the universe, perhaps as fundamental as the more usual physical properties such as mass, charge and spin (Fodor 1987); something similar has been proposed about information by Chalmers 1996; for further discussion see Pylkkänen 1992.

Neural oscillations, or brainwaves, are rhythmic or repetitive patterns of neural activity in the central nervous system and were observed by researchers as early as 1924. At the level of neural ensembles, synchronized activity of large numbers of neurons can give rise to macroscopic oscillations, which can be observed in an electroencephalogram (EEG). Oscillatory activity in groups of neurons generally arises from feedback connections between the neurons that result in the synchronization of their firing patterns. The interaction between neurons can give rise to oscillations at a different frequency than the firing frequency of individual neurons. A major area of research in neuroscience involves determining how oscillations are generated and what their roles are. Numerous experimental studies support a functional role of neural oscillations; a unified interpretation, however, is still lacking.

The first discovered and best-known frequency band is alpha activity (8-12 Hz) that can be detected from the occipital lobe during relaxed wakefulness and which increases when the eyes are closed. Other frequency bands are: delta (1-4 Hz), theta (4-8 Hz), beta (13-30 Hz), low gamma (30-70 Hz), and high gamma (70-150 Hz) frequency bands, where faster rhythms such as gamma activity have been linked to cognitive processing. Indeed, EEG signals change dramatically during sleep and show a transition from faster frequencies to increasingly slower frequencies such as alpha waves. In fact, different sleep stages are commonly characterized by their spectral content. Consequently, neural oscillations have been linked to cognitive states, such as awareness and consciousness.

Penrose and Hameroff, Consciousness in the Universe (2011).
>Neuronal synchrony, e.g. gamma synchrony EEG (30 to 90 Hz), the best measurable correlate of consciousness, does not derive from neuronal firings. […] The best known temporal correlate for consciousness is gamma synchrony EEG, 30 to 90 Hz, often referred to as coherent 40 Hz. One possible viewpoint might be to take this oscillation to represent a succession of 40 or so conscious moments per second (τ = 25 milliseconds). This would be reasonably consistent with neuroscience (gamma synchrony), with certain ideas expressed in philosophy (e.g. Whitehead ‘occasions of experience’), and perhaps even with ancient Buddhist texts which portray consciousness as ‘momentary collections of mental phenomena’ or as ‘distinct, unconnected and impermanent moments which perish as soon as they arise.’ (Some Buddhist writings quantify the frequency of conscious moments. For example the Sarvaastivaadins, according to von Rospatt 1995, described 6,480,000 ‘moments’ in 24 hours – an average of one ‘moment’ per 13.3 msec, ~75 Hz – and some Chinese Buddhism as one “thought” per 20 msec, i.e. 50 Hz.) These accounts, even including variations in frequency, could be considered to be consistent with Orch OR events in the gamma synchrony range. Accordingly, on this view, gamma synchrony, Buddhist ‘moments of experience’, Whitehead ‘occasions of experience’, and our proposed Orch OR events might be viewed as corresponding tolerably well with one another. […] It may be noted that Tibetan monk meditators have been found to have 80 Hz gamma synchrony, and perhaps more intense experience (Lutz 2004).

Hiley, Process and the implicate order (2005).
>The time synchronization provided by the well-known 40 Hz oscillation does not necessarily qualify as an explanation of the unity of consciousness insofar as it relies on essentially classical notions which presuppose separability.

### Consciousness at the cell level <a name="p57"></a>

Eukaryotes are organisms whose cells have a nucleus enclosed within membranes, unlike prokaryotes, which have no membrane-bound organelles. Protozoan is an informal term for single-celled eukaryotes, either free-living or parasitic, which feed on organic matter such as other microorganisms or organic tissues and debris. Historically, the protozoa were regarded as “one-celled animals”, because they often possess animal-like behaviors, such as motility and predation, and lack a cell wall, as found in plants and many algae. Although the traditional practice of grouping protozoa with animals is no longer considered valid, the term continues to be used in a loose way to identify single-celled organisms that can move independently and feed by heterotrophy. 
Recently, researchers put forward some kind of cognition and volition – that could be referred to as consciousness – at the level of single-cell organisms, that do not possess a “brain”. In particular, currently examined organisms are ciliates (a group of protozoans characterized by the presence of hair-like organelles called cilia) such as Paramecium and Stentor roeseli. The internal structure of cilia is made up of a microtubule-based cytoskeleton and the same microtubules are the basis of the Orchestrated objective reduction theory of Penrose and Hameroff.

Baluska and Reber, Sentience and consciousness in single cells (2019).
>We put forward a reductionistic, bottom-up, cellular-based concept of the origins of sentience and consciousness. Because all life is based on cells, any evolutionary theory of the emergence of sentience and consciousness must be grounded in mechanisms that take place in prokaryotes, the simplest unicellular species. We posit that subjective awareness is a fundamental property of cellular life. It emerged as an inherent feature of, and contemporaneously with, the very first life-forms. All other varieties of mentation are the result of evolutionary mechanisms based on this singular event. Therefore, all forms of sentience and consciousness evolved from this original instantiation in prokaryotes. There is a growing suspicion among biologists, ethologists, and geneticists that mental states, awareness, consciousness or, to use a more general term, sentience, is an inherent feature of life. This proposition, of course, is not new but it has never been part of the standard model. Over a century ago anatomist Charles Minot (1902) maintained that “A frank unbiased study of consciousness must convince every biologist that it is one of the fundamental phenomena of at least all animal life if not, as is quite possible, of all life”. Since then several prominent biologists and life scientists have expressed similar sentiments. George Gaylord Simpson noted that “All the essential problems of living organisms are already solved in the one-celled … protozoan and these are only elaborated in man or the other multicellular animals”. In her Nobel speech geneticist Barbara McClintock referred to the “knowledge the cell has of itself and how it utilizes this knowledge in a ‘thoughtful’ manner”. Evolutionary biologist Lynn Margulis, in a paper provocatively titled “The conscious cell”, maintained that “consciousness, awareness of the surrounding environment, starts with the beginning of life itself”. We concur. […] Two critical elements, however, have been largely absent from these proposals and, perhaps, kept this perspective from becoming part of mainstream evolutionary biology. The first is acknowledging the fact that sentience is a necessity for an adaptive life. Within standard approaches to evolutionary biology, sentience/consciousness is viewed as a feature that emerged at some time in some species and is not regarded as essential for all life. We will present arguments supporting the contention that this position is wrong – that, in fact, all adaptive and functioning organisms, from the earliest on, must be sentient, conscious, and have an ontological self-awareness. A non-sentient organism, we maintain, would be an evolutionary dead-end. As philosopher Thomas Nagel might put it, “there is something that it is like to be a Paramecium”. The second missing element is identifying one or more coherent mechanisms through which the non-sentient, prebiotic slurry of molecules became the epistemic foundation of a sentient agent – in short, how “the material creates the mental”, or what philosopher David Chalmers famously called the “hard problem” of consciousness. There have been a few efforts here but the issue has been largely neglected by cell biologists. […] A fundamental axiom of our model is that these internal, subjective forms of sentience are an essential component of all life forms. They emerged with the first appearance of life and all more complex, varied forms of mental life are the result of evolutionary mechanisms. Just as all life forms extant and extinct are the descendants of a singular event some 3.5 billion years ago, so all the varied forms of mental life, of sentience, are derived directly from those initial ancient prokaryotic species. In short, life and sentience are co-terminous. We find it interesting that evolutionary biologists, psychologists, philosophers are all comfortable with the notion that the bio-physical elements of life appeared just once but, somehow, are uncomfortable with the notion that mental elements accompanied them. […] There are arguments against our overall proposition but they suffer from various problems. For example, take the one put forward by prominent philosopher Daniel Dennett (2017) who argues that behaviors such as learning or communicating, when observed in unicellular or simpler multicellular species, are nothing but the blind actions of genetic programs that spin themselves out without awareness or other internal subjective states. In his terms they have “competence” but lack “comprehension” of what they are doing. This proposition is not wrong in any fundamental way. It is true, as he notes, that termites build nests without any mental plan whereas humans build cathedrals with well-constructed blueprints. The deeper question that concerns us is whether that more primitive state, the one where competence is displayed, is one that has a subjective, sentient component. Dennett demurs on this point. He grants the proposition that prokaryotes are subjectively self-aware a non-zero probability of being true. We agree but argue that that probability is, in fact, 1.0. Prokaryotes such as bacteria are extremely adept organisms. We recognize that the prokaryotes studied currently are organisms that have been evolving for several billion years and almost certainly are not identical with the original species. But the challenges of survival in the various environments that they currently inhabit share features with the primordial one. […] It seems not unreasonable to use contemporary species as representative of the first life-forms. And, of course, we really have no choice in this matter. […] Biophysicist Jané Kondev was so taken with the range of behaviors of E. coli that he expressed the opinion that they seemed to simply have “free will”. […] What we are arguing is that the maximally adaptive process would be for the first unicellular organisms to have sentience encoded in its DNA and, in virtue of such a subjective capacity to monitor its metabolic and behavioral functions, be able to react adaptively to each of the valenced objects and events it encountered. This, we maintain, is where the first minds appeared and the mental states and functions of all subsequent species are derived from this [proto]-consciousness. It happened only once on this planet and, like the underlying biomechanical processes that gave rise to life, every other species carries these essential pieces of genetic material that code for sentience. […] There are at least three possible subcellular sources for the emergence of sentience and consciousness at the cellular and subcellular levels that would seem to be candidates here. First are the excitable membranes, ones equipped with critical proteins enriched especially in highly ordered lipid rafts. Second are the excitable and vibrating microtubules and actin filaments in cells. The third are biological quasicrystals with the five-fold symmetry. […] How could individual cellular consciousness generate larger-order supracellular consciousness of multicellular organisms? In fact, this crucial question is relevant already at the level of the eukaryotic cell, which is, in fact, a consortium of several prokaryotic cells transformed into the cytoplasm, mitochondria, plastids and perhaps also nuclei. These organelles of eukaryotic cells lived originally independent lives and are also enclosed via membranes equipped with critical proteins endowing sentience and an internal awareness of features of the external world. Eukaryotic cells are – evolutionarily speaking – multi-cellular assemblies based on three (animals) or four (plants) originally independent organisms/cells. In contrast to simpler prokaryotic cells, which are strictly unicellular organisms, complex eukaryotic cells are multicellular organisms (cells within cells). In order to integrate the eukaryotic cells into a single coherent supracellular unit, individual partner cells use their synaptic cell-cell adhesion domains to negotiate the structural and functional unity seen in present supracellular eukaryotic cells. In this sense, the pre-eukaryotic cell emerged from the union of two different cells. […] Later, during evolution of such multi-cellular eukaryotic cells, bacteria were internalized and transformed into the energy power-houses of eukaryotic cells: mitochondria and plastids. These new bacterial organelles allowed evolution of higher cellular complexity and embarking on the true multicellular evolutionary pathway leading to complex fungi, animals, and plants. It might be speculated that the synaptic-like nature of the eukaryotic cell is the central feature allowing evolution of true multicellularity, which was never attained by in the prokaryote domains. Finally, the multi-cellular nature of the eukaryotic cell has profound implications for the sentience of such a multi-cellular cell. […] Ecologist Tom Fenchel stated that the eukaryotic cell can be viewed as a cellular consortium acting as a self-contained ecosystem. It is relevant in this respect that all living systems require biocommunication to solve problems during their evolution. Besides synaptic principles, another possible mechanism might be provided via ephaptic phenomena when the adjacent cells do not need to be in direct structural contacts but interact via extracellular electric and electromagnetic fields. Such ephaptic coupling is found in tissues organized by other excitable cells, such as cardiac muscle cells, and it can be expected to be active in any tissue composed of excitable cells assembling into such oscillating units. In plants, the best candidate for such an ephaptic unit is the oscillating zone of the root apex.

Dexter, Prabakaran and Gunawardena, A complex hierarchy of avoidance behaviors in a single-cell eukaryote (2019).
>Complex behavior is associated with animals with nervous systems, but decision-making and learning also occur in non-neural organisms, including singly nucleated cells and multinucleate synctia. Ciliates are single-cell eukaryotes, widely dispersed in aquatic habitats, with an extensive behavioral repertoire. In 1906, Herbert Spencer Jennings described in the sessile ciliate Stentor roeseli a hierarchy of responses to repeated stimulation, which are among the most complex behaviors reported for a singly nucleated cell. These results attracted widespread interest and exert continuing fascination but were discredited during the behaviorist orthodoxy by claims of non-reproducibility. These claims were based on experiments with the motile ciliate Stentor coeruleus. We acquired and maintained the correct organism in laboratory culture and used micromanipulation and video microscopy to confirm Jennings’ observations. Despite significant individual variation, not addressed by Jennings, S. roeseli exhibits avoidance behaviors in a characteristic hierarchy of bending, ciliary alteration, contractions, and detachment, which is distinct from habituation or conditioning. Remarkably, the choice of contraction versus detachment is consistent with a fair coin toss. Such behavioral complexity may have had an evolutionary advantage in protist ecosystems, and the ciliate cortex may have provided mechanisms for implementing such behavior prior to the emergence of multicellularity. Our work resurrects Jennings’ pioneering insights and adds to the list of exceptional features, including regeneration, genome rearrangement, codon reassignment, and cortical inheritance, for which the ciliate clade is renowned. Ciliates form a clade of single-cell eukaryotes characterized by their eponymous cilia, nuclear dimorphism, and sexual conjugation. S. roeseli is colorless, trumpet shaped, and visible to the naked eye. Ciliary rows along the axis and ciliary spirals at the wider end generate a fluid vortex to bring food particles to the “mouth”. S. roeseli is typically sessile, anchoring itself to algal detritus with a holdfast of secreted mucus. […] Jennings reported a hierarchy of behaviors – resting (R), bending away (B), ciliary alteration (A), contraction (C), and detachment from the holdfast (D) – in response to repeated stimulation. […] We found these same behaviors repeatedly in experiments conducted over several months. […] We consider the behavior hierarchy as a form of sequential decision-making, in the sense that, when given similar stimulation repeatedly, the organism “changes its mind” about which response to give, thereby following the observed hierarchy. Cellular decision-making has been widely discussed, but this form of it is simpler than heritable phenotypic change or adaptive choice when confronting multiple stimulations. […] The ciliate membrane and cytoskeletal cortex are the most likely candidates for mechanistically implementing the behaviors observed here. […] The ciliate membrane is excitable. It harbors voltage-dependent and mechanosensitive ion channels that generate action potentials, analogous to those in neurons, and these channels play a key role in habituation. The cortex can propagate to daughter cells, in a non-genetic and Lamarckian manner, micro-surgical alterations to ciliary geometry, giving rise thereby to “cortical inheritance”. […] The cortex also plays the central role in regeneration: excised fragments of a ciliate, provided they contain appropriate parts of the cortex, will reconstruct themselves into smaller, whole organisms. With such extravagant capabilities for self-organization at its disposal, S. roeseli’s avoidance hierarchy may begin to seem less extraordinary.

Gunawardena, Interview by the Harvard Medical School (2019).
>Organisms like S. roeselii were apex predators prior to multicellular life, and they are extremely widespread in many different aquatic environments. They have to be “clever” at figuring out what to avoid, where to eat and all the other things that organisms have to do to live. I think it’s clear that they can have complex ways of doing so. They do the simple things first, but if you keep stimulating, they “decide” to try something else. S. roeselii has no brain, but there seems to be some mechanism that, in effect, lets it “change its mind” once it feels like the irritation has gone on too long. This hierarchy gives a vivid sense of some form of relatively complex, decision-making calculation going on inside the organism, weighing whether it’s better to execute one behavior versus another. In developmental biology or cancer research, for example, the processes cells undergo are often referred to as programs, suggesting that cells are “programmed” to do what they do. But cells exist in a very complex ecosystem, and they are, in a way, talking and negotiating with each other, responding to signals and making decisions. I think this experiment forces us to think about the existence of, very speculatively, some form of cellular “cognition”, in which single cells can be capable of complex information processing and decision-making in response. All life has the same underpinnings, and our results give us at least one piece of evidence for why we should be broadening our view to include this kind of thinking in modern biology research.

Penrose and Hameroff, Consciousness in the Universe (2011).
>Protozoans like Paramecium can swim, find food and mates, learn, remember and have sex, all without synaptic computation (Sherrington, 1957). […] Hameroff had been intrigued by seemingly intelligent, organized activities inside cells, accomplished by protein polymers called microtubules. Major components of the cell’s structural cytoskeleton, microtubules also accounted for precise separation of chromosomes in cell division, complex behavior of Paramecium, and regulation of synapses within brain neurons. The intelligent function and periodic lattice structure of microtubules suggested they might function as some type of biomolecular computer. Microtubules are self-assembling polymers of the peanut-shaped protein dimer tubulin, each tubulin dimer (110,000 atomic mass units) being composed of an alpha and beta monomer. Thirteen linear tubulin chains (‘protofilaments’) align side-to-side to form hollow microtubule cylinders (25 nanometers diameter) with two types of hexagonal lattices. […] Microtubules also fuse side-by-side in doublets or triplets. Nine such doublets or triplets then align to form barrel-shaped mega-cylinders called cilia, flagella and centrioles, organelles responsible for locomotion, sensation and cell division. Either individually or in these larger arrays, microtubules are responsible for cellular and intra-cellular movements requiring intelligent spatiotemporal organization. Microtubules have a lattice structure comparable to computational systems. Could microtubules process information? The notion that microtubules process information was suggested in general terms by Sherrington (1957) and Atema (1973). With physicist colleagues through the 1980s, Hameroff developed models of microtubules as information processing devices, specifically molecular (‘cellular’) automata, self-organizing computational devices. Cellular automata are computational systems in which fundamental units, or ‘cells’ in a grid or lattice can each exist in specific states, e.g. 1 or 0, at a given time (Wolfram, 2002). […] Approximately 108 tubulins in each neuron switching and oscillating in the range of 107 per second (e.g. Pokorny 8 MHz) gives an information capacity at the microtubule level of 1015 operations per second per neuron. […] Total brain capacity when taken at the microtubule level (in 1011 neurons) would potentially be 1026 operations per second, pushing the goalpost for AI brain equivalence farther into the future, and down into the quantum regime. […] Microtubules may have appeared in eukaryotic cells 1.3 billion years ago due to symbiosis among prokaryotes, mitochondria and spirochetes, the latter the apparent origin of microtubules which provided movement to previously immobile cells (e.g. Margulis and Sagan 1995).


### Beyond: universal consciousness and love <a name="p58"></a>

Universal mind or universal consciousness is a metaphysical concept suggesting an underlying essence of all being and becoming in the universe. It includes the being and becoming that occurred in the universe prior to the arising of the concept of “Mind”, a term that more appropriately refers to the organic, human, aspect of universal consciousness. It addresses inorganic being and becoming and the interactions that occur in that process without specific reference to the physical and chemical laws that try to describe those interactions. Those interactions have occurred, do occur, and continue to occur. Universal consciousness is the source that underlies those interactions and the awareness and knowledge they imply. This concept was presented by Anaxagoras who taught that the growth of living things depends on the power of mind within the organisms that enables them to extract nourishment from surrounding substances. Nowadays, this concept is associated with viewed as esoteric and mystic, yet, some researchers do not hesitate to refer to it implicitly or explicitly. Universal mind may be viewed from a (fringe) scientific perspective as non-local consciousness. Michael Persinger (1945-2018, American) wrote on non-local consciousness. He argued that all phenomena including consciousness, spiritual experiences, and “paranormal events” can be explained by universal physical mechanisms and can be verified using the scientific method.

Kafatos, Tanzi and Chopra, How consciousness becomes the physical universe (2011).
>In our view, it may well be that the subject-object dichotomy is false to begin with and that consciousness is primary in the cosmos, not just an epiphenomenon of physical processes in a nervous system. Accepting this assumption would turn an exceedingly difficult problem into a very simple one. We will sidestep any precise definition of consciousness, limiting ourselves for now to willful actions on the part of the observer. These actions, of course, are the outcome of specific choices in the mind of the observer. Although some mental actions could be automated, at some point the will of conscious observer(s) sets the whole mechanical aspects of observation in motion. […] Quantum theory today encompasses the interplay of the observer’s free choices and nature’s “choices” as to what constitute actual outcomes. This dance between the observer and nature gives practical meaning to the concept of the participatory role of the observer. […] It is essential that we avoid the mistake of rooting a physical universe in the physical brain, for both are equally rooted in the non-physical. For practical purposes, this means that the brain must acquire quantum status, just as the atoms that make it up have. The standard assumption in neuroscience is that consciousness is a byproduct of the operation of the human brain. The multitude of processes occurring in the brain covers a vast range of spatio-temporal domains, from the nanoscale to the everyday human scale (e.g. Bernroider and Roy 2004). Even though they differ on certain issues, a number of scientists accept the applicability of QM at some scales in the brain (cf. Kafatos 2009). For example … Hameroff and Penrose (1996) postulate collapses occurring in microtubules induced by quantum gravity. In their view, quantum coherence operates across the entire brain. Stapp (2007) prefers a set of different classical brains that evolve according to the rules of QM, in accordance with the uncertainty principle. He contends that bringing in (the still not developed) quantum gravity needlessly complicates the picture. […] It is conceivable that the overall biological structures of the brain may require global relationships, which come down processes to global complementarity – every single process is subordinated to the whole. Not just single neurons but massive clusters and networks communicate all but instantaneously. One must also account for the extreme efficiency with which biological organisms operate in a holistic manner, which may only be possible by the use of quantum mechanical formalisms at biological, and neurophysiological relevant scales (cf. Frohlich 1983; Roy and Kafatos 2004; Bernroider and Roy 2005; Davies, 2004; Stapp 2004; Hameroff 2002; Hagan 2002; Hammeroff and Tuszynski 2003; Rosa and Faber 2004; Mesquita 2005; Hunter 2006; Ceballos 2007). […] Whatever is the fundamental source of creation, it itself must be uncreated. Otherwise, there is a hidden creator lying in the background, and then we must ask who or what created that. What does it mean to be uncreated? The source of reality must be self-sufficient, capable of engendering complex systems on the micro and macro scale, self-regulating, and holistic. Nothing can exist outside its influence. Ultimately, the uncreated source must also turn into the physical universe, not simply oversee it as God or the gods do in conventional religion. We feel that only consciousness fits the bill, for as a prima facie truth, no experience takes place outside consciousness, which means that if there is a reality existing beyond our awareness (counting mathematics and the laws of physics as part of our conscious experience), we will never be able to know it. The fact that consciousness is inseparable from cognition, perception, observation, and measurement is undeniable; therefore, this is the starting point for new insights into the nature of reality. What is the nature of consciousness in our model? We take it as a field phenomenon, analogous to but preceding the quantum field. This field is characterized by generalized principles already described by quantum physics: complementarity, non-locality, scale-invariance and undivided wholeness. But there is a radical difference between this field and all others: we cannot define it from the outside. To extend Wheeler’s reasoning, consciousness includes us human observers. We are part of a feedback loop that links our conscious acts to the conscious response of the field. In keeping with Heisenberg’s implication, the universe presents the face that the observer is looking for, and when she looks for a different face, the universe changes its mask. Consciousness includes human mental processes, but it is not just a human attribute. Existing outside space and time, it was “there” “before” those two words had any meaning. In essence, space and time are conceptual artifacts that sprang from primordial consciousness. The reason that the human mind meshes with nature, mathematics, and the fundamental forces described by physics, is no accident: we mesh because we are a product of the same conceptual expansion by which primordial consciousness turned into the physical world. The difficulty with using basic terms like “concept” and “physical” is that we are accustomed to setting mind apart from matter; therefore, thinking about an atom isn’t the same as an atom. Ideas are not substances. But if elementary particles and all matter made of them aren’t substances, either, the playing field has been leveled. Quantum theory gives us a model that applies everywhere, not just at the micro level. The real question, then, isn’t how to salvage our everyday perception of a solid, tangible world but how to explore the mysterious edge where micro processes are transformed into macro processes, in other words, how Nature gets from microcosm to macrocosm. There, where consciousness acquires the nature of a substance, we must learn how to unify two apparent realities into one. We can begin to tear down walls, integrating objects, events, perceptions, thoughts, and mathematics under the same tent: all can be traced back to the same source. […] This simple unifying approach must be taken, we realize, as a basic ontological assumption, since it cannot be proven in an objective sense. We cannot extract consciousness from the physical universe, despite the fervent hope of materialists and reductionists. They are forced into a logical paradox, in fact, for either the molecules that make up the brain are inherently conscious (a conclusion to be abhorred in materialism), or a process must be located and described by which those molecules invent consciousness – such a process has not and never will be specified. It amounts to saying that table salt, once it enters the body, finds a way to dissolve in the blood, enter the brain, and in so doing learns to think, feel, and reason.

Langer expresses monistic and universal views with the concept of mindfulness. Mindfulness is originally a significant element of Buddhist traditions and meditation techniques. This is further explained by Rinpoche.

Langer, A mindful alternative to the mind/body problem (2011).
>Today it is more or less taken for granted that mind affects body although the pathways are still unknown but mind-body monism is not accepted by the modern scientific world because pathways are still unknown. […] Mindfulness is defined as an active state of mind characterized by novel distinction–drawing that results in being 1) situated in the present; 2) sensitive to context and perspective and 3) guided (but not governed) by rules and routines. The phenomenological experience of mindfulness is the felt experience of engagement. […] Mindlessness, by contrast, is defined as an inactive state of mind characterized by reliance on distinctions and categories drawn in the past. Here 1) the past over-determines the present; 2) we are trapped in a single perspective but oblivious to that entrapment; 3) we are insensitive to context; and 4) rules and routines govern rather than guide our behavior. Moreover, mindlessness typically comes about by default rather than by design. […] Mindfulness allows for doubt and that allows for choice and thus free will. Being in a mindful state, removed from rigid routines, introduces possibilities from which one can make alternative choices and thus exercise their free will. When mindless, by contrast, our behavior is predetermined by the past, closing us off to choice and new possibilities. […] Science becomes mindless when we automatically begin to conflate precision with certainty. Certainties lead to mindlessness; when we think we know, there is no reason to find out. Too often scientists observe a phenomenon, create a theory to explain it and then collect data to prove their theory. Not surprisingly, confirmation is found. […] This illusory sense of knowing is pervasive, extending even to the point where we misconstrue the nature of our own mental processes. What are we actually doing when we hold a certain concept in our mind’s eye? Picture a car, for example. Now, start taking away individual elements that seem essential to the “car-ness” of it all, and ask yourself if you’d still know it’s a car. A car without wheels? Still a car. […] Wittgenstein (1953) famously performed a similar dissection of conceptual categories, effectively demonstrating (in his case, with the concept of “game”) the inherent illusion that our mental categories for things are actually based upon some identifiable set of core features. So what is it that makes a car a car? Not much, as it turns out. Recent findings in the field of cognitive neuropsychology have begun to indicate that this assertion – that conceptual categories lack inherent unifying features – is backed by more than just sound logic. Barsalou (2009) and Wilson-Mendelhall et al. (2010) have established that the brain doesn’t actually use a set of core concepts to define mental categories of objects and phenomena – rather, our thought processes remain in a perpetual state of collection, assessment, and reaction to incoming information. It is only at the point of higher-level cognitive processes that we begin to grow lazy and assume that all examples of cars have some inherent “car-ness” about them (Or, for that matter, that all instances of fear, or anger, or pride, must necessarily be connected by some unifying element). In reality, the idea of “car” (or “fear”, or any other concept) is actually represented in our brains as a loose amalgam of instances (this morning on the way to work in traffic, on a showroom floor, in a junkyard), specific examples (a smartcar, a station wagon, a jeep), functions (creating momentum, providing shelter, controlling climate), and other characteristics of certain objects that we learn at some point to clump together. In short, there’s no core element that makes a car a car every time, all the time. Mindfulness requires that we engage the world with this same degree of dynamism and flexibility. […] More important to the present discussion is recent work that follows up on research originally conducted in 1981. The idea was and is deceptively simple. Mind and body are just words, concepts to which we rigidly adhere. Consider artificial boundaries like North vs South Korea or old (>65) vs. young (<65), where the concepts may have been mindfully generated initially but then took on a life of their own. What would happen, we asked, if we got rid of the distinction between mind and body? If we put the mind and body back together so to speak, then wherever the mind is, so too would be the body. Within this understanding there is no reason to search for mediating mechanisms. Whatever is going on at the level of the brain is happening simultaneously with the thought and is just another level of analysis.

Rinpoche, The Tibetan book of living and dying (1992).
>If everything is impermanent, then everything is “empty”, that is to say without intrinsic, stable or lasting existence. And all things, understood in their true relationship, are seen not as independent but as interdependent. The Buddha compared the universe to a huge net woven from an infinite variety of sparkling diamonds. Each diamond, with countless facets, reflects all the other diamonds in the net and is one with each of them. If you look closely, nothing has an intrinsic existence. It is this lack of independent existence that we call “emptiness”. […] Buddhism offers a vision that is still revolutionary to this day, that life and death exist in the mind and nowhere else. The spirit is revealed as the universal basis of experience. […] Among the many aspects of the mind, there are two in particular. The first is the ordinary spirit, which the Tibetans call sem. A master defines it this way: “that which is endowed with a discriminating consciousness, that which possesses a sense of duality – which grasps or rejects what is external to it: that is the spirit”. Basically, it is what we associate with the ‘other’ – an ‘object’ perceived as different from whom perceives it. Sem is the discursive, dualistic mind, the “thinking” mind, which can only function in relation to an external reference point projected by it and falsely perceived. […] It is the spirit which must tirelessly justify itself, consolidate its “existence” and prove its validity by fragmenting experience, conceptualizing and solidifying it. Inconsistent and futile, the ordinary spirit is the constant prey of external influences, habits and conditioning: the masters compare it to the flame of a candle in the doorway, vulnerable to all winds of circumstances. […] The second aspect is the very nature of the mind, its deepest essence, which is absolutely never affected by change or by death. For the moment, it remains hidden inside our own mind – our sem –, enveloped and obscured by the disordered mental agitation of our thoughts and emotions. […] The very nature of the mind is the very source of all understanding. In Tibetan, we call it Rigpa, primordial clear consciousness, pure, original, at the same time intelligence, discernment, radiance and constant awakening. You could say that it is the knowledge of knowledge itself. Make no mistake, the nature of the mind is not limited to our mind alone. It is, in fact, the nature of everything. We cannot repeat it enough: to understand the nature of the mind is to understand the nature of everything. Throughout history, saints and mystics have adorned their accomplishments with various names and given them various faces and interpretations; but basically their experience is that of the essential nature of the mind. Christians and Jews call him “God”, Hindus “the Self”, “Shiva”, “Brahman”, “Vishnu”; Sufi mystics call it “the Hidden Essence” and Buddhists call it “The Nature of Buddha”. At the heart of all religions is the certainty that there is a fundamental truth, and that it offers a sacred opportunity to evolve and realize it. […] What is the origin of consciousness? It cannot arise from nothing. A moment of consciousness cannot happen without the moment of consciousness that immediately preceded it. […] The Buddhist acceptance of the concept of rebirth is based mainly on the notion of continuity of consciousness. Take, for example, the physical world: we consider that we can go back to the origin of all the elements of our current universe – and even to a microscopic level – to an initial point where all the elements of the material world are condensed into what are known in technical terms as “space particles”. These particles are, in turn, the state resulting from the disintegration of a previous universe. There is therefore a constant cycle in which the universe evolves, disintegrates and returns to existence. Our mind works analogously. It is quite obvious that we have what we call “a mind” or “a consciousness”: our experience can testify. It is also obvious, always from our experience, that what we call “mind” or “consciousness” is subject to change, when it is exposed to different conditions and circumstances. […] It is also evident that at the grossest level the “mind”, or “consciousness”, is intimately linked to the physiological states of the body; in fact, it depends on them. But there must be a certain basis, an energy, a source which allows the mind, in its interaction with material particles, to produce conscious living beings. Just like in material terms, this basis is also, without a doubt, in continuity with the past. So if you go back to the origin of our present spirit, of our present consciousness, you will realize that, just as for the origin of the material universe, you go back to the origin of the continuity of the spirit up to an infinite dimension; as you can see, the continuity of the spirit is without origin. Therefore, there must be successive rebirths to make this continuum of the mind possible. Buddhism believes in universal causation: everything is subject to change, to causes and to conditions. It therefore gives no place to a divine creator, nor to a “spontaneous generation” of beings; on the contrary, everything manifests itself as a consequence of causes and conditions. Thus, the present state of mind, or consciousness, results from its previous moments. The causes and conditions we are talking about are mainly of two types: the substantial “causes” that are behind what is happening, and the various “factors” that help produce the causal situation. In the case of mind and body, although one can affect the other, one cannot become the substance of the other … Although mind and matter depend on one another, one cannot be the substantive cause of the other. It is on this basis that Buddhism accepts the notion of rebirth. Most people consider that the word “reincarnation” implies “something” which is reincarnated, which travels from life to life. But we do not believe, in Buddhism, in an independent and immutable entity such as the soul or the ego, which would survive the death of the body. What ensures continuity between lives is not, in our view, an entity, but consciousness at its ultimate level of subtlety. […] The successive lives of a series of rebirths are not like the pearls of a necklace, held together by a thread, “the soul”, which would pass through the pearls; rather, they look like dice stacked on top of each other. Each dice is separate, but it supports the one that is placed on it and with which it has a functional link. The dice are not linked by identity, but by conditionality. […] The truth and the driving force behind rebirth is what is called karma. The concept of karma is often very poorly understood in the West: it is confused with fate or with predestination. A better approach is to think of it as the inevitable law of cause and effect that governs the universe. The term karma literally means “action”; karma is both the latent power contained in actions and the result of these actions.

In the first chapter, we saw the views of Empedocles in the words of Heisenberg: “Empedocles describes the formation of the world in the following picture: First, there is the infinite Sphere of the One, as in the philosophy of Parmenides. But in the primary substance all the four “roots” are mixed together by Love. Then, when Love is passing out and Strife coming in, the elements are partially separated and partially combined. After that the elements are completely separated and Love is outside the World. Finally, Love is bringing the elements together again and Strife is passing out, so that we return to the original Sphere”.
This concept of love as a universal field is absolutely not backed by any scientific research but I find it interesting that it comes recurring through time, even in modern philosophy without it being tainted by New Age ideas. The view of Empedocles refers to elements of reality being connected together. The following authors refer more to a feeling of interpersonal love – understand compassion, benevolence, kindness, tolerance, altruism, charity, etc. – which could be called universal, in the sense that the following philosophers wish for everyone the same happiness that one wishes for oneself. It is very often linked to the removal of the ego. Thus, in my readings, here is what I encountered that was related to love.

Kamalasila, Stages of meditation (IXth CE).
>In this cycle of existence without beginning, there is not a single being who has not been my friend or relative hundreds of times. Therefore, since attaching to some beings by hating others doesn’t rest upon any foundation, I will develop the spirit of equanimity with regard to all beings.
	
Jacobi, Open letter to Fichte (1799).
>It is seriously, ardently, that, like few, I have, since my childhood, struggled to conquer the truth; like few, I experienced my disability and my heart became very charitable, oh! very charitable, my dear Fichte, and my voice became very soft. The same pity that, as a man, I have for myself, I have for others. 

Spinoza, explained by Misrahi in commentaries to Spinoza’s Ethics (2005).
>Spinozist ethics is the call to a transmutation of the emotional life by reason, but not a call to a sacrifice of this life. The value, at the same time fundamental and ultimate, is the enjoyment of being: joy of existing, joy of relating to others in positivity, enjoyment of the goods of existence to the extent that they allow simultaneous affirmation of the increase existential of others and of oneself. For the perfection which the free and wise man pursues for himself, he also desires for others. Because “it is part of [his] happiness” that others too can access the happiness he desires.
	
Mach, The analysis of sensations and the relation of the physical to the psychical (1902).
>The ego must be given up. It is partly the perception of this fact, partly the fear of it, that has given rise to the many extravagances of pessimism and optimism, and to numerous religious, ascetic, and philosophical absurdities. In the long run we shall not be able to close our eyes to this simple truth, which is the immediate outcome of psychological analysis. We shall then no longer place so high a value upon the ego, which even during the individual life greatly changes, and which, in sleep or during absorption in some idea, just in our very happiest moments, may be partially or wholly absent. We shall then be willing to renounce individual immortality, and not place more value upon the subsidiary elements than upon the principal ones. In this way we shall arrive at a freer and more enlightened view of life, which will preclude the disregard of other egos and the overestimation of our own.

Nietzsche
>[Thus spoke Zarathustra (1883)] Peoples once hung over them tables of the good. Love which would rule and love which would obey, created for themselves such tables. Older is the pleasure in the herd than the pleasure in the ego: and as long as the good conscience is for the herd, the bad conscience only said: ego. Verily, the crafty ego, the loveless one, that seek its advantage in the advantage of many it is not the origin of the herd, but its ruin.
>
>[The will to power (1901)] “One furthers one’s ego always at the expense of others”; “Life always lives at the expense of other life” – he who does not grasp this has not taken even the first step toward honesty with himself. The “subject” is only a fiction: the ego of which one speaks when one censures egoism does not exist at all. The “ego” – which is not one with the central government of our nature! – is, indeed, only a conceptual synthesis – thus there are no actions prompted by “egoism”.

From a very early age, McTaggart had what he took to be mystical experiences. These experiences presented the world as being fundamentally unified by the relation of love. Reality as it appeared to him in these experiences consisted fundamentally of immaterial spirits who stand in the relation of love to one another. These experiences provided him with great comfort, but he believed that the fact that he had them did not provide others with a reason to believe in the unity he took them to reveal. Philosophical argument was needed to supply others with a reason.
	
McTaggart, The nature of existence (1900).
>This seems to me to be the essence of love. Love is an emotion which springs from a sense of union with another self. The sense of union is essential – without it there is no love. And it is sufficient – whenever there is a sense of a sufficiently close union, then there is love. […] I believe that in absolute reality every self will love every other self whom he directly perceives. We came to the conclusion that no condition was necessary for love except that the love should be conscious of his unity with the beloved. […] But in present experience the consciousness of unity is not always strong enough to be love, since we do not love all the people we know. […] In absolute reality all the life of every self is, or is dependent on, love. The self has no parts except his perceptions himself, of other selves, and of parts of selves. All perceptions of other selves are states of love. […] In absolute reality, then, love is supreme, not only in value – for that we have not to wait for absolute reality – but supreme in power. Nothing is alien to love, everything is dependent on it. […] I shall love all the other selves which I directly perceive. And acquiescence is a necessary consequence of love. I may not get happiness from my beloved, or from my love of him. I may not approve of him morally. I may desire that many of his qualities should have been otherwise. But there is one thing I must desire if I love him. I desire his existence. I want him to be there. […] Even if he should love all the other selves in the universe, his love would be greater if there were other selves, whom he also loved. Nor is the intensity of love capable of a maximum. There is always an intensity of love greater than any given intensity. […] Love can be greater in a later life because it was there in an earlier life, and the fact that two people love one another in this life is a reason for holding that they will love one another in various future lives. The value of love, then, does not cease with the cessation of its memory.

As a conclusion, here are the words of Hawking in the twilight of his life, expressing a poetic ontological thought.

Hawking, Brief answers to the big questions (2018).
>It would be an empty universe indeed if it were not for the people I love, and who love me. Without them, the wonder of it all would be lost on me.

