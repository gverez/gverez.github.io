---
title: Brian Greene
---
[<img src="/images/accueil.png">](/)
[<img src="/images/ancientestament.png">](/pages/ancientestament.html)
[<img src="/images/deuterocanoniques.png">](/pages/deuterocanoniques.html)
[<img src="/images/nouveautestament.png">](/pages/nouveautestament.html)

# Brian Greene

[<img src="/images/briangreene.png">](https://fr.wikipedia.org/wiki/Brian_Greene)

**1963-**, physicien américain.


## L’univers élégant <a name="arkhe"></a>
*Robert Laffont, 2000*

### Pris dans les cordes [p. 34]

>C’est une première dans l’histoire de la physique : un formalisme unique, capable d’expliquer chacune des caractéristiques fondamentales de la nature ! De fait, la théorie des cordes est parfois qualifiée de « théorie du tout », de « théorie ultime », ou même de « théorie finale ». On entend par là que cette théorie serait la plus profonde des théories physiques envisageables, contenant toutes les autres, sans s’appuyer sur aucune. En réalité, nombre de théoriciens des cordes en ont une approche plus prosaïque et considèrent que la « théorie du tout » est, plus simplement, une théorie qui explique toutes les propriétés des particules élémentaires et de leurs interactions. Un réductionniste pur et dur vous dirait que cela ne constitue pas une limitation, que la compréhension des processus microscopiques élémentaires suffit en principe à expliquer tout le reste, du big-bang jusqu’à nos rêves. Si, vous comprenez tout des ingrédients, prétend le réductionniste, alors vous comprenez tout.
>
>La philosophie réductionniste provoque des débats enflammés. Beaucoup trouvent révoltant, ou tout bonnement niais, de prétendre que les merveilles de la vie et de l’Univers ne sont que le fruit de cette valse vaine qui anime de petites particules microscopiques, rythmées par les seules lois de la physique. La joie, le désespoir ou l’ennui ne sont-ils vraiment que le résultat de réactions chimiques à l’intérieur de notre cerveau – des réactions entre molécules et atomes, qui ne résultent en fait que des réactions, à plus petite échelle, entre les particules, elles-mêmes n’étant que de petites cordes vibrantes ? Face à ce type de critique, les propos du prix Nobel Steven Weinberg, dans Le Rêve d’une théorie ultime, restent prudents :
>
>>« De l’autre côté du spectre se trouvent les opposants au réductionnisme. Ce qu’ils interprètent comme la rigueur aride des sciences modernes leur fait horreur. Que leur univers ou eux-mêmes puissent être réduits, dans quelque mesure que ce soit, à une histoire de particules, de champs et de leurs interactions, et ils se sentent diminués… Je ne tenterai pas de répondre à de telles critiques par un exposé passionné sur les beautés des sciences contemporaines. Certes, le point de vue réductionniste fait froid dans le dos. Mais nous l’avons accepté tel qu’il est, non parce qu’il nous plaît, mais parce que c’est ainsi que fonctionne le monde. »
>
>Certains conviennent de cette vue austère et crue, mais pas tous.
>
>D’autres ont bien tenté, en faisant appel par exemple à la théorie du chaos, d’argumenter que de nouvelles formes de lois s’imposent dès qu’augmente le niveau de complexité d’un système. C’est une chose que de comprendre le comportement des électrons ou de quarks ; c’en est une tout autre que d’appliquer ce savoir à la description d’une tornade, par exemple. Presque personne ne le conteste. Mais les opinions divergent dès qu’il s’agit de la diversité, et du caractère parfois inattendu, des phénomènes qui peuvent émerger de systèmes plus complexes qu’une particule isolée. Sont-ils le fruit de principes nouveaux ? Ou s’agit-il plutôt de phénomènes dérivés, reposant de façon terriblement compliquée sur les lois physiques qui régissent les très nombreux constituants élémentaires ? Mon sentiment est qu’il ne s’agit pas de nouvelles lois physiques, indépendantes. Bien sûr, il serait fort difficile de décrire une tornade avec les lois de la physique des particules élémentaires, mais je ne vois là qu’une question liée à l’insuffisance de nos moyens de calcul, et non le signe d’un besoin de nouvelles lois. Toutefois, je le répète, tout le monde ne partage pas cet avis.
>
>Un point, essentiel au voyage entrepris dans ce livre, ne fait pas l’ombre d’un doute : même si l’on adopte le point de vue du réductionniste pur et dur, la théorie et la pratique sont deux choses différentes. On s’accorde en général sur le fait que la découverte de la « théorie du tout » ne résoudrait (ni ne simplifierait) en rien la psychologie ou la biologie, ni la géologie ni la chimie, ni même la physique. L’Univers est si fabuleusement divers et complexe que percer à jour la théorie finale, telle qu’elle est décrite ici, n’annoncerait en rien la fin des sciences. Bien au contraire. Cette théorie, ultime description de l’Univers dans ses moindres détails les plus microscopiques serait indépendante de toute explication plus profonde. Elle fournirait les plus solides fondations pour bâtir notre compréhension de l’Univers. Sa découverte marquerait un commencement, pas une fin. La théorie ultime apporterait une base ferme et cohérente qui nous assurerait que notre Univers et un lieu intelligible.







### Relativité générale et mécanique quantique : vers une nouvelle théorie [p. 142]

>Les relations d’incertitude de Heisenberg stipulent que de tels échanges frénétiques d’énergie et d’impulsion ont lieu continuellement, sur des distances ou des durées microscopiques. Même dans une région vide de l’espace – une boite vide, par exemple –, le principe de Heisenberg énonce qu’énergie et impulsion sont « incertaines ». Elles fluctuent entre des valeurs extrêmes, d’autant plus éloignées que la taille de la boite ou l’échelle de temps sont plus petites. C’est un peu comme si l’intérieur de la boite était une zone « emprunteuse », extorquant continuellement des « prêts » à l’Univers pour ensuite les lui « rembourser ». Mais quels sont les protagonistes de ces échanges dans une région vide de l’espace ? Tout. Littéralement tout. L’énergie (ainsi que l’impulsion) est l’ultime monnaie d’échange. E=mc² : l’énergie peut devenir matière et inversement. Ainsi, une fluctuation d’énergie suffisamment importante pourra, par exemple, donner naissance à un électron et à son compagnon antimatériel, le positron, même si l’espace était initialement vide ! Puisque cette énergie doit être rapidement remboursée, les deux particules vont s’annihiler mutuellement après un instant, remboursant ainsi l’énergie empruntée pour leur création. Il en va de même pour toutes les transactions d’énergie et d’impulsion – création puis annihilation de particules, folles oscillations du champ électromagnétique, fluctuations des champs de forces faible et forte. Avec l’incertitude quantique, l’Univers à l’échelle microscopique devient une arène grouillante et chaotique. Feynman aimait ironiser : « Créer, annihiler ; créer, annihiler… quelle perte de temps ! » En moyenne, l’emprunt et le remboursement s’équilibrent, de sorte qu’une région vide a toutes les apparences de la tranquillité tant qu’on ne l’observe pas avec une précision microscopique. Le principe d’incertitude révèle pourtant que la moyenne macroscopique estompe la richesse d’activité du monde microscopique.
>
>Si l’idée qu’il puisse se passer quoi que ce soit dans une région vide vous laisse perplexe, il est important de réaliser que le principe d’incertitude pose une limite à la vacuité, qui n’a plus la même signification. Par exemple, le principe d’incertitude appliqué à la distorsion des ondes en présence d’un champ (telles que des ondes électromagnétiques qui se propagent dans une région où règne un champ électromagnétique) montre que l’amplitude de l’onde est liée à la vitesse de variation de l’amplitude par la même relation que celle qui unit position et vitesse d’une particule : plus l’amplitude est connue avec précision, moins on aura d’information sur ses variations. Dire qu’une région de l’espace est une région vide signifie, entre autres, qu’il ne s’y propage aucune onde, que tous les champs y ont une valeur nulle. Plus simplement, l’amplitude des ondes qui traversent cette région vide est exactement égale à zéro. Or, l’amplitude étant connue de manière exacte, le principe d’incertitude indique qu’elle varie de façon aléatoire : elle peut prendre une tout autre valeur d’un instant à l’autre, de sorte qu’à l’instant suivant l’amplitude n’est plus nulle, mais la portion d’espace considérée demeure une région « vide ». Ce n’est qu’en moyenne que le champ sera nul puisqu’il prend, ici une valeur positive, là une valeur négative. En moyenne, l’énergie de cette région n’est pas modifiée. Mais cela ne vaut qu’en moyenne. Du fait de l’incertitude quantique, l’énergie du champ – même dans une région vide – fluctue entre des valeurs d’autant plus éloignées que les échelles de distance et de temps d’observation sont plus courtes. L’énergie associée à ces fluctuations brèves du champ pourra être matérialisée, via E=mc², en une particule et son antiparticule, lesquelles s’annihileront en toute hâte pour que l’énergie, en moyenne, n’en soit pas modifiée. […]
>
>Les efforts pour concilier relativité restreinte et théorie quantique se sont d’abord concentrés sur la force électromagnétique et ses interactions avec la matière. Ainsi, une succession d’avancées astucieuses a donné naissance à l’électrodynamique quantique, la plus simple des théories quantiques des champs (relativistes). Théorie quantique, car aspects probabilistes et incertitude font partie de ses ingrédients de base. Théorie des champs, car elle relie les principes quantiques à la notion classique du champ de forces, ici, le champ électromagnétique de Maxwell. Enfin, il s’agit bien d’une théorie relativiste puisque la relativité restreinte en est aussi un ingrédient de départ. (Pour nous représenter un champ quantique en partant de l’image du champ classique – un océan de lignes de champ invisibles, qui imprègnent tout l’espace –, il nous faut compléter cette image par deux aspects. Il faut d’abord imaginer qu’un champ quantique est constitué d’éléments corpusculaires, comme les photons pour le champ électromagnétique : puis que l’énergie prend la forme du mouvement et de la masse des particules. Elle passe sans cesse d’un champ quantique à l’autre, tandis que les champs vibrent, dans l’espace et dans le temps.) […]
>
>Sheldon Glashow, Abdus Salam et Steven Weinberg ont reçu le prix Nobel pour avoir unifié les interactions faible et électromagnétiques. Leurs travaux démontrent que ces théories quantiques des champs s’unissent naturellement, bien qu’elles semblent très différentes. Les champs de la force nucléaire faible s’atténuent très vite pour s’annuler au-delà des échelles subatomiques, tandis que les champs électromagnétiques – lumière visible, rayons X, signaux radio ou de télévision – se manifestent à l’échelle macroscopique. Glashow, Salam et Weinberg ont pourtant établi qu’à des températures et à des énergies suffisamment élevées – par exemple lors des quelques secondes qui ont suivi le big-bang – les champs « fusionnent ». Rien ne les distingue plus et on les appelle, plus justement, des champs électrofaibles. Dès que la température baisse, ce qu’elle fait depuis le big-bang, nos deux théories « cristallisent » en se différenciant. Dans notre Univers froid, par brisure de symétrie – procédé que l’on décrira plus loin –, les forces électrodynamique et faible ont des formes différentes et se distinguent l’une de l’autre. […]
>
>Les bosons de jauge – photons, gluons boson faibles – fournissent une description microscopique des interactions qu’ils véhiculent. Prenons l’exemple de deux particules de même charge. Ces deux particules se repoussent, ce que l’on peut expliquer grossièrement en faisant appel au champ électrique qui accompagne chacune d’elles, comme le halo de leur quintessence électrique. Elles se repoussent car leurs champs de force se repoussent. En fait, l’explication microscopique est assez différente. Le champ électromagnétique est semblable à une nuée de photons dont les particules chargées se bombardent mutuellement. Imaginons deux patineurs sur glace qui se lancent des boules de bowling et s’empêchent d’avancer. C’est une analogie approximative de la façon dont deux particules chargées s’influencent par l’échange de petits « paquets » de lumière.
>
>Un aspect essentiel fait pourtant défaut à l’image des patineurs puisque, pour eux, l’échange de boules de bowling est toujours « répulsif » (il les sépare). Mais l’échange de photons entre particules de charges opposées résulte d’une force attractive. C’est un peu comme si le photon transmettait moins la force elle-même qu’un message indiquant au destinataire le comportement à adopter en présence de cette force. Entre particules de même charge, le photon délivre le message « séparez-vous ». Entre particules de charge opposées, il transmet « rejoignez-vous ». Pour cette raison, on dit parfois que le photon est la particule intermédiaire de la force électromagnétique. Il en va de même pour les gluons et les bosons fiables, particules intermédiaires des interactions forte et faible. La force nucléaire forte confine les quarks à l’intérieur des protons et des neutrons, par l’échange de gluons entre les quarks. Les gluons sont comme la « glu » qui assure la cohésion des particules subatomiques. La force nucléaire fiable est responsable de la désintégration des particules dans certaines formes de radioactivité ; ses médiateurs sont les bosons faibles. […]
>
>On dit d’une sphère qu’elle possède la symétrie de révolution car on a beau la tourner dans tous les sens, ou l’observer sous n’importe quel angle, elle garde le même aspect. De même, on dira de l’Univers qu’il possède la symétrie de l’interaction forte : ces décalages des charges sont sans effet sur la physique. Pour des raisons historiques, les chercheurs appellent cette symétrie de l’interaction forte une symétrie de jauge.
>
>Voici l’essentiel : tout comme l’indépendance des points de vue en relativité générale requiert l’existence de la force gravitationnelle, des recherches fondées sur les travaux de Hermann Weyl (années vingt) puis de Chen-Ning Yang et Robert Mills (années cinquante) ont montré que tout symétrie de jauge requiert l’existence d’une certaine force. Imaginez que la température, l’humidité et la pression de l’air dans une pièce soient maintenues constantes par un système qui compense parfaitement la moindre influence du milieu extérieur. Certains champs de force, comme l’ont montré Yang et Mills, jouent un rôle analogue, puisqu’ils compensent parfaitement le moindre décalage des charges des particules, de sorte que leurs interactions physiques n’en sont pas modifiées. La force correspondant à la symétrie de jauge des couleurs des quarks n’est autre que la force nucléaire forte. Sans elle, cette symétrie de jauge ne serait pas une symétrie de la nature : la physique serait différente après décalage des couleurs.
>
>La gravitation et la force nucléaire forte ont des propriétés complètement différentes (souvenons-nous, par exemple, que la gravitation est beaucoup moins intense que la force forte, et qu’elle agit sur des distances bien plus grandes). Elles ont pourtant un héritage commun puisque toutes deux sont nécessaires à la réalisation de certaines symétries de l’Univers. Il en est de même des forces faible et électromagnétique, qui sont associées à d’autres symétries de jauge – les symétries de jauge faible et électromagnétique. Les quatre interactions sont donc directement liées à des principes de symétrie. […]
>
>Le domaine d’application de la relativité générale est celui des grandes distances, les distances astronomiques. A de telles échelles, la théorie d’Einstein prévoit qu’un espace vide (de masses) est un espace plat. Si l’on cherche à marier relativité générale et mécanique quantique, il faut opérer un changement d’échelle radical et scruter les propriétés microscopiques de l’espace. […]
>
>Absolument tout est sujet aux fluctuations inhérentes au principe d’incertitude. Même le champ de gravitation. S’il est nul classiquement, il ne l’est, du point de vue quantique, qu’en moyenne ; sa valeur fluctue. En outre, toujours en vertu du principe d’incertitude, on s’attend à ce que la valeur du champ oscille d’autant plus qu’on l’observe à plus petite échelle. La mécanique quantique nous a enseigné les aléas du confinement : si l’on réduit davantage l’échelle, les ondulations seront encore plus importantes.
>
>C’est la courbure de l’espace qui révèle la présence du champ de gravitation. Ses fluctuations conduisent donc à des distorsions de plus en plus violentes de l’espace. […] Réduisons encore l’échelle, pour atteindre le dernier niveau : les fluctuations quantiques du champ gravitationnel produisent des distorsions terribles. A tel point que l’espace n’a plus rien d’un objet à courbure douce comme la membrane plate du chapitre 3. Il prend plutôt la forme turbulente et biscornue qu’illustre le dernier niveau de la figure. L’examen ultra-microscopique de l’espace (et du temps) révèle une terrible facette de l’Univers : les notions conventionnelles de droite et gauche, devant et derrière, haut et bas (et même avant et après) perdent leur sens. John Wheeler a inventé le terme de mousse quantique pour décrire cette écumeuse effervescence. Ici se dévoile l’incompatibilité fondamentale entre la mécanique quantique et la relativité générale : aux échelles microscopiques, la violence des fluctuations du monde quantique invalide l’hypothèse centrale de la relativité générale puisque l’espace n’est plus lisse. La clef de voûte de la théorie quantique – les relations d’incertitude – entre ici en conflit direct avec celle de la relativité générale – un espace(-temps) géométriquement lisse. […]
>
>Les principes fondateurs de la relativité générale et de la mécanique quantique permettent d’évaluer l’échelle à partir de laquelle ces phénomènes pernicieux deviendraient perceptibles. […] Cette échelle, appelée longueur de Planck, est le résultat de l’union de la constante de la gravitation (qui mesure l’intensité de la force gravitationnelle) et de la constante de Planck (mesurant l’intensité des effets quantiques). La très faible valeur de ces deux constantes conduit, pour la longueur de Planck, à un résultat qui dépasse l’entendement : 10-33 centimètres ! Pour saisir ce que cela veut dire, imaginez que les atomes aient la taille de l’Univers connu ; la longueur de Planck correspondrait alors à la hauteur d’un arbre ordinaire !
>
>Vous comprenez maintenant que l’incompatibilité entre relativité générale et mécanique quantique n’intervient que pour un domaine quasi ésotérique de l’espace-temps. Cela vaut-il la peine que l’on s’en préoccupe ?



### En avant la musique : les bases de la théorie des cordes [p. 158]

>Selon la théorie des cordes, les ingrédients élémentaires de la nature ne sont pas des particules ponctuelles. Il s’agit plutôt de minuscules filaments unidimensionnels, un peu comme de petites élastiques de caoutchouc, extrêmement fins, qui vibrent inlassablement. Toutefois, ne vous laissez pas berner par leur nom : contrairement à un bout de corde ordinaire, lui-même composé de molécules et d’atomes, les cordes de la théorie des cordes seraient les fondements mêmes de la matière, les constituants ultramicroscopiques des particules qui composent les atomes eux-mêmes. Les cordes de la théorie des cordes sont si infinitésimales – en moyenne, leur longueur et à peu près égale à la longueur de Planck – qu’elles semblent ponctuelles, même lorsqu’on les examine avec nos instruments les plus puissants. […]
>
>En 1988, le physicien David Gross et son étudiant Paul Mende ont montré qu’en tenant compte de la théorie quantique, si l’on augment continuellement l’énergie d’une corde, on n’augmente pas ses capacités à sonder des structures plus fines (ce qui est en opposition directe avec ce que l’on observe pour les particules ponctuelles). Ils ont établi que lorsqu’on augmente l’énergie d’une corde, en premier lieu, elle est d’abord capable de sonder des structures à plus petite échelle, de même qu’une particule ponctuelle énergétique. Mais, dès que son énergie dépasse celle requise pour analyser des structures à l’échelle de Planck, le surplus d’énergie cesse d’aiguiser ses capacités d’analyse. Au contraire, cette énergie fait que la corde s’allonge, et diminue donc sa sensibilité à petite échelle. En fait, bien que la taille typique d’une corde soit la longueur de Planck, si on pouvait lui fournir suffisamment d’énergie – une quantité d’énergie dépassant nos rêves les plus fous, de l’ordre des énergies du big-bang –, nous pourrions l’agrandir jusqu’à une taille macroscopique, ce qui en fait une bien piètre sonde pour le microcosmos ! C’est un peu comme si les cordes, contrairement aux particules ponctuelles, avaient deux sources de « flou » : l’agitation quantique, comme pour une particule ponctuelle, et leur propre extension spatiale. Augmenter l’énergie d’une corde atténue les effets de la première source mais amplifie finalement les effets de la seconde. Quoi qu’on fasse, le caractère étendu de la corde nous empêche de l’utiliser pour sonder des phénomènes en deçà de la longueur de Planck.
>
>Or tout le conflit entre la relativité générale et la théorie quantique découle des propriétés de la structure spatiale aux distances inférieures à la longueur de Planck. Si les constituants élémentaires de l’Univers eux-mêmes ne permettent pas de sonder des distance inférieures à la longueur de Planck, alors, ni eux ni rien d’autre ne sera jamais affecté par les prétendues désastreuses ondulations quantiques qui règnent à cette échelle. Cela ressemble à ce qui se passe lorsque l’on caresse des mains un plan de granit très finement poli. Bien qu’à l’échelle microscopique le granit soit granuleux et irrégulier, nos doigts sont incapables de distinguer ces variations et sa surface nous paraît parfaitement lisse. Nos gros doigts patauds « estompent » le caractère microscopique du granit. De même, puisqu’elle a une extension spatiale, la corde a également une sensibilité limitée pour les courtes distances. Elle ne peut pas détecter de variations de taille inférieures à la longueur de Planck. Comme nos doigts sur le granit, les cordes « estompent » les fluctuations ultramicroscopiques frénétiques du champ gravitationnel. Même si les fluctuations en question sont bel et bien là, ce flou les atténue juste assez pour remédier à l’incompatibilité entre la relativité générale et la mécanique quantique. En particulier, la théorie des cordes balaie les infinis pernicieux (dont nous avons parlé au chapitre précédent) qui surgissent dès que l’on tente de fonder une théorie quantique de la gravitation en termes de particules ponctuelles. […]
>
>Dans un Univers régi par les lois de la théorie des cordes, la notion conventionnelle selon laquelle nous pouvons disséquer la nature jusqu’à des échelles toujours plus petites et sans limites n’est plus valable. Il existe une limite, et l’on bute sur elle avant que l’effervescence quantique ne fasse sentir ses effets dévastateurs. Finalement, dans une certaine mesure (que nous préciserons dans les derniers chapitres), on pourrait même faire comme si les tumultueuses fluctuations quantiques subplanckiennes n’existaient pas. Un positiviste vous dirait que rien n’existe s’il ne peut – au moins en principe – être observé et mesuré. Puisque l’on suppose que la corde est l’objet le plus élémentaire de l’Univers, et puisqu’elle est trop grande pour subir les violentes ondulations de la structure spatiale, eh bien, ces fluctuations ne peuvent être mesurées et, dans le cadre de la théorie des cordes, ne se manifestent tout simplement pas. […]
>
>Premièrement, l’argument précédent laisse penser que les fluctuations problématiques sont un artefact de la formulation de la relativité générale et de la théorie quantique en termes de particules ponctuelles. Ainsi, en un sens, nous aurions créé de toutes pièces le conflit centrale de la physique théorique contemporaine. Parce que nous avions envisagé en premier lieu toutes les particules matérielles et toutes les particules d’interaction comme des objets ponctuels sans dimensions spatiales, nous étions contraints d’étudier les propriétés de l’Univers jusqu’à des distances arbitrairement courtes. Et, aux distances le plus infimes, nous nous sommes heurtés à des problèmes apparemment insurmontables. La théorie des cordes nous enseigne que nous avons buté sur ces problèmes uniquement parce que nous n’avions pas compris les vraies règles du jeu ; les nouvelles règles posent qu’il existe une limite à la finesse avec laquelle nous pouvons sonder l’Univers – et une limite réelle à la finesse avec laquelle notre notion conventionnelle de distance peut s’appliquer à la structure ultramicroscopique du cosmos. Il apparaît maintenant que ces fluctuations spatiales pernicieuses ont émergé de nos théories car nous n’avions pas conscience de ces limites et que notre interprétation en termes de particules ponctuelles nous conduisait à franchir outrageusement les bornes de la réalité physique.


### Le « super » de la supercorde [p. 192]

>Les physiciens disent que ces deux propriétés des lois physiques – le fait qu’elles ne dépendent ni de l’endroit ni de l’instant où vous les utilisez – sont des symétries de la nature. Ce qu’ils veulent dire, c’est que la nature traite chaque instant et chaque endroit de la même façon – symétriquement – en faisant en sorte que les mêmes lois fondamentales y soient en vigueur. Comme en art et en musique, ces symétries sont fondamentalement satisfaisantes : elles soulignent l’ordre et la cohérence des mécanismes de la nature. L’élégance de la richesse, de la complexité et de la diversité des phénomènes qui émergent d’un simple ensemble de lois universelles est au moins en partie ce à quoi pensent les physiciens quand ils parlent de « beauté ».
>
>Dans notre exposé des théories de la relativité restreinte et générale, nous avons rencontré d’autres symétries de la nature. Souvenez-vous que le principe de relativité, qui se trouve au cœur de la relativité restreinte, stipule que les lois physiques sont les mêmes quel que soit le mouvement relatif, à vitesse constante, d’observateurs individuels. C’est une symétrie parce que cela implique que la nature traite tous ces observateurs de la même façon, symétriquement. […]
>
>Existe-t-il d’autres principes de symétrie concernant l’espace, le temps et le mouvement que les lois de la nature doivent respecter ? En y réfléchissant, on pourrait envisager une autre possibilité. Les lois de la physique doivent être indépendantes de l’angle sous lequel vous faites vos observations. Par exemple, si vous faites une expérience puis décidez de retourner tout l’appareillage et de recommencer, alors les mêmes lois devraient s’appliquer. On appelle cela la symétrie de rotation, et elle signifie que les lois de la physique traitent de manière identique toutes les orientations possibles. Ce principe de symétrie va de paire avec les précédents. […]
>
>En 1967, les physiciens Sidney Coleman et Jeffrey Mandula ont réussi à prouver qu’aucune autre symétrie associée à l’espace, au temps ou au mouvement ne peut être combinée à celles que nous venons de détailler pour donner une théorie ayant le moindre rapport avec le monde dans lequel nous vivons.
>
>Pourtant, par la suite, un examen minutieux de ce théorème, fondé sur les idées de bon nombre de physiciens, révéla qu’il y avait une lacune, assez subtile : le résultat de Coleman et Mandula n’exploitait pas entièrement les symétries relatives à ce que l’on appelle le spin. […]
>
>En 1925, les physiciens néerlandais George Uhlenbeck et Samuel Goudsmit ont compris qu’un certain nombre de propriétés surprenantes de la lumière émise et absorbée par les atomes pouvaient être expliquées si l’on admettait que l’électron possède des propriétés magnétiques très particulières. Une centaine d’années plus tôt, le Français André Marie Ampère avait montré que le magnétisme résultait du mouvement de charges électriques. Uhlenbeck et Goudsmit ont suivi cette piste et trouvé que seul un mouvement bien particulier de l’électron pouvait produire les propriétés magnétiques que suggéraient les données : un mouvement de rotation sur lui-même, le spin. Contrairement à ce que l’on pouvait penser, les électrons ont un mouvement de révolution et un mouvement de rotation.
>
>Uhlenbeck et Goudsmit voulaient-ils vraiment dire que l’électron tourne sur lui-même ? Oui et non. Ce qu’ils ont montré, en fait, c’est qu’il existe une notion quantique du spin à peu près équivalent à l’imagine usuelle de rotation sur soi, mais inhérente à la théorie quantique. […] Selon leurs travaux et les études ultérieures, chaque électron de l’Univers tourne constamment et indéfiniment à un rythme fixe et invariable. Le spin de l’électron est une propriété intrinsèque, comme sa masse ou sa charge électrique. Un électron qui ne tourbillonnerait pas de la sorte ne serait pas un électron. […]
>
>Toutes les particules matérielles (ainsi que leurs partenaires antimatérielles) ont un spin égal à celui de l’électron. Dans leur jargon, les physiciens disent que les particules de matière sont toutes de « spin 1/2 », où la valeur 1/2 est, grosso modo, une mesure quantique de la rapidité avec laquelle les particules « tournent ». Par ailleurs, les physiciens ont montré que les porteurs des forces non gravitationnelles – les photons, les bosons de jauge faibles et les gluons – possèdent également cette caractéristique intrinsèque, qui se trouve valoir le double de celle des particules de matière. Toutes sont de « spin 1 ».
>
>Et la gravitation ? Bien avant la théorie des cordes, les physiciens avaient pu déterminer le spin que devrait avoir l’hypothétique graviton afin de transmettre l’interaction gravitationnelle. La réponse : le double du spin des photons, des bosons faibles et des gluons, c’est-à-dire un « spin 2 ». […]
>
>Comme nous l’avons souligné, si le concept de spin fait penser à l’image d’une toupie, il s’en écarte radicalement par sa nature quantique. Sa découverte en 1925 révéla une nouvelle sorte de mouvement de rotation qui n’existait pas dans l’univers classique.
>
>Cela suggère la question suivante : de même que le mouvement de rotation ordinaire s’assortit du principe de symétrie de l’invariance par rotation (« la physique traite toutes les orientations spatiales sur un pied d’égalité »), se pourrait-il que le mouvement de rotation un peu plus subtil associé au spin conduise à une autre symétrie des lois de la nature ? Dès 1971, les physiciens avaient démontré que la réponse était affirmative. Bien que l’histoire complète soit assez compliquée, l’idée de base est que, si l’on prend en compte le spin, une symétrie supplémentaire des lois de la nature devient mathématiquement possible. Celle-ci est connue sous le nom de supersymétrie. […]
>
>Au début des années soixante-dix, les physiciens réalisèrent que, si l’Univers était supersymétrique, les particules devaient exister par paires avec des spins respectifs différant d’une demi-unité. On appelle ces paires de particules des superpartenaires, qu’elles soient ponctuelles (comme dans le modèle standard) ou de petites cordes vibrantes. Puisque les particules de matière ont un spin 1/2 alors que les particules d’interaction ont un spin 1, la supersymétrie pourrait associer – apparier – particules de matière et particules d’interaction. Ainsi, elles apparaîtrait comme un concept unificateur fabuleux. Le problème réside dans les détails.
>
>Dès le milieu des années soixante-dix, en cherchant à incorporer la supersymétrie au modèle standard, les physiciens ont découvert qu’aucune des particules connues ne pouvait être partenaire l’une de l’autre. A l’inverse, des analyses théoriques détaillées ont montré que, si l’Univers était supersymétrique, chaque particule connue devait avoir une particule superpartenaire non encore découverte, dont le spin serait inférieur d’une demi-unité à celui de son homologue connue. Par exemple, il devait exister une particule de spin 0 partenaire de l’électron ; cette particule hypothétique a été baptisée sélectron (contraction de « supersymétrique » avec « électron »). Idem pour toutes les autres particules de matière, par exemple avec les superpartenaires hypothétiques des neutrinos et des quarks : les sneutrinos et les squarks, de spin 0. De même, les particules d’interaction devaient avoir des superpartenaires de spin 1/2 ; les photinos pour les photons, les gluinos pour les gluons et, pour les bosons Z et W, les zinos et les winos.
>
>A première vue, la supersymétrie ne paraît absolument pas rentable : elle requiert toute une série de nouvelles particules et se solde par le dédoublement de la liste des ingrédients élémentaires. Puisque aucune de ces particules superpartenaires n’a encore jamais été détectée, vous seriez tout à fait en droit de reprendre à votre compte le commentaire de Rabi sur la découverte du muon, en déclarant que « personne n’a commandé de supersymétrie », et de rejeter en bloc ce principe de symétrie. Pourtant, un grand nombre de physiciens estiment qu’on aurait tort de récuser d’emblée la supersymétrie. […]
>
>La supersymétrie change complètement la donne, car les contributions quantiques des bosons, les particules dont le spin est un nombre entier (leur nom rend hommage au physicien indien Satyendra Bose), et des fermions, particules dont le spin est un nombre demi-entier (leur nom se réfère au physicien italien Enrico Fermi), tendent à s’annuler. Comme dans une sorte de jeu à bascule, lorsque les spasmes quantiques des bosons comptent positivement, ceux des fermions comptent négativement, et vice versa. Et, puisque la supersymétrie assure que bosons et fermions interviennent par paires, de réelles compensations auront lieu dès le début, et ces compensations atténuent considérablement certains des effets quantiques les plus frénétiques. […]
>
>Nous savons tous que l’attraction électrique entre deux particules de charge opposée, ou l’attraction gravitationnelle entre deux corps massifs, est d’autant plus intense que les objets sont proches. Ce sont des caractéristiques simples et bien connues de la physique classique. Il se passe pourtant des choses inattendues quand on étudie les effets de la théorie quantique sur l’intensité des forces. Mais pourquoi la mécanique quantique devrait-elle avoir un quelconque effet ? Ici encore, la réponse est que cela est dû aux fluctuations quantiques. Lorsque l’on étudie le champ de force électrique d’un électron, par exemple, on l’observe en fait à travers un « brouillard » d’éruptions et d’annihilations temporaires de paires particule/antiparticule, qui se produisent dans l’espace environnant l’électron. Voici quelque temps, les physiciens ont compris que ce brouillard foisonnant de fluctuations microscopiques obscurcissait partiellement le champ de force de l’électron, un peu comme une légère brume peut atténuer la lueur d’un phare. Notez que si l’on se rapproche de l’électron, ayant traversé une couche plus importante de ce voile nuageux de particules et d’antiparticules, nous sentirons moins cette atténuation. C’est pourquoi l’intensité du champ électrique d’un électron augmente à mesure que l’on s’en rapproche.
>
>Les physiciens distinguent cette augmentation d’intensité d’origine quantique de celle que l’on connaissait en physique classique en disant que l’intensité intrinsèque de la force électromagnétique augmente aux courtes échelles de distance. Ce n’est donc pas que l’intensité du champ augmente parce que l’on se rapproche de l’électron, mais plutôt qu’une plus grande partie de son champ électrique intrinsèque devient visible. En fait, cette analyse s’applique non seulement à l’électron, mais à toutes les particules chargées, et c’est cela que l’on exprime en disant que les effets quantiques font que l’intensité de la force électromagnétique augmente à plus courte distance.
>
>Qu’en est-il des autres forces du modèle standard ? Comment leurs intensité intrinsèques varient-elles avec les échelles de distance ? En 1973, Gross et Frank Wilczek, à Princeton et, indépendamment, David Politzer, à Harvard, se sont penchés sur la question et ont obtenu une réponse surprenante : le nuage quantique de création et d’annihilation de particules amplifie l’intensité des forces nucléaires faible et forte. Ainsi, lorsqu’on les examine de plus près, pénétrant plus avant le voile nuageux, nous subirons moins son effet amplificateur. L’intensité de ces forces est donc atténuée lorsqu’on se rapproche.
>
>S’emparant de cette découverte, Georgi, Quinn et Weinberg en ont tiré un résultat remarquable. Ils ont prouvé qu’en prenant soigneusement en compte cette frénésie quantique on parvenait finalement à ce que les intensités des forces non gravitationnelles convergent. Bien que les intensités de ces trois forces restent très différentes aux échelles accessibles à la technologie actuelle, Georgi, Quinn et Weinberg soutenaient que ces écarts n’étaient dus qu’aux différentes effets qu’exerçait sur chaque force le halo de suractivité quantique. Leurs calculs montraient que si l’on pénétrait cette brume afin d’étudier les forces non pas aux échelles quotidiennes, mais sur des distances de 10-29 centimètre, à peine dix mille fois la longueur de Planck, les intensités des trois forces non gravitationnelles deviendraient équivalentes. […]
>
>En 1991, Ugo Amaldi, du CERN, ainsi que Wim de Boer et Hermann Fürstenau, de l’université de Karlsruhe, en Allemagne, ont refait les calculs de l’extrapolation de Georgi, Quinn et Weinberg en tenant compte de ces perfectionnements expérimentaux et mis au jour deux faits importants. Le premier est qu’aux très courtes distances (autrement dit, haute énergie/haute température) les intensités des trois forces non gravitationnelles sont presque égales, mais pas tout à fait. Le second est que cet écart, ténu mais incontestable, disparaît si l’on incorpore la supersymétrie. Cela est dû au fait que les nouvelles particules superpartenaires que requiert la supersymétrie contribuent aux fluctuations quantiques, et ces fluctuations additionnelles fournissent juste le coup de pouce qu’il faut pour que les intensités des forces convergent.
>
>Beaucoup de chercheurs ont du mal à croire que la nature a choisi les forces en sorte que, pour l’extrêmement petit, leurs intensités s’unifient presque, mais pas tout à fait. C’est comme si l’on assemblait un puzzle dont la pièce finale était légèrement déformée et ne rentrait pas exactement dans l’emplacement qui lui est réservé. La supersymétrie rectifie prestement sa forme, et ainsi toutes les pièces tiennent bien en place.
>
>Cette dernière découverte a un autre intérêt ; elle répond à la question « Pourquoi n’a-t-on jamais observé aucune des particules supersymétriques ? » Les calculs qui ont mené à la convergence des intensités des trois forces, ainsi que d’autres considérations étudiées par bien d’autres physiciens, indiquent que les particules superpartenaires doivent être beaucoup plus lourdes que les particules connues. Bien que l’on ne puisse faire aucune prédiction définitive, les recherches suggèrent que les particules supersymétriques pourraient être mille fois plus lourdes que le proton, voire plus. Le fait que même les plus sophistiqués de nos accélérateurs de particules ne peuvent atteindre ces énergies permettrait d’expliquer pourquoi ces particules n’ont pas encore été découvertes. […]
>
>La théorie des cordes originale, telle qu’elle a émergé des travaux de Veneziano à la fin des années soixante, incorporait toutes les symétries que nous avons introduites au début de ce chapitre, mais pas la supersymétrie (qui n’avait pas encore été découverte). En fait, la première théorie fondée sur l’idée des cordes a été baptisée théorie des cordes bosoniques. L’adjectif bosonique indique que tous les modes de vibration de cette corde ont un spin égal à un nombre entier ; il n’y a pas de configurations fermioniques, c’est-à-dire pas de configuration dont les spins différeraient d’un nombre entier par une demi-unité. Cela a engendré deux problèmes.
>
>Premièrement, si la théorie des cordes est censée décrire toutes les forces et toute la matière, elle devrait, d’une manière ou d’une autre, contenir des modes de vibration fermioniques, puisque les particules matérielles connues ont toutes un spin égal à 1/2. Deuxièmement, et cela est bien plus troublant, il existe une configuration vibratoire de la théorie des cordes bosoniques dont la masse (plus exactement, dont le carré de la masse) est négative ; on appelle ce type de particule un tachyon. […]
>
>En 1971, Pierre Ramond, de l’université de Floride, a relevé le défi consistant à modifier la théorie des cordes bosoniques pour inclure des modes de vibration fermioniques. Avec ces travaux et les résultats ultérieurs de John Schwarz et d’André Neveu, une nouvelle version de la théorie des cordes a vu le jour. Et, à la surprise quasi générale, les modes de vibrations bosoniques et fermioniques de cette nouvelle théorie semblaient exister par paires. A chaque configuration bosonique était associée une configuration fermionique, et vice versa. Dès 1977, les travaux de Ferdinando Gliozzi, de l’université de Turin, de Scherk et de David Olive, de l’Imperial College de Londres, ont fait la lumière sur cet appariement. La nouvelle théorie des cordes contenait la supersymétrie, et le fait que les modes de vibration bosoniques et fermioniques allaient par paires reflétait cette nature hautement symétrique. La théorie des cordes supersymétriques – c’est-à-dire la théorie des supercordes – venait de naître. De plus, Gliozzi, Scherk et Olive avaient prouvé un autre résultat essentiel : les supercordes ne souffraient pas de l’embarrassante vibration tachyonique des cordes bosoniques. Petit à petit, les pièces du puzzle se mettaient en place.

### Dimensions cachées [p. 211]

>Dans un article qu’il a adressé à Einstein en 1919, Kaluza faisait une suggestion surprenante. Il proposait que la structure spatiale de l’Univers puisse posséder plus de dimensions que les trois habituelles. La raison pour laquelle il en était venu à formuler cette thèse si radicale, comme nous allons le voir sous peu, était qu’il avait découvert qu’elle fournissait un cadre élégant et séduisant pour unifier la théorie de la relativité générale d’Einstein et la théorie de l’électromagnétisme de Maxwell en un modèle conceptuel unique. Mais, tout d’abord, comment cette proposition peut-elle bien être compatible avec le fait flagrant que nous voyions trois dimensions ?
>
>La réponse à cette question, implicite dans le travail de Kaluza, puis explicitées et affinée par le mathématicien suédois Oskar Klein en 1926, est que la structure spatiale de l’Univers peut présenter à la fois des dimensions étendues et des dimensions enroulées. […]
>
>Ainsi, nous découvrons avec surprise que, même si nous ne voyons que trois dimensions spatiales, le raisonnement de Kaluza et de Klein montre que cela n’exclut ps l’existence de dimensions supplémentaires, dans la mesure où celles-ci sont ultrapetites. L’Univers pourrait très bien avoir des dimensions cachées.
>
>Petites, certes, mais « petites » à quel point ? Les expériences les plus sophistiquées peuvent détecter des structures de l’ordre d’un milliardième de milliardième de mètre. Une dimension enroulée à une échelle inférieure à cette distance minuscule serait trop ténue pour être detectée. En 1926, Klein a combiné la suggestion initiale de Kaluza avec des idées issues du domaine naissant de la théorie quantique. Ses calculs indiquaient qu’une dimension circulaire additionnelle pourrait être aussi petite que la longueur de Planck, ce qui dépasse largement nos capacités de détection. Depuis lors, les physiciens appellent théorie de Kaluza-Klein la possible existence de minuscules dimensions spatiales supplémentaires. […]
>
>L’idée de Kaluza selon laquelle notre Univers posséderait plus de dimensions spatiales que celles que nous voyons directement était déjà remarquable en soi. Un autre point la rendit réellement intéressante. Einstein avait formulé la relativité générale dans le cadre familier d’un univers comportant trois dimensions spatiales et une dimension temporelle. Le formalisme mathématique de la théorie, quant à lui, peut se généraliser de manière tout à fait directe, conduisant à des équations analogues pour un univers présentant des dimensions supplémentaires. Partant de la « modeste » hypothèse d’une seule dimension supplémentaire, Kaluza a refait toute l’analyse mathématique et dérivé de nouvelles équations.
>
>Il a obtenu ainsi des équations relatives aux trois dimensions ordinaires quasi identiques à celles d’Einstein. Mais, puisqu’il avait ajouté une quatrième dimension spatiale, Kaluza n’a pas été surpris de découvrir des équations supplémentaires, outre celles qu’Einstein avait obtenues initialement. Après avoir étudié ces équations associées à la nouvelle dimension, il a compris qu'il se passait quelque chose d'incroyable. Les équations additionnelles n'étaient autres que les équations de Maxwell décrivant la force électromagnétique ! En ajoutant une seule dimension spatiale, Kaluza avait réuni la théorie de la gravitation d'Einstein et la théorie de la lumière de Maxwell !
>
>Avant les travaux de Kaluza, on pensait que la gravitation et l'électromagnétisme étaient deux forces sans rapport entre elles. En ayant l'audace et la créativité de proposer que notre Univers avait une dimension spatiale supplémentaire, Kaluza suggérait qu'elles étaient, en fait, intimement liées. En vertu de sa théorie, gravitation et électromagnétisme étaient toutes deux associées aux ondulations de la structure de l'espace-temps ; la gravitation procède des distorsions des trois dimensions spatiales habituelles, tandis que l'électromagnétisme découle de celles de la nouvelle dimension enroulée sur elle-même.
>
>Kaluza a adressé son article à Einstein, et, en premier lieu, celui-ci a été fort intrigué. Le 21 avril 1919, il répondait à Kaluza qu'il ne lui était jamais venu à l'esprit que l'unification pouvait émerger « d'un monde cylindrique à cinq dimensions [quatre d’espace et de temps] ». Et il ajoutait : « A première vue, votre idée me plaît énormément. » Cependant, environ une semaine plus tard, Einstein écrivait de nouveau à Kaluza, mais, cette fois, il se montrait plus sceptique : « J’ai lu votre article en détail et le trouve réellement intéressant. Jusqu'ici, je ne décèle aucune impossibilité. D'un autre côté, je dois admettre que les arguments que vous avancez ne sont pas assez convaincants. » Puis finalement, le 14 octobre 1921, plus de deux années plus tard, Einstein écrivait encore une fois à Kaluza, après avoir pris le temps de digérer un peu mieux l'approche radicale de celui-ci : « Je reviens sur mon opinion qui vous avait empêché, voici deux ans, de publier votre idée d'unification de la gravitation et de l'électricité, Après tout, s'il vous en convient, je proposerai tout de même votre article à l’Académie. » C'était tardif, mais Kaluza avait finalement reçu l'approbation du maître.
>
>L'idée était de toute beauté, mais des études ultérieures de la proposition de Kaluza, améliorée par Klein, ont prouvé qu'elle entrait directement en conflit avec les données expérimentales. Les tentatives d'introduction de l'électron dans la théorie prédisaient, entre sa masse et sa charge, des relations très différentes des valeurs expérimentales. Comme il n'y avait apparemment aucun moyen de résoudre ce problème, les physiciens qui s’étaient intéressés à l’idée de Kaluza ont fini par s’en détourner. Einstein et quelques autres ont continué, de temps à autre, à envisager vaguement l’éventualité de dimensions supplémentaires enroulées, mais l’approche fut rapidement reléguée aux marges de la physique théorique. […]
>
>Au cours des soixante années qui ont suivi la proposition de Kaluza, la compréhension de la physique s’est considérablement modifiée et approfondie. La théorie quantique avait été amplement vérifiée. Les forces faible et forte, inconnues dans les années vingt, avaient été découvertes et analysées. Certains physiciens ont émis l’idée que si la proposition de Kaluza avait échoué c’était qu’ils ne connaissait pas ces autres forces ; peut-être s’était-il montré trop conservateur dans son réaménagement de l’espace. Plus de forces, cela voulait dire plus de dimensions. On a même démontré qu’une seule dimension circulaire supplémentaire, même si elle fournissait les indices de la relation entre la relativité générale et l’électromagnétisme, était tout bonnement insuffisante. […]
>
>Il se trouve que la théorie des cordes requiert que notre Univers possède des dimensions supplémentaires.
>
>Voici pourquoi. L’une des plus grandes découvertes de la théorie quantique est que notre pouvoir prédictif se limite à l’affirmation que tel ou tel résultat se produira avec telle ou telle probabilité. Einstein trouvait cela déplaisant, et peut-être être-vous d’accord avec lui, mais c’est comme ça. Or nous savons tous qu’une probabilité est toujours un nombre compris entre 0 et 1, ou entre 0 et 100 si on les exprime par des pourcentages. Les physiciens ont découvert le signe qui trahit le détraquement de la théorie quantique : dès qu’un calcul particulier conduit à des « probabilités » qui n’entrent pas dans ces bornes. […] A la naissance de la théorie des cordes, les chercheurs ont trouvé que certains calculs donnaient des « probabilités » négatives, elles aussi en dehors des limites autorisées. […]
>
>Les physiciens ont découvert que les calculs problématiques dépendaient du nombre de directions indépendantes dans lesquelles elles peuvent vibrer. Les « probabilités » négatives provenaient d’une disparité entre ce que la théorie exigeait et ce que la réalité semblait imposer : les calculs montraient que, si les cordes pouvaient vibrer dans neuf directions spatiales indépendantes, toutes les « probabilités » négatives s’annuleraient entre elles. Très bien ; c’est génial, en théorie ; et alors ? Si la théorie des corde est censée décrire notre monde avec ses trois dimensions spatiales, nous ne sommes pas tirés d’affaire.
>
>Vraiment ? Suivant une piste vieille de plus d’un demi-siècle, nous devinons que Kaluza et Klein fournissent une échappatoire. Puisque les cordes sont si petites, non seulement elles peuvent vibrer dans des dimensions grandes et étendues, mais elles peuvent tout aussi bien osciller dans d’autres dimensions, enroulées et minuscules. Et nous pouvons ainsi satisfaire, dans notre Univers, l’exigence de neuf dimensions spatiales de la théorie des cordes, en supposant – à la Kaluza-Klein – qu’en plus de nos trois dimensions spatiales il en existe six autres, enroulées sur elles-même. Et c’est ainsi que la théorie des cordes, sur le point d’être radiée du royaume de la pertinence physique, fut sauvée. De plus, au lieu de postuler simplement l’existence de ces dimensions supplémentaires comme l’avaient fait Kaluza, Klein et leurs successeurs, la théorie des cordes les exige. Pour qu’elle ait un sens, l’Univers doit posséder neufs dimensions spatiales et une dimensions temporelle, soit dix dimensions en tout. Et la suggestion de Kaluza, datant de 1919, a trouvé sa tribune la plus convaincante et la plus puissante.
>
>Cela soulève un certain nombre de questions. Premièrement, pourquoi la théorie des cordes requiert-elle ce nombre précis de neuf dimensions spatiales pour s’affranchir des probabilités absurdes ? C’est sans doute la question dont la réponse est la plus difficile à formuler sans recours au formalisme mathématique. Un calcul direct y répond, mais personne n’a d’interprétation intuitive, ni d’explication exempte de détails techniques, du nombre qui en ressort. […]
>
>Deuxièmement, si les équations de la théorie des cordes (ou, plus précisement, les équations approchées qui sous-tendent nos explications préliminaires) montrent que l’Univers possède neuf dimensions spatiales et une dimension temporelle, comment se fait-il que trois dimensions spatiales (et une temporelle) soient grandes et étendues, alors que toutes les autres sont minuscules et entortillées ? Pourquoi ne sont-elles pas toutes étendues, ou toutes entortillées, ou n’importe quoi d’autre ? Pour le moment, personne ne connaît la réponse à cette question. […]
>
>Puisque les cordes minuscules vibrent dans toutes les dimensions spatiales, la façon précise dont les dimensions supplémentaires s’entortillent et s’enroulent sur elles-mêmes aura une grande influence et restreindra sévèrement les configurations vibratoires possibles. Ce sont ces configurations résonnantes, en grande partie déterminées par la géométrie des nouvelles dimensions, qui forment le spectre des propriétés possibles des particules que l’on observe dans les dimensions étendues. Et cela signifie que la géométrie des dimensions supplémentaires détermine les attributs de la physique fondamentale, comme les masses ou les charges des particules que l’on observe dans les trois dimensions habituelles de notre expérience quotidienne.
>
>Ce point est si important et si fondamental qu’il mérite d’être lourdement souligné. Selon la théorie des cordes, l’Univers se compose de petites cordes, dont les résonances vibratoires sont l’origine microscopique des masses et des charges d’interaction des particules. La théorie des cordes exige également la présence de dimensions spatiales supplémentaires, qui doivent être entortillées en une taille minuscule pour s’accorder avec le fait qu’elles n’ont jamais été observées. Or une toute petite corde peut explorer un espace tout petit. La corde se déplace tout en oscillant, et la forme géométrique de ces dimensions supplémentaires joue un rôle essentiel dans la détermination des configurations de résonance. Puisque les modes de vibration des cordes se manifestent en tant que masses et charges des particules élémentaires, nous en concluons que ces propriétés fondamentales de l’Univers sont déterminées, dans une large mesure, par la taille et la forme géométrique des dimensions supplémentaires. C’est l’une des découvertes les plus profondes de la théorie des cordes.



### Signatures expérimentales [p. 234]

>Edward Witten aime à dire que la théorie des cordes a déjà fait une prédiction spectaculaire, amplement confirmée expérimentalement : « La théorie des cordes présente la propriété remarquable de prédire la gravité. » Ce que Witten veut dire par là, c’est que Newton et Einstein ont élaboré des théories de la gravitation car leurs observations de notre monde leur montraient clairement l’existence de la gravité, qui requérait donc une explication précise et cohérente. A l’inverse, un physicien qui étudierait les théorie des cordes, même en ne sachant absolument rien de la relativité générale, en trouverait inexorablement le chemin. Avec son mode vibratoire du graviton, de spin 2 et de masse nulle, la théorie des cordes contient entièrement la gravitation, bien ancrée au cœur de sa structure théorique. Comme le dit encore Witten, « le fait que la gravité soit une conséquence de la théorie des cordes représente l’une des plus grandes découvertes théoriques que l’on ait jamais faites ». A condition que cette « prédiction » soit rebaptisée « postdiction », dans la mesure où les physiciens avaient découvert des descriptions théoriques de la gravitation bien avant de connaître la théorie des cordes. Witten signale qu’il s’agit là d’un accident de l’Histoire. Et il ajoute qu’on pourrait tout à fait imaginer que d’autres civilisations évoluées de l’Univers aient découvert la théorie des cordes en premier lieu, et qu’une théorie de la gravité n’en ait été obtenue que comme une conséquence stupéfiante. […]
>
>Peu de temps avant sa mort, Richard Feynman avait clairement dit qu’il ne pouvait croire que la théorie des cordes était le remède unique aux problèmes – en particulier, les infinis pernicieux – qui entravaient la fusion harmonieuse entre la gravitation et la théorie quantique.
>
>>« Mon opinion – mais je peux me tromper – est qu’il y a plus d’une façon de dépecer un chat. Je ne crois pas qu’il n’existe qu’une seule manière de se débarrasser des infinis. Selon moi, le fait qu’une théorie s’en affranchisse ne constitue pas une raison suffisante pour croire en son unicité. » […]
>
>Comme c’est souvent le cas pour les débats de grande importe, à chacun de ces détracteurs s’oppose un fervent défenseur. Witten raconte que, lorsqu’il a appris comme la théorie des cordes englobait la gravité et la théorie quantique, cela a été « le plus grand frisson intellectuel » de sa vie. Cumrun Vafa, de Harvard, l’un des chefs de file de la théorie des cordes, dit que « la théorie des cordes laisse vraiment entrevoir la plus profonde compréhension de l’Univers que l’on ait jamais eue ». Pour Murray Gell-Mann, prix Nobel, la théorie des cordes est « une chose fabuleuse » et il s’attend qu’une version de la théorie des cordes soit un jour la théorie du monde dans sa totalité. […]
>
>La longueur de Planck est quelque dix-sept ordres de grandeur en dessous de ce que nous pouvons voir actuellement. Ainsi, avec les technologies actuelles, il nous faudrait un accélérateur de la taille de la galaxie pour voir les cordes individuellement. En fait, Shmuel Nussinov, de l’université de Tel-Aviv, a montré que cette estimation grossière, fondée sur de simples calculs de proportionnalité, est sans doute trop optimiste ; selon son analyse, plus précise, il faudrait un accélérateur de la taille de l’Univers. (L’énergie nécessaire pour sonder la matière à la longueur de Planck est à peu près égale à un millier de kilowatts/heure, c’est-à-dire l’énergie nécessaire pour faire fonctionner un radiateur électrique pendant une centaine d’heures, ce qui n’est pas particulièrement extravagant. Le défi technologique qui semble insurmontable est de concentrer toute cette énergie sur une seule particule, c’est-à-dire une seule corde.) […]
>
>Dans leur article fondateur, Candelas, Horowitz, Strominger et Witten avaient fait un premier pas dans cette direction. Ils ont découvert non seulement que les dimensions supplémentaires de la théorie des cordes doivent être enroulées en une forme de Calabi-Yau, mais ils en ont déduit aussi certaines conséquences pour les configurations vibratoires possibles des cordes. L’un de leurs résultats essentiels met en lumière des solutions spectaculaires et inattendues qu’offre la théorie des cordes à des problèmes anciens en physique des particules.
>
>Souvenez-vous que les particules élémentaires découvertes par les physiciens s’organisent en trois familles identiques, avec des particules de plus en plus massives d’une famille à l’autre. La question mystérieuse qui restait sans réponse avant la théorie des cordes est : pourquoi y-a-t-il des familles, et pourquoi trois familles ? Et voici ce que propose la théorie des cordes. Un espace de Calabi-Yau typique contient des trous, semblables à ceux d’un beignet, ou encore d’un « multibeignet ». Pour les espaces de Calabi-Yau de dimension supérieure, il peut, en fait, y avoir un grand nombre de trous de différentes sortes, et ces trous peuvent, eux-mêmes, avoir diverses dimensions (ce sont des trous « multidimensionnels »). Candelas, Horowitz, Strominger et Witten ont soigneusement analysé l’effet de ces trous sur les modes de vibration possibles des cordes. […]
>
>Il existe une famille de vibrations de cordes de plus basse énergie associée à chaque trou de la portion de l’espace que représente la forme de Calabi-Yau. Puisque les particules élémentaires habituelles doivent correspondre aux modes de vibration de plus basse énergie, l’existence de trous multiples – un peu comme ceux du multibeignet – implique que les configurations vibratoires des cordes s’organisent en familles multiples. Si le minuscule espace de Calabi-Yau possède trois trous, nous aurons trois familles de particules élémentaires. Ainsi, la théorie des cordes proclame que l’organisation en familles, observée expérimentalement, n’est ni inexplicable, ni aléatoire, ni d’origine divine, mais reflète le nombre de trous de la forme géométrique que composent les dimensions supplémentaires ! […]
>
>Malheureusement, le nombre de trous que présente chacune des dizaines de milliers de formes de Calabi-Yau connues parcourt une large gamme. Certaines en ont trois. D’autres en ont quatre, cinq, vingt-cinq, et ainsi de suite – certaines en ont même jusqu’à quatre cent quatre-vingts. Le problème est qu’actuellement personne ne sait déduire des équations de la théorie des cordes la forme de Calabi-Yau que composent les dimensions spatiales supplémentaires. […]
>
>Dans un travail ultérieur, Strominger et Witten ont montré que les masses des particules de chaque famille dépendent – accrochez-vous, ça va tanguer – de la façon dont les frontières des différents trous multidimensionnels de l’espace de Calabi-Yau s’intersectent et se chevauchent. C’est assez difficile à visualiser, mais l’idée est que, les cordes vibrant dans les dimensions supplémentaires, l’arrangement précis des différents trous et la façon dont l’espace de Calabi-Yau les enveloppe dans ses replis ont un impact direct sur les configurations vibratoires possibles. […]
>
>Pourquoi ne peut-on pas trouver la « bonne » forme de Calabi-Yau ? La plupart des théoriciens des cordes blâment l’inadéquation des outils théoriques actuellement disponibles. […]
>
>Alors, où en sommes-nous ? Eh bien, même si nous n’avons aucun critère fondamental pour sélectionner un espace de Calabi-Yau plutôt qu’un autre, même si nous ne disposons pas de tous les outils théoriques nécessaires à l’extraction des conséquences observables de tel ou tel choix, nous pouvons toujours nous demander s’il existe ne serait-ce qu’un seul choix dans le catalogue des espaces de Calabi-Yau, qui coïncide au moins à peu près avec les observations. La réponse à cette question est assez encourageante. Bien que la plupart des espaces de Calabi-Yau possibles conduisent à des conséquences observables fort différentes de notre monde (différents nombres de familles de particules, différentes nombres ou types de force fondamentales, parmi d’autres déviations plus substantielles), il en existe quelques-unes dont la physique résultante est qualitativement proche de ce que l’on observe effectivement. Ainsi, il existe des espaces de Calabi-Yau qui, si on les choisit pour les dimensions enroulées de la théorie des cordes, engendrent des vibrations très proches des particules du modèle standard. Et, fait très important, la théorie des cordes réconcilie avec succès la force gravitationnelle et ce cadre quantique.
>
>Avec notre degré de compréhension actuel, on ne peut guère espérer mieux. S’il existait un grand nombre d’espaces de Calabi-Yau en accord approximatif avec l’expérience, le lien entre un choix spécifique et la physique observée serait moins intéressant. Beaucoup de possibilités feraient l’affaire, de sorte qu’aucune ne se distinguerait, même d’un point de vue expérimental. D’un autre côté, si aucune des formes de Calabi-Yau n’était capable de fournir les propriétés physiques observées, cela voudrait dire que le cadre magnifique de la théorie des cordes n’a rien à voir avec notre Univers. Le fait que l’on ait trouvé un petit nombre de formes de Calabi-Yau qui, avec nos capacités actuelles – encore grossières – à déterminer leurs implications physiques, semblent correspondre à de vagues conditions d’acceptabilité est un résultat extrêmement encourageant. […]
>
>Et, finalement, un cinquième moyen de relier la théorie des cordes aux observations pourrait faire intervenir la constante cosmologique. C’est le terme qu’Einstein a ajouté aux équations de la relativité générale pour faire en sorte que l’Univers soit statique. La découverte ultérieure de l’expansion de l’Univers a conduit Einstein à se déjuger, mais, entre-temps, les physiciens ont réalisé qu’il n’y avait aucune raison particulière pour que la constante cosmologique soit nulle. En fait, celle-ci peut s’interpréter comme une sorte d’énergie globale accumulée dans le vide de l’espace, de sorte que sa valeur devrait être calculable en théorie et mesurable en pratique. Or, à ce jour, les calculs et les mesures sont totalement en désaccord : les observations montrent que la constante cosmologique est ou nulle (comme le suggéra finalement Einstein) ou très petite. En revanche, les calculs indiquent que les fluctuations quantiques du vide devraient conduire à une constante cosmologique non nulle dont la valeur serait supérieure, de quelque cent vingt ordres de grandeur (un un suivi de cent vingt zéros) aux résultats expérimentaux ! Cela représente un défi et une opportunité fantastiques pour les théoriciens des cordes : peuvent-ils réduire cette discordance et expliquer pourquoi la constante cosmologique est nulle ? Ou encore, si les expériences établissent finalement qu’elle prend une valeur faible mais non nulle, la théorie des cordes peut-elle fournir une explication ? Si les théoriciens des cordes parviennent à relever ce défi – ce qui n’est pas encore le cas –, nous aurons alors un argument de poids en faveur de la théorie des cordes.



### Géométrie quantique [p. 255]

>La relativité générale déclare que les propriétés courbes de l’Univers sont décrites par la géométrie de Riemann, mais la théorie des cordes soutient que cela est vrai seulement si l’on considère la structure de l’Univers à grande échelle. A l’échelle de la longueur de Planck, une nouvelle géométrie doit émerger, qui va de pair avec la nouvelle physique des cordes. Ce cadre géométrique nouveau s’appelle la géométrie quantique. […]
>
>Selon le modèle cosmologique du big-bang, la totalité de l’Univers aurait émergé d’une explosion cosmique violente, voici une quinzaine de milliards d’années. Aujourd’hui, comme l’a découvert l’astronome américain Edwin Hubble, les « débris » de cette explosion, sous la forme de plusieurs millions de galaxies, continuent à s’éloigner les uns les autres. L’Univers est en expansion. Nous ne savons pas si cette croissance cosmique continuera pour toujours ou si l’expansion ralentira un jour, puis s’arrêtera pour s’inverser et produire une implosion cosmique. Astronomes et astrophysiciens cherchent à répondre à cette question par l’expérience, car la réponse fait intervenir une grandeur qu’on devrait, en principe, pouvoir mesurer : la densité de matière moyenne de l’Univers.
>
>Si la densité moyenne de matière dépasse une densité dite critique, valant environ 10-29 gramme par centimètre cube – à peu près cinq atomes d’hydrogène par mètre cube –, alors la force gravitationnelle qui imprègne le cosmos sera suffisamment importante pour freiner et inverser l’expansion. Si la densité moyenne de matière est inférieure à cette valeur critique, l’attraction gravitationnelle sera trop faible pour arrêter l’expansion, et celle-ci continuera indéfiniment. (En vous fondant sur vos propres observations de notre monde, vous pourriez croire que la densité moyenne de l’Univers dépasse de loin la valeur critique. Mais gardez à l’esprit que la matière – comme l’argent – à tendance à se rassembler. […]
>
>Une analyse méticuleuse de la distribution des galaxies a permis aux astronomes de se faire une idée assez précise de la quantité moyenne de matière visible dans l’Univers. Il se trouve que celle-ci est largement inférieure à la valeur critique. Mais de nombreux indices, aussi bien théoriques qu’expérimentaux, semblent indiquer que l’Univers est rempli de matière noire, invisible. Cette matière-là ne participe pas aux processus de fusion nucléaire qui alimentent les étoiles, de sorte qu’elle ne produit pas de lumière ; elle est donc indétectable par les télescopes des astronomes. Personne ne sait encore ce qu’est cette matière invisible, et encore moins combien il y en a. A ce jour, le sort de notre Univers, actuellement en expansion, reste un mystère. […]
>
>Puisque la corde est un objet étendu, il existe une autre configuration en plus de celles que l’on a déjà mentionnées : elle peut s’enrouler – à la manière d’un lasso, en quelque sorte – autour de la partie circulaire de l’Univers Tuyau. La corde pourra toujours glisser et osciller, mais elle le fera dans cette configuration généralisée. En fait, la corde peut s’enrouler autour de la partie circulaire de l’espace un nombre quelconque de fois, tout en continuant de vibrer. Lorsqu’une corde est dans cette configuration, on dit de son mouvement qu’il est dans un mode enroulé. Être dans un mode enroulé est clairement une caractéristique propre aux cordes. Il n’y a pas d’équivalent pour les particules ponctuelles. […]
>
>Brandenberger, Vafa et d’autres chercheurs ont exploité ces idées pour proposer une réécriture des lois de la cosmologie dans laquelle le big-bang comme le big-crunch, au lieu de faire intervenir un univers de taille nulle, impliqueraient un univers dont toutes les dimensions atteindraient la longueur de Planck. Cette proposition très séduisante permettrait d’éviter les énigmes mathématiques, physiques et logiques d’un univers qui émanerait de (ou s’effondrait en) un point infiniment dense. Il est certes difficile d’imaginer l’Univers entier compressé à l’intérieur d’une minuscule coquille planckienne. Mais l’idée de la comprimer jusqu’en un point, sans aucune extension, dépasser complètement l’entendement. La cosmologie des cordes n’en est qu’à ses balbutiements, mais c’est un domaine très prometteur qui pourrait bien proposer une alternative plus facile à accepter que le modèle habituel du big-bang.


### Déchirer l’espace-temps [p. 289]

>Étirez un élastique avec insistance, il finira tôt ou tard par casser. Ce simple fait a inspiré beaucoup de physiciens au fil des années : peut-on faire de même avec la structure spatiale de l’Univers ? L’espace peut-il se déchirer, ou bien nous trompons-nous lourdement en prenant l’analogie de la toile élastique au pied de la lettre ?
>
>La relativité générale répond qu’on ne peut pas déchirer la structure de l’espace. Ses équations sont fermement ancrées dans la géométrie riemannienne, et, comme nous l’avons noté au chapitre précédent, il s’agit d’un cadre qui analyse les distorsions dans les relations de distance entre points voisins de l’espace. Pour que ces relations de distance aient un sens, le formalisme mathématique sous-jacent requiert que le substrat de l’espace soit lisse – ce terme a aussi un sens mathématique, mais le sens qu’on lui donne dans la vie de tous les jours le résume correctement : pas de plis, pas d’accrocs, pas de morceaux « rapiécés » et pas de déchirures. Si la structure de l’espace venait à développer de telles irrégularités, alors les équations de la relativité générale cesseraient d’être valables, ce qui serait le signal de telle ou telle catastrophe cosmique, issue désastreuse à laquelle échappe notre Univers apparemment discipliné.
>
>Mais, au fil du temps, cela n’a pas empêché des théoriciens imaginatifs d’envisager que, dans le contexte d’une nouvelle formulation de la physique (qui irait au-delà de la théorie classique d’Einstein afin d’incorporer la physique quantique), de telles déchirures, perforations et recollements de la structure spatiale seraient possibles. En fait, la découverte que la théorie quantique conduit à de violentes ondulations à petite échelle a poussé certains à spéculer que des déchirures pourraient être une caractéristique microscopique banale de la structure spatiale. Le concept des trous de ver procède de telles rêveries. […]
>
>Le trou de vers, en revanche, crée une nouvelle région de l’espace, puisque l’espace courbe bidimensionnel représente tout ce qui existe (dans le cadre de notre modèle à deux dimensions). Les régions « extérieures » ne sont que le reflet de l’inadéquation de l’illustration, qui dépeint l’Univers en « U » comme un objet plongé dans notre Univers de dimension supérieure. Le trou de ver crée une nouvelle portion d’espace et ouvre donc la voie à un nouveau territoire spatial.
>
>Existe-t-il des trous de ver dans l’Univers ? Personne ne le sait. Et, s’ils existent, sont-ils microscopiques ou embrassent-ils de vastes région de l’Univers ? Un élément essentiel pour décider de leur possibilité serait de déterminer si la structure de l’espace peut tout simplement se déchirer.
>
>Les trous noirs fournissent un deuxième exemple intéressant où la structure de l’espace est étirée au bout de ses limites. L’énorme champ gravitationnel d’un trou noir produit une courbure si extrême qu’en son centre la structure de l’espace semble avoir été pincée ou perforée. Contrairement aux trous de ver, il existe de bonnes raisons expérimentales de penser que les trous noirs existent, et la question de savoir ce qui se passe en leur centre est une question scientifique et non un jeu de l’esprit. Les conditions au centre du trou noir sont si extrêmes que les équations de la relativité générale cessent d’y être valables. Certains physiciens pensent qu’il y a vraiment une perforation, mais que nous sommes protégés de cette « singularité » cosmique par l’horizon du trou noir, qui interdit à quoi que ce soit d’échapper à son emprise gravitationnelle. En raisonnant de la sorte, Roger Penrose, de l’université d’Oxford, a échafaudé l’hypothèse d’une « censure cosmique » qui permettrait à ces irrégularités de l’espace d’exister seulement si elles restent bien à l’abri de notre regard, profondément enfouies derrière le voile d’un horizon. Par ailleurs, avant la découverte de la théorie des cordes, certains physiciens ont conjecturé que l’union de la théorie quantique et de la relativité générale montrerait à coup sûr que les perforations apparentes de l’espace sont en fait adoucies – « reprisées », si l’on peut dire – par des considérations quantiques.
>
>La découverte de la théorie des cordes, avec l’union harmonieuse qu’elle réalise entre la mécanique quantique et la relativité générale, permet enfin d’étudier ces questions. Jusqu’à présent, les théoriciens des cordes n’ont pas été en mesure d’y répondre complètement, mais, ces dernières années, certains problèmes ont été résolus. Dans ce chapitre, nous allons voir comment, pour la première fois, la théorie des cordes montre qu’il existe vraiment des situations – différentes des trous de ver et des trous noirs par certains côtés – dans lesquelles la structure de l’espace peut effectivement se déchirer. […]
>
>Ce lundi-là, nous nous sommes rendus triomphalement auprès de Witten pour l’informer de notre succès. Il était ravi de notre résultat. Or nous découvrîmes que lui aussi venait de trouver un moyen de prouver que les transitions de flop se produisaient bien en théorie des cordes. Ses arguments étaient très différents des nôtres et permettaient de mieux comprendre pourquoi les déchirures spatiales n’ont pas de conséquences catastrophiques.
>
>Son approche souligne la différence de comportement entre les particules ponctuelles et les cordes lorsque de telles déchirures se produisent. La distinction essentielle en est qu’il existe pour la corde deux types de mouvement autour de l’accroc, mais seulement un pour la particule ponctuelle. La corde peut se déplacer le long de la déchirure, de même que la particule ponctuelle, mais elle peut aussi l’entourer tout en avançant. Dans son essence, l’analyse de Witten révèle que les cordes qui entourent la déchirure, ce que les particules ponctuelles ne peuvent faire, protègent l’univers environnant des effets catastrophiques qui pourraient se produire. C’est comme si la surface d’univers de la corde – la surface bidimensionnelle que balaie la corde lorsqu’elle se déplace dans l’espace – constituait un écran protecteur qui annule précisément les aspects désastreux de la dégénérescence géométrique de la structure spatiale.
>
>Vous vous demandez peut-être ce qui se passerait si une telle déchirure avait lieu et s’il n’y avait pas de cordes autour pour faire écran. Vous vous dites peut-être aussi qu’une corde – une boucle infiniment fine – fournit une protection aussi efficace qu’un cerceau contre une bombe à fragmentation. La résolution de ces deux points repose sur un aspect central de la théorie quantique. Nous avons vu que, dans la formulation de Feynman, un objet, qu’il s’agisse d’une particule ou d’une corde, chemine d’un point à un autre en empruntant toutes les trajectoires possibles. Le mouvement résultant est une combinaison de toutes les possibilités, la contribution relative de chacune des trajectoires possibles étant déterminée par les mathématiques de la théorie quantique. Si une déchirure devait avoir lieu dans la structure de l’espace, certains des chemins possibles des cordes en mouvement l’entoureraient. Même s’il semble n’y avoir aucune corde autour de la déchirure au moment où celle-ci a lieu, la théorie quantique tient compte des effets physiques de toutes les trajectoires de cordes possibles et, parmi elles, d’un grande nombre (infini, en fait) de chemins qui l’encerclent. Ce sont ces contributions dont Witten a prouvé qu’elles annulent justement la calamité cosmique que créerait, sinon, la déchirure. […]
>
>Ce problème soulève deux questions. Premièrement, nous nous sommes concentrés sur des déchirures de la structure spatiale qui concernent la composante de l’Univers du Calabi-Yau six-dimensionnel. De telles déchirures peuvent-elles avoir lieu aussi dans les trois dimensions étendues qui nous sont familières ? La réponse est, presque certainement, oui. Après tout, l’espace, c’est l’espace – qu’il soit enroulé en forme de Calabi-Yau ou qu’il se déploie sur l’immense étendue de l’Univers que nous percevons par une nuit claire et étoilée. En fait, nous avons vu plus haut que les dimensions spatiales habituelles pourraient elles aussi être enroulées en une forme géante qui se refermerait sur elle-même, au loin, de l’autre côté de l’Univers, de sorte que même la distinction entre les dimensions qui sont enroulées et celles qui ne le sont pas est quelque peu artificielle. Bien que notre analyse et celle de Witten s’appuient effectivement sur des propriétés physiques des formes de Calabi-Yau, le résultat – le fait que la structure de l’espace puisse se déchirer – s’applique à n’en pas douter plus largement.
>
>Deuxièmement, une de ces déchirures qui changent la topologie pourrait-elle se produire aujourd’hui ? Demain ? Est-il possible qu’une déchirure ait déjà eu lieu dans le passé ? Oui. Les mesures expérimentales des masses des particules élémentaires indiquent que leurs valeurs sont très stables dans le temps. Mais si nous remontons aux toutes premières ères qui ont suivi le big-bang, même des théories qui ne sont pas fondées sur les cordes invoquent d’importantes périodes durant lesquelles les masses des particules élémentaires ont effectivement pu changer avec le temps. Ces périodes, du point de vue de la théorie des cordes, auraient très bien pu faire intervenir des déchirures modifiant la topologie comme celles abordées dans ce chapitre. Plus près de nous, la stabilité observée des masses des particules élémentaires implique que, si l’Univers était en train de subir une de ces déchirures spatiales, celle-ci serait excessivement lente – si lente que ses effets sur les masses des particules élémentaires seraient plus faible que notre sensibilité expérimentale actuelle. Fait remarquable, si cette condition était remplie, l’Univers pourrait être, en ce moment même, en proie à une césure spatiale. Si celle-ci avait lieux suffisamment lentement, nous n’en saurions rien. C’est là l’un des rares exemples en physique ou l’absence de phénomènes observables importants est source d’une grande excitation. L’absence de conséquences catastrophiques d’une évolution géométrique aussi exotique témoigne à quel point la théorie des cordes a dépassé les attentes d’Einstein.



### Au-delà des cordes : à la recherche de la théorie M [p. 309]

>Dans sa longue quête d’une théorie unifiée, Einstein s’est demandé si « Dieu aurait pu faire l’Univers d’une manière différente ; c’est-à-dire si la contrainte de la simplicité logique laisse quelque liberté ou pas du tout ». Avec cette remarque, Einstein formulait pour la première fois un point de vue partagé aujourd’hui par beaucoup de physiciens : s’il existe une théorie ultime de la nature, alors, un des arguments les plus convaincants en faveur de sa forme précise serait que la théorie ne peut pas en avoir une autre. La théorie ultime aurait nécessairement la forme qu’elle a car elle représenterait l’unique cadre explicatif capable de décrire l’Univers sans incohérences internes ni aberrations logiques. Une telle théorie proclamerait que les choses sont ce qu’elles sont car elles doivent être ainsi. […]
>
>On voit que les cinq théories des cordes étaient envisagées comme complètement distinctes. A l’instar des cinq bras de l’étoile de mer, grâce aux récentes découvertes, toutes les théories des cordes sont maintenant comprises comme faisant partie d’un cadre unique. (En fait, nous verrons avant la fin de ce chapitre qu’une sixième théorie – un sixième bras – sera même ajouté à cette union.) Ce cadre unificateur a été provisoirement baptisé théorie M. […]
>
>Bien qu’il reste beaucoup à accomplir, les physiciens ont déjà dévoilé deux aspects fondamentaux de la théorie M. D’abord, celle-ci vit en onze dimensions (dix spatiales et une temporelle). Kaluza avait découvert qu’une dimension spatiale supplémentaire permettait l’union inattendue de la relative générale et de l’électromagnétisme. De manière analogue, les théoriciens ont réalisé qu’une dimension spatiale additionnelle en théorie des cordes – outre les neufs dimensions spatiales et la dimension temporelle expliquées dans les chapitres précédents – permettait une synthèse profondément satisfaisante des cinq versions de la théorie. De plus, cette dimension spatiale supplémentaire ne sort pas d’un chapeau ; les théoriciens des cordes ont compris que les raisonnements des années soixante-dix et quatre-vingt qui ont conduit aux dix dimensions d’espace-temps étaient approchés, et que les calculs exacts, que l’on peut maintenant mener à bien, montrent que, jusqu’ici, une dimension spatiale nous avait échappé.
>
>La seconde propriété de la théorie M que l’on a découverte est qu’outre les cordes elle contient d’autres objets : des membranes bidimensionnelles, des bulles tridimensionnelles (appelées des trois-branes) et tout un tas d’autres ingrédients. Comme pour la onzième dimension, cet aspect de la théorie M émerge lorsqu’on se libère des contraintes dues aux approximations utilisées jusqu’au milieu des années quatre-vingt-dix.
>
>Au-delà de ces découvertes et des nombreuses autres auxquelles ont est parvenu au cours de ces dernières années, la vrai nature de la théorie M reste mystérieuse – c’est un des sens que suggère le « M ». A travers le monde, les physiciens s’échinent à atteindre une compréhension complète de cette théorie M, qui pourrait bien constituer le problème central de la physique du XXIe siècle. […]
>
>Un peu comme la résistance d’une ficelle nous indique la probabilité qu’elle finisse en lambeaux si on l’étire et si on la malmène vigoureusement, il existe un nombre qui détermine la probabilité qu’une corde se sépare en deux cordes sous l’effet des fluctuations quantiques, formant temporairement une paire virtuelles. Ce nombre est connu sous la nom de constante de couplage des cordes (plus précisément, chacune des cinq théories des cordes possède sa propre constante de coupage, comme nous allons le voir sous peu). Le nom est plutôt éloquent : la constante de couplage décrit la force avec laquelle les spasmes quantiques de trois cordes (la corde initiale et les deux autres, virtuelles, en lesquelles elle se scinde) sont reliés – avec quelle ardeur, si l’on peut dire, elles sont couplées les unes aux autres. Le formalisme montre que plus la constante de couplage est élevées, plus les spasmes quantiques sont susceptibles de faire se scinder (et se refondre ultérieurement) une corde initiale ; plus la constante de couplage est faible, moins il y a de chances pour que ces cordes virtuelles voient le jour. […]
>
>Cela nous conduit naturellement à la question essentielle : quelle est la valeur de la constante de couplage (ou, plus précisément, quelles sont les valeurs des constantes de couplage dans chacune des cinq théories des cordes) ? Aujourd’hui personne ne le sait. C’est l’une des questions ouvertes les plus importantes de la théorie des cordes.


### Les trous noirs du point de vue de la théorie des cordes/théorie M [p. 348]

>A première vue, il est difficile d’imaginer deux choses plus différentes que les trous noirs et les particules élémentaires. Nous nous représentons habituellement les trous noirs comme les corps célestes les plus gargantuesques, et les particules élémentaires comme les grains de matière les plus ténus. Mais les recherches de la fin des années soixante et du début des années soixante-dix, dues, entre autres, à Demetrios Christodoulou, Werner Israel, Richard Price, Brandon Carter, Roy Kerr, David Robinson, Stephen Hawking et Roger Penrose, ont montré que les trous noirs et les particules élémentaires sont peut-être moins différents qu’on pourrait le croire. John Wheeler a résumé leurs découvertes en disant que « les trous noirs n’ont pas de cheveux ». Il entendait par là qu’à l’exception d’un petite nombre de signes particuliers tous les trous noirs se ressemblent. Les signes particuliers ? Le premier, bien sûr, est la masse. Quels sont les autres ? Les recherches ont révélé qu’il s’agit de la charge électrique et des autres charges que peuvent posséder les trous noirs, ainsi que de leur vitesse de rotation. Et c’est tout. Deux trous noirs de même masse, de mêmes charges et de même moment angulaire sont rigoureusement identiques. […] Souvenez-vous que ce sont justement ces propriétés – la masse, les charges et le spin – qui distinguent une particule élémentaire d’une autre. Au fil des années, ces analogies ont conduit beaucoup de chercheurs à penser que les trous noirs pourraient n’être en fait que de gigantesques particules élémentaires.
>
>En fait, selon la théorie d’Einstein, il n’existe pas de masse minimale pour un trou noir. Quelle qu’en soit la masse, si l’on comprime un petite morceau de matière jusqu’à une taille suffisamment petite, la relativité générale prédit qu’il deviendra un trou noir. (Plus la masse est faible, plus il faut l’écraser.) Ainsi, on peut imaginer une expérience de pensée où l’on partirait de paquets de matière de plus en plus légers, pour les écrabouiller en des trous noirs de plus en plus petites, afin de comparer les propriétés de ces trous noirs avec celles des particules élémentaires. Comme ils n’ont pas de cheveux, pour des masses suffisamment petites, les trous noirs ainsi formés ressembleraient fort à des particules élémentaires. Dans les deux cas, il s’agirait de petits paquets entièrement définis par leur masse, leurs charges d’interaction et leur spin.
>
>Mais il y a un hic. Les trous noirs astrophysiques, dont les masses sont égales à plusieurs masses solaires, sont si énormes et si lourds que la théorie quantique n’entre pas en ligne de compte : les seules équations de la relativité générale suffisent à expliquer leurs propriétés. (Il s’agit ici de la structure globale du trou noir et non de la zone d’effondrement centrale, dont la taille minuscule requiert très certainement l’usage d’une description quantique.) Or, en tenant de fabriquer des trous noirs de moins en moins massifs, on arrive à un point où ils deviennent si légers et si petits que la théorie quantique entre tout de même dans la danse. Cela se produit si la masse totale du trou noir est inférieure ou égale à la masse de Planck. (Du point de vue de la physique des particules élémentaires, la masse de Planck est énorme – quelque dix milliards de milliards de fois la masse du proton. Toutefois, du point de vue des trous noirs, cette masse, qui équivaut à celle d’un grain de poussière moyen, est infime.) Ainsi, les physiciens qui réfléchissaient aux rapports intimes entre trous noirs minuscules et particules élémentaires se heurtaient immédiatement à l’incompatibilité entre la relativité générale – l’âme théorique des trous noirs – et la théorie quantique. A l’époque, cet antagonisme a bloqué tout progrès dans cette curieuse direction. […]
>
>Pendant longtemps, certains des physiciens théoriciens les plus éminents ont émis des hypothèses sur l’existence possible de processus avec déchirure de l’espace et de liens entre trous noirs et particules élémentaires. Bien que ces spéculations fussent, au départ, perçues comme de la science-fiction, l’avènement de la théorie des cordes et la découverte de ses capacités à unir relativité générale et théorie quantique nous ont permis de faire passer ces conjectures sur le devant de la scène, à la point de la science. Enhardis par ce succès, nous en arrivons à nous demander si d’autres des mystérieuses propriétés de notre Univers qui ont résisté avec entêtement à toute résolution pendant des années ne pourraient pas succomber elles aussi à la puissance de théorie des cordes. La principale parmi celles-ci est la notion d’entropie d’un trou noir. C’est ici que la théorie des cordes a donné sa plus belle démonstration, en fournissant la solution d’un problème fondamental, vieux d’un quart de siècle. […]
>
>Que se passe-t-il, s’est demandé Bekenstein, si l’on range son bureau près de l’horizon d’un trou noir et qu’on installe une pompe à vide qui aspire toutes les molécules d’air fraîchement agitées pour les rejeter dans les profondeurs cachées à l’intérieur de l’horizon ? Allons même encore plus loin : et si la pompe évacuait, dans les entrailles du trou noir, tout l’air de la pièce, tout le fouillis du bureau, et même le bureau avec, vous laissant dans une pièce vide d’air, froide et toute ce qu’il y a de plus ordonnée ? Puisque l’entropie de votre pièce a diminué, Bekenstein en déduisit que pour satisfaire la seconde loi de la thermodynamique il fallait que le trou noir possède une entropie, et que celle-ci augmente suffisamment, lorsqu’il absorbait de la matière, pour compenser la diminution d’entropie observée à l’extérieur.
>
>En fait, pour étayer sa proposition, Bekenstein s’est même appuyé sur un résultat connu dû à Stephen Hawking. Celui-ci avait montré que, dans n’importe quelle interaction physique, l’aire de l’horizon d’un trou noir – souvenez-vous, il s’agit de la surface de non-retour qui masque tous les trous noirs – augmente forcément. Hawking argumentait que, lorsqu’un astéroïde tombait dans un trou noir, ou qu’un trou noir aspirait une partie de l’enveloppe gazeuse d’une étoile voisine, ou que deux trous noirs entraient en collision puis fusionnaient – dans tous ces cas ainsi que dans tous les autres –, l’aire totale de l’horizon du trou noir augmentait toujours. Pour Bekenstein, l’évolution fatale vers une aire toujours plus grande suggérait un lien avec l’évolution inexorable vers un état de plus haute entropie, incarnée par la seconde loi de la thermodynamique. Il a fait la proposition que l’aire de l’horizon du trou noir fournissait une mesure précise de son entropie.
>
>Toutefois, en y regardant de plus près, il y a deux raisons pour lesquelles la plupart des physiciens n’ont pas pris l’idée de Bekenstein au sérieux. Premièrement, les trous noirs semblaient être certains des objets les plus ordonnés et les plus organisés de tout l’Univers. Une fois que l’on a mesuré la masse d’un trou noir, ses charges d’interaction et son moment angulaire, son identité est précisément définie. Avec si peu de caractéristiques définitives, le trou noir semble ne pas posséder une structure suffisante pour abriter un quelconque désordre. La seconde raison qui rendait l’idée de Bekenstein difficile à avaler était que l’entropie, comme nous l’avons vu, est un concept quantique, tandis que les trous noirs, jusqu’à très récemment, restaient désespérément fidèles au camp opposé de la relativité générale, classique. Au début des années soixante-dix, sans moyen d’unir la relativité générale et la théorie quantique, il semblait pour le moins curieux d’envisager l’hypothèse d’une entropie des trous noirs.
>
>En fait, Hawking lui-même avait pensé à l'analogie entre sa loi d'augmentation de l'aire du trou noir et la loi de l'inévitable accroissement de l'entropie, mais il avait écarté l'idée comme une simple coïncidence. Hawking se fondait sur sa loi d'extension d'aire et sur certains autres résultats (obtenus en collaboration avec James Bardeen et Brandon Carter) pour affirmer qu'après tout, si l'on prenait au sérieux l'analogie entre les lois concernant les trous noirs et celles de la thermodynamique, non seulement il faudrait identifier l'aire de l'horizon du trou noir à l'entropie, mais il faudrait aussi lui affecter une température. (Et sa valeur précise serait déterminée par l'intensité du champ gravitationnel du trou noir sur son horizon.) Mais, si un trou noir possède une température non nulle – même infime –, les principes physiques les plus élémentaires et les plus sûrs exigent qu' il émette des radiations, un peu comme un tison ardent. Or tout le monde sait que les trous noirs sont noirs, c'est-à-dire qu'ils n'émettent rien. Hawking, et presque tout le monde, estimait donc que cela excluait l’hypothèse de Bekenstein. Il pensait au contraire que l'aspiration par un trou noir de matière possédant de l'entropie revenait à la perte pure et simple de cette entropie. Et voilà pour la seconde loi de la thermodynamique !
>
>Telle était la situation jusqu'en 1974, lorsque Hawking fit une découverte vraiment étonnante. Les trous noirs n'étaient pas complètement noirs. Si l'on fait abstraction de la théorie quantique et si l'on n'invoque que les lois classiques de la relativité générale, alors, comme on l'avait établi quelque soixante ans plus tôt, les trous noirs ne devaient rien laisser échapper – pas même la lumière – à leur emprise gravitationnelle, Mais l'introduction des préceptes quantiques modifie profondément cette conclusion. Hawking ne disposait pas d'une version quantique de la relativité générale, mais il put exploiter une union partielle de ces deux outils théoriques pour obtenir des résultats limités mais fiables. Et le plus important d’entre eux était que les trous noirs émettent effectivement des radiations, d'un point de vue quantique.
>
>Les calculs sont longs et fastidieux, mais l'idée centrale de Hawking est simple. Nous avons vu que les relations d'incertitude assurent que même le vide est le théâtre d' une agitation frénétique de particules virtuelles qui apparaissent un court instant et s'annihilent ensuite. On trouve aussi ce comportement quantique agité dans la région de l’espace qui entoure l'horizon d'un trou noir. Et Hawking a compris que la puissance gravitationnelle du trou noir pouvait fournir à une paire virtuelle, une paire de photons par exemple, l'énergie qui suffirait à les séparer juste assez pour que l'une des deux particules soit happée par le trou noir. Son alter ego ayant disparu dans les entrailles du trou noir, l’autre photon n’a plus de partenaire avec qui s’annihiler. Hawking a montré que le photon restant recevrait même un élan d'énergie de la force gravitationnelle du trou noir et, alors que son partenaire tomberait à l'intérieur, se trouverait éjecté au loin. Hawking a alors perçu que, pour quelqu’un qui observerait le trou noir à distance, l’effet combiné de cette séparation de paires de photons virtuels, lequel a lieu continuellement tout autour de l'horizon du trou noir, apparaîtrait comme un flux constant de radiation sortante. Les trous noirs rayonnent.
>
>De plus, Hawking a calculé la température qu'un observateur éloigné associerait à la radiation émise et trouvé qu’elle était donnée par l’intensité de la force gravitationnelle sur l’horizon du trou noir, exactement comme l’indiquait l’analogie proposée entre les lois de la physique des trous noirs et celles de la thermodynamique. Bekenstein avait raison : les résultats obtenus par Hawking prouvaient que l’analogie devait être prise au sérieux. Et ils montraient même que c'était bien plus qu'une analogie ; c'était une identité. Les trous noirs ont une entropie. Ils ont une température. Et les lois gravitationnelles de la physique des trous noirs ne sont rien d’autre qu’une réécriture des lois de la thermodynamique dans un contexte gravitationnel complètement atypique. La découverte de Hawking éclata comme une bombe.
>
>Pour vous donner une idée des échelles mises en jeu, sachez que, si l’on prend soigneusement en compte tous les détails, un trou noir dont la masse atteindrait environ trois fois celle du Soleil aurait une température de l’ordre d'un centième de millionième de degré au-dessus du zéro absolu. Ce n’est pas zéro, mais presque. Les trous noirs ne sont pas noirs, mais c’est tout juste. Malheureusement, il en résulte que la radiation qu’ils émettent est infime et donc impossible à détecter expérimentalement. Toutefois, il existe une exception. Les calculs de Hawking ont aussi prouvé que plus la masse du trou noir est faible, plus élevée sera sa température et donc plus importantes seront ses radiations. Par exemple, un trou noir léger comme un petit astéroïde rayonnerait autant qu’une bombe à hydrogène d’un million de mégatonnes, avec des rayonnements concentrés dans la partie gamma du spectre électromagnétique. Les astrophysiciens ont scruté le ciel à la recherche de ces radiations, mais, à part des possibilités peu convaincantes, ils sont revenus bredouilles. Cela semblerait indiquer que ces trous noirs légers, s'ils existent, sont très rares. Hawking aime à dire en plaisantant que c’est bien dommage, car si le rayonnement des trous noirs que prédisent ses travaux avait été détecté, il aurait très certainement obtenu le prix Nobel.
>
>Contrairement à sa minuscule température de moins d’un millionième de degré, si l’on calcule l'entropie d'un trou noir de trois masses solaires, par exemple, le résultat est un nombre absolument énorme : 1 suivi de 78 zéros ! Et plus le trou noir est massif, plus grande est l'entropie. Le succès des calculs de Hawking établit sans équivoque que cela traduit le désordre colossal que représentent les trous noirs. […]
>
>Malgré ces développements impressionnants, deux mystères fondamentaux subsistent. Le premier concerne l’influence des trous noirs sur le concept de déterminisme. Au début du XIXe siècle, le mathématicien français Pierre Simon de Laplace a énoncé la conséquence des lois du mouvement de Newton la plus stricte et la plus décisive concernant les rouages de l’Univers :
>
>>« Une intelligence qui, à un instant donné, pourrait comprendre toutes les forces qui animent la nature et la situation respective de tous les êtres qui la composent, qui pourrait, de plus, être assez vaste pour soumettre ces données à une analyse, pourrait englober, dans une même formule, les mouvements des plus grands objets de l’Univers et ceux des plus légers des atomes. Pour une telle intelligence, il n’existerait rien d’incertain et le futur comme le passé s’ouvriraient à ses yeux. » […]
>
>Cette vision rigide de l’expansion de l’Univers soulevait toutes sortes de dilemmes philosophiques délicats autour de la question du libre arbitre, mais son ampleur a été grandement atténuée par l’avènement de la théorie quantique. Les relations d’incertitude de Heisenberg tempèrent le déterminisme de Laplace, puisqu’il nous est fondamentalement impossible de connaître les positions et les vitesses précises de tous les constituants de l’Univers. Au contraire ces propriétés classiques sont remplacées par les fonctions d'onde quantiques, qui nous indiquent seulement la probabilité qu’une particule donnée se trouve ici ou là, ou qu’elle ait telle ou telle vitesse. Toutefois, l'échec de la vision de Laplace ne ruine pas entièrement l’idée du déterminisme. Les fonctions d'onde – les ondes de probabilités de la mécanique quantique – évoluent avec le temps en vertu de règles mathématiques bien précises, comme les équations de Schrödinger (ou ses versions relativistes, l’équation de Dirac et l’équation de Klein-Gordon). Cela signifie que le déterminisme quantique remplace le déterminisme classique de Laplace : la connaissance des fonctions d’onde pour tous les ingrédients fondamentaux de l’Univers à un instant donné permettrait à une intelligence « suffisamment vaste » de déterminer ces fonctions d’onde à n'importe quel instant passé ou futur. En vertu du déterminisme quantique, la probabilité qu’un quelconque événement donné ait lieu à un instant choisi dans le futur est entièrement déterminée par la connaissance des fonctions d’onde à tout instant antérieur. L’aspect probabiliste de la mécanique quantique adoucit considérablement le déterminisme de Laplace en transformant l'inévitabilité des événements en éventualités, mais ces dernières restent parfaitement déterminées au sein du cadre conventionnel de la théorie quantique.
>
>En 1976, Hawking a compris que l’existence des trous noirs transgresserait même cette forme adoucie de déterminisme. Une fois encore, les calculs sur lesquels repose cette affirmation sont épouvantablement compliqués, mais l’idée centrale est assez simple. Lorsque quelque chose tombe dans un trou noir, celui-ci aspire aussi sa fonction d'onde. Mais cela implique que, dans sa tâche de détermination des fonctions d’onde à tous les instants futurs, notre intelligence « suffisamment vaste » sera leurrée de manière irrémédiable. Pour prédire entièrement le futur, il faut connaître entièrement toutes les fonctions d’onde d'aujourd'hui. Si certaines disparaissent dans les abysses des trous noirs, l'information qu'elles représentent est perdue.
>
>A première vue, on pourrait penser que cette complication due aux trous noirs ne mérite pas tant d'inquiétude. Puisque tout ce qui gît en deçà de l’horizon du trou noir est coupé du reste de l’Univers, ne peut-on pas tout simplement oublier ce qui a eu le malheur d’y tomber ? En outre, d' un point de vue philosophique, ne peut-on pas se dire que l'Univers n’a pas vraiment perdu l'information que contenait la matière aspirée par le trou noir, qu’elle est juste enfermée dans une région de l’espace que nous, êtres rationnels, cherchons à éviter à tout prix ? Avant la découverte par Hawking du fait que les trous noirs ne sont pas vraiment noirs, la réponse à toutes ces questions était affirmative. Mais, après que Hawking eut montré que les trous noirs rayonnaient, l’histoire a été transformée. Les rayonnements transportent de l’énergie, et le trou noir qui les émet voit donc sa masse diminuer : il s’évapore peu à peu. Ce faisant, la distance qui sépare la partie centrale du trou noir de son horizon se réduit lentement, et, tandis que ce voile recule, des régions de l’espace, initialement découplées, réintègrent la vie cosmique. Et nos spéculations philosophiques sont maintenant confrontées au fait suivant : les informations contenues dans les choses qu’a avalées le trou noir – les données que nous imaginions perdues dans ses entrailles – refont-elles surface à mesure que le trou noir s'évapore ? C’est l'information requise pour que survive le déterminisme quantique ; cette question conduit donc directement à se demander si les trous noirs n’imprègnent pas l’évolution de notre Univers d'une suite fortuite d'événements, encore plus fondamentale.
>
>A l’heure où j’écris ces mots, la réponse à cette question ne fait pas l’unanimité parmi les physiciens. Pendant des années, Hawking a défendu avec ferveur l’hypothèse selon laquelle l'information ne réapparaissait pas : selon lui, les trous noirs détruisent l’information, et donc « introduisent un nouveau type d’incertitude au sein de la physique, en plus de l’incertitude habituelle associée à la théorie quantique ». En fait, Hawking, en association avec Kip Thorne, de Caltech, a fait un pari avec John Preskill (lui aussi de Caltech) concernant le sort de l’information piégée par le trou noir : Hawking et Thorne ont parié que l’information était perdue à tout jamais, alors que Preskill a parié que l’information réapparaissait à mesure que le trou noir rayonnait et rétrécissait. L’enjeu ? De l'information, au sens propre du terme : « Le (ou les) perdant récompensera le (ou les) gagnant en lui offrant l'encyclopédie de son choix. »
>
>La question n’est pas encore réglée, mais Hawking a reconnu récemment que la nouvelle approche des trous noirs offerte par la théorie des cordes montre, comme nous l’avons expliqué plus haut, qu’il pourrait y avoir un moyen permettant à l’information de refaire surface. La nouveauté est que, pour le type de trous noirs étudiés par Strominger et Vafa ainsi que par d’autres physiciens depuis leur article initial, l’information peut être enregistrée puis restituée par ses branes constitutives. Cette éventualité, comme l’a dit récemment Strominger, « a conduit certains théoriciens des cordes à crier victoire, à affirmer que l'information était récupérée lorsque le trou noir s'évapore. À mon avis, cette conclusion est prématurée ; il reste beaucoup de travail à fournir pour savoir si c'est vraiment le cas ». Vafa est d’accord et ajoute qu’il demeure « agnostique sur ce point, qui pourrait se solder par l’une ou l’autre issue ». La réponse à cette question est l’un des principaux enjeux de la recherche actuelle. Voici comment la présente Hawking :
>
>>« La plupart des physiciens veulent croire que l’information n’est pas perdue, car cela assurerait que le monde est un lieu sûr et prévisible. Mais je crois que, si l’on prend au sérieux la relativité générale, on doit accepter l’éventualité que l’espace-temps se noue lui-même et que l’information se perde dans ses replis. La question de savoir si oui ou non il y a perte d'information est aujourd'hui l’une des principales questions de la physique théorique. »
>
>Le second mystère non résolu concernant les trous noirs est celui de la nature de l’espace-temps en leur centre. Une application directe de la relativité générale, qui remonte aux travaux de Schwarzschild en 1916, montre que la masse et l’énergie énormes entassées au cœur du trou noir engendrent une fissure destructrice de l’espace-temps, qui le déforme de manière radicale en un état de courbure infinie : il est pincé par une singularité spatio-temporelle. Voici l'une des conclusions que les physiciens en tirent : puisque toute la matière qui a traversé l’horizon est inexorablement attirée vers le point central du trou noir, et puisque cette matière n’a aucun futur, alors, au cœur du trou noir, le temps lui-même touche à sa fin. D’autres physiciens, qui, des années durant, ont étudié les propriétés du cœur des trous noirs à l’aide des équations d’Einstein, ont émis l’hypothèse extravagante que celui-ci pourrait constituer une porte vers un autre univers, qui ne serait relié au nôtre que par le centre du trou noir. En gros, là où le temps de notre Univers touche à sa fin commencerait le temps de l’univers qui lui est attaché.



### Réflexions cosmologiques [p. 374]

>La théorie moderne des origines cosmiques remonte aux quinze années qui ont suivi l’achèvement, par Einstein, de la relativité générale. Einstein refusait de croire à la valeur de sa propre théorie et n’acceptait pas qu’elle implique un univers qui ne soit ni éternel ni statique ; Alexander Friedmann, lui, l’accepta. Et il a découvert la solution des équations d’Einstein que l’on connaît maintenant sous le nom de big-bang : cette solution révèle que l’Univers a violemment émergé d’un état de compression infinie et qu’il subit toujours l’expansion issue de cette explosion originelle. Einstein était tellement sûr que cette solution dépendante du temps ne pouvait pas être une conséquence de sa théorie qu’il est allé jusqu'à publier un court article où il déclarait avoir découvert une faille fatale dans les travaux de Friedmann. Huit mois plus tard, Friedmann réussissait à convaincre Einstein qu’il n’y avait, en fait, aucune erreur ; ce dernier s’est rétracté publiquement mais est demeuré très froid. Einstein était toujours persuadé que les résultats de Friedmann n’avaient rien à voir avec la réalité. Or, environ cinq ans plus tard, les observations détaillées par Hubble de quelques douzaines de galaxies, à l’aide du grand télescope de l’observatoire du mont Wilson, ont confirmé que l’Univers était effectivement en expansion. Les travaux de Friedmann, remaniés sous une forme plus systématique et plus efficace par les physiciens Howard Robertson et Arthur Walker, constituent toujours les fondements de la cosmologie moderne. […]
>
>Nous venons de voir qu’après que les électrons et les noyaux se sont unis pour former les atomes, les photons sont devenus libres de cheminer sans entrave à travers tout l’Univers. Cela signifie que, depuis lors, celui-ci est rempli d’un « gaz » de photons qui voyagent dans tous les sens, uniformément distribués dans tout le cosmos. L’Univers étant en expansion, ce gaz de photons se dilate aussi puisque, en fait, l’Univers constitue son récipient. Et, à l’instar de la température d’un gaz plus ordinaire (comme l’air du pneu de la bicyclette) qui baisse à mesure que le gaz se détend, la température de ce gaz de photons décroît à mesure que l’Univers se dilate. En fait, dès les années cinquante, les physiciens George Gamow, ses étudiants Ralph Alpher et Robert Hermann, puis Robert Dicke et Jim Peebles vers le milieu des années soixante, ont compris que l’Univers actuel devait être imprégné d’un bain quasi uniforme de ces photons originels qui, pendant les quinze milliards d’années d’expansion cosmique, se seraient refroidis à une température de quelques degrés à peine au-dessus du zéro absolu. En 1965, Arno Penzias et Robert Wilson, deux chercheurs des laboratoires de a compagnie téléphonique Bell, dans le New Jersey, ont fait accidentellement l’une des découvertes les plus importantes de notre époque : ils ont détecté cette dernière lueur du big-bang alors qu’ils réglaient une antenne prévue pour communiquer avec des satellites. Les cherches ultérieures ont affiné à la fois la théorie et l’expérience et ont été couronnées, au début des années quatre-vingt-dix, par les relevés du satellite COBE (Cosmic Background Explorer) de la NASA. Ces données ont permis aux physiciens et aux astronomes de confirmer avec une très bonne précision que l’Univers baignait effectivement dans un rayonnement de micro-ondes (et si nos yeux étaient sensibles aux micro-ondes, nous verrions une lueur diffuse tout autour de nous) d’une température de 2.7 degrés au-dessus du zéro absolu, en accord parfait avec les prévisions de la théorie du big-bang. En termes plus concrets, disons que chaque mètre cube de l’Univers – y compris celui que vous occupez à présent – contient, en moyenne, quatre cent millions de photons qui, collectivement, composent le vaste océan des radiations micro-ondes, écho de la création. Une partie de la « neige » qu’émet votre écran télévisé, réglé sur une chaîne qui a interrompu ses émissions, lorsque vous débranchez le câble d’antenne, est due à ces conséquences ténues du big-bang. L’accord entre la théorie et l’expérience confirme l’interprétation cosmologique du big-bang jusqu’au moment, dans le passé, où les photons ont commencé à se déplacer librement dans l’Univers, quelques centaines de milliers d’années après l’explosion. […]
>
>Les trois forces non gravitationnelles s’unissent dans l’environnement extrêmement chaud de l’Univers primordial. Les calculs des physiciens donnant la variation de l’intensité de ces forces avec l’énergie et la température montrent qu’avant 10-35 seconde environ après l’explosion les forces forte, faible et électromagnétique n’étaient qu’une « super »-force, de « grande unification ». A cette époque, l’Univers était bien plus symétrique qu’aujourd’hui. A l’instar de l’homogénéité obtenue en fondant une collection disparate de métaux différents, l’énergie et la température extrêmes de l’Univers originel effaçaient les différences fondamentales entre les forces telles que nous les observons aujourd’hui. Mais à mesure que le temps a passé, que l’Univers s’est dilaté et refroidi, le formalisme de la théorie quantique des champs montre que cette symétrie aurait été sévèrement réduite, au cours d’une série d’étapes plutôt abruptes, conduisant finalement à la forme comparativement asymétrique qui nous est familière.
>
>La physique qui se cache derrière cette réduction de symétrie, que l’on appelle plus précisément brisure de symétrie, n’est pas difficile à comprendre. Imaginez un grand récipient rempli d’eau. Les molécules d’H2O y sont uniformément réparties et, quel que soit l’angle sous lequel vous l’observez, l’eau garde la même apparence. Surveillez le récipient tout en abaissant la température. Au début, il ne se passe grand-chose. A l’échelle microscopique, la vitesse moyenne des molécules d’eau diminue, mais c’est à peu près tout. Toutefois, en réduisant la température jusqu’à zéro degré Celsius, vous réalisez soudainement qu’il se passe quelque chose de plus radical. L’eau commence à geler et se transforme en glace. Nous l’avons expliqué au chapitre précédent : c’est un exemple simple de transition de phase. Mais, pour ce qui nous intéresse ici, le point important est de remarquer que cette transition de phase entraîne une réduction de la symétrie qu’affichent les molécules d’H2O. Alors que l’eau liquide garde le même aspect quel que soit l’angle sous lequel on l’observe – elle possède une symétrie de rotation –, la glace est différente. Elle possède une structure cristalline, et cela signifie que si on l’observe avec suffisamment de précision, comme tout cristal, elle se présentera différemment selon l’angle d’observation. La transition de phase a provoqué une diminution manifeste de la symétrie de rotation.
>
>Ce n’est là qu’un exemple ordinaire, mais la conclusion reste valable de manière plus générale : lorsque l’on diminue la température de nombre de systèmes physiques, ceux-ci finissent, à un certain moment, par subir une transition de phase qui consiste, typiquement, en une diminution ou en une « brisure » de certaines de leurs symétries. Un système donné peut même traverser toute une série de transitions de phase si l’on fait varier sa température sur un large spectre. L’eau en fournir un autre exemple simple. Partons d’H2O au-dessus de cent degrés Celsius : c’est un gaz, la vapeur d’eau. Sous cette forme, le système possède encore plus de symétries que l’eau, puisque les molécules d’H2O se sont maintenant libérées de leur forme liquide, encombrée, où elles sont collées les unes aux autres : à présent, elles fusent à travers le récipient, toutes sur un même pied d’égalité, sans former ni tas ni « cliques » qui pourraient singulariser des groupes de molécules aux dépens des autres. A des températures suffisamment élevées, la démocratie moléculaire est la règle. […]
>
>Les physiciens pensent qu’entre le temps de Planck et le centième de seconde après le big-bang l’Univers se serait comporté d’une manière très semblable, traversant au moins deux transitions de phase. Aux températures supérieures à 1028 degrés Kelvin, les trois forces non gravitationnelles apparaissaient comme une seule force, présentant le plus haut degré de symétrie possible. (Nous exposerons à la fin de ce chapitre comment la théorie des cordes y introduit la force gravitationnelle.) Lorsque la température est descendue au-dessous de 1028 degrés Kelvin, l’Univers a subi une transition de phase dans laquelle les trois forces se sont cristallisées de manières différentes. Leurs intensités relatives et les détails de leur influence sur la matière ont commencé à diverger. Ainsi, la symétrie entre les forces, manifeste aux plus hautes températures, a été brisée avec le refroidissement de l’Univers. Néanmoins, comme l’ont montré Glashow, Salam et Weinberg, la symétrie des hautes températures n’a pas été complètement éliminée : les forces faible et électromagnétique sont restées étroitement liées. L’Univers a continue son expansion et s’est refroidi encore plus sans qu’il se passe grand-chose, jusqu’à 1015 degrés Kelvin – environ cent millions de fois la température au cœur du Soleil –, lorsqu’il a subi une nouvelle transition de phase. Cette fois, ce sont les forces faible et électromagnétique qui ont été affectées. A cette température, elles se sont elles aussi détachées de leur union précédente, plus symétrique, et, tandis que l’Univers continuait de se refroidir, leurs différences se sont accentuées. Ces deux transitions e phase sont à l’origine des trois interactions non gravitationnelles qui œuvrent de manière apparemment distincte, bien que ce résumé d’histoire cosmique prouve qu’il existe entre elles un lien fondamental. […]
>
>Les physiciens ont démontré que le modèle standard du big-bang souffrait précisément de ce problème. Des calculs serrés indiquent qu’en aucune manière des régions de l’espace actuellement très lointaines auraient pu effectuer l’échange de chaleur qui expliquerait qu’elles possèdent aujourd’hui la même température. Le mot horizon désigne la limite ultime de notre vision – le plus loin que puisse atteindre la lumière, pour ainsi dire –, et c’est pour cette raison que les physiciens ont baptisé problème de l’horizon l’uniformité inexpliquée de la température à travers l’immense étendue du cosmos. Ce casse-tête ne veut pas dire que la théorie cosmologique standard est fausse. Mais l’homogénéité des températures montre qu’il nous manque une parte importante du récit cosmologique. En 1979, le physicien Alan Guth, maintenant au MIT, a rédigé le chapitre manquant. […]
>
>La solution de Guth est facile à énoncer. Il a découvert une autre solution des équations d’Einstein, dans laquelle l’Univers primordial subit une brève période d’expansion, extrêmement rapide, une période durant laquelle il « enfle » à un rythme exponentiel. Contrairement au cas de la balle qui ralentit après qu’on a lancée en l’air, l’expansion exponentielle, elle, s’accélère. […]
>
>Grâce à la découverte de Guth et à son perfectionnement ultérieur […] le modèle cosmologique standard a été rénové en modèle cosmologique inflationnaire. Celui-ci modifie le modèle de cosmologie standard sur un très bref laps de temps – entre 10-36 et 10-34 seconde après le big-bang – pendant lequel l’Univers aurait subi une inflation colossale, d’un facteur d’au moins 1030 (comparez celui-ci au facteur de l’ordre de cent que prévoit le scénario standard pour la même période). Ainsi, en un infime sursaut du temps, environ un milliardième de milliardième de milliardième de milliardième de second après le big-bang, la taille de l’Univers aurait augmenté d’un pourcentage jamais égalé par la suite, même au cours des quinze milliards d’années suivantes. Avant cette expansion, la matière que l’on trouve maintenant dans des régions éloignées du cosmos était bien plus étroitement regroupée qu’elle ne l’était en cosmologie standard, facilitant ainsi l’obtention d’une température commune. Puis, pendant l’éphémère paroxysme de l’inflation cosmologique de Guth – suivi par l’expansion plus normale du modèle cosmologique standard –, ces régions de l’espace ont pu se trouver séparées par les distances prodigieuses que nous observons aujourd’hui. Et c’est ainsi que la modification inflationnaire, brève mais fondamentale, apportée au modèle standard de cosmologie a résolu le problème de l’horizon (ainsi que d’autres difficultés également importantes dont nous n’avons pas parlé) et a été très largement acceptée par la communauté des cosmologistes. […]
>
>Il reste un tout petit morceau entre le big-bang et le temps de Planck, dont nous n’avons pas encore parlé. En appliquant aveuglément les équations de la relativité générale à cette région, les physiciens obtiennent que l’Univers continue de devenir toujours plus petit, toujours plus chaud et plus dense, à mesure que l’on approche du big-bang. Au temps zéro, la taille de l’Univers s’annule, la température et la densité filent à l’infini et déclenchent ainsi la plus stridente des sonnettes d’alarme : notre modèle théorique de l’Univers, fermement ancré dans le cadre classique de la relativité générale, s’est effondré.
>
>La nature nous indique solennellement que, dans de telles conditions, nous devons fusionner la relativité générale et la théorie quantique – en d’autres termes, nous devons faire appel à la théorie des cordes. […]
>
>La théorie des cordes modifie le modèle standard de la cosmologie essentiellement de trois façons. Premièrement, d’une manière que les recherches actuelles continuent de préciser, la théorie des cordes assigne à l’Univers la taille qui se révèle la plus petite possible. Cela a de profondes conséquences quant à notre compréhension de l’Univers au moment du big-bang lui-même où, selon la théorie standard, sa taille se réduit jusqu’à devenir nulle. Deuxièmement, la théorie des cordes présente la dualité grand rayon/petit rayon (laquelle est intimement liée au fait que la taille soit la plus petite possible), ce qui a aussi des conséquences cosmologiques considérables, comme nous le verrons sous peu. Et, finalement, la théorie des cordes pose un nombre de dimensions d’espace-temps supérieur à quatre et, du point de vue cosmologique, nous devons approfondir l’évolution de chacune d’elles. […]
>
>Vers la fin des années quatre-vingt, Robert Brandenberger et Cumrun Vafa ont fait les premiers pas vers une compréhension de la façon dont l’application de ces caractéristiques de la théorie des cordes modifie les conclusions du modèle cosmologique standard. Ils en ont tiré deux découvertes importantes. Primo, lorsqu’on remonte le temps jusqu’au commencement, la température augmente jusqu’à ce que, dans toutes les directions, la taille de l’Univers atteigne la longueur de Planck. La température atteint alors son maximum puis commence à décroître. […]
>
>Cela les a conduits à brosser le tableau cosmologique suivant. Au commencement, toutes les dimensions spatiales de la théorie des cordes sont fermement enroulées de sorte que leur étendue soit la plus petite possible, c’est-à-dire, en gros, la taille de Planck. La température et l’énergie sont élevées, mais pas infinies, puisque la théorie des cordes évite le casse-tête du point de départ infiniment comprimé en une taille nulle. A ce moment initial de l’Univers, toutes les dimensions spatiales de la théorie des cordes sot une pied d’égalité – elles sont parfaitement symétriques –, toutes entortillées en une coquille multidimensionnelle de la taille de Planck. Selon Brandenberger et Vafa, l’Univers a alors subi une première brisure de symétrie quand, à peu près au temps de Planck, trois des dimensions spatiales se sont singularisées pour l’expansion, tandis que les autres gardaient leur taille initiale, la longueur de Planck. Ces trois dimensions d’espace s’identifient alors à celles du scénario de la cosmologie inflationnaire, et l’évolution « post-planckienne » prend le relais : les trois dimensions se dilatent jusqu’à la forme qu’on observe aujourd’hui.
>
>Une question se pose immédiatement, : qu’est-ce qui est à l’origine de cette brisure de symétrie, qui sélectionne trois dimensions pour l’expansion ? Au-delà du fait expérimental que seulement trois des dimensions spatiales ont atteint des tailles assez grandes pour être observables, la théorie des cordes permet-elle d’expliquer pourquoi un autre nombre de dimensions (quatre, cinq, six, et ainsi de suite) ou, plus symétriquement encore, toutes les dimensions, ne se sont pas dilatées elles aussi ? Brandenberger et Vafa ont avancé une explication. Souvenez-vous que la dualité grand rayon/petit rayon de la théorie des cordes repose sur le fait que, lorsqu’une dimension est enroulée en un cercle, une corde peut l’envelopper. Brandenberger et Vafa ont compris qu’à l’instar d’élastiques qui entoureraient la chambre à air d’une roue de bicyclette, ces cordes enroulées tendent à resserrer la dimension qu’elles enveloppent, l’empêchant ainsi de croître. A première vue, cela voudrait dire que toutes les dimensions devraient être enserrées, puisque les cordes peuvent – et elles le font – les envelopper toutes. L’échappatoire est que, si une corde enroulée et sa partenaire anti-corde (une corde qui, en gros, enveloppe la dimension dans la direction opposée) entrent en contact, elles s’annihilent prestement l’une et l’autre, pour donner une corde qui n’est pas enroulée. Si ces processus sont assez rapides et assez efficaces, les constrictions analogues à celle de l’élastique seront suffisamment atténuées pour que les dimensions puissent se dilater. Brandenberger et Vafa ont stipulé que cette réduction de l’effet suffocant des cordes enroulées ne se produisait que dans trois dimensions spatiales. […]
>
>Cela conduit à l’interprétation suivante. Dans les premiers instants, le tumulte résultant de la température élevée, mais finie, de l’Univers fait que toutes les dimensions circulaires tentent de se dilater. Cependant, les cordes qui les enveloppent empêchent leur expansion, ramenant leur rayon à leur taille initiale, la longueur de Planck. Mais, tôt ou tard, une fluctuation thermique aléatoire laissera trois dimensions croître un peu plus que les autres. Or, d’après nos explications, les cordes qui entourent ces dimensions ont une grande probabilité d’entrer en collision. À peu près la moitié des collisions consistera en paires corde/anti-corde qui s’annihilent, relâchant ainsi continuellement la constriction initiale, pour finalement permettre à ces trois dimensions de continuer leur expansion, Et plus elles se dilatent, plus il est improbable que d'autres cordes les entourent, puisqu’il faut d’autant plus d’énergie à la corde pour envelopper une dimension plus grande. Ainsi, l’expansion se nourrit d’elle-même, de moins en moins contrainte à mesure que les dimensions grandissent. Nous pouvons alors imaginer que ces trois dimensions poursuivent leur évolution à la manière décrite dans les parties précédentes, et qu’elles atteignent une taille aussi grande (ou plus) que celle de l’Univers actuellement observé.
>
>Par souci de simplicité, Brandenberger et Vafa avaient supposé que toutes les dimensions spatiales étaient circulaires. En fait, comme nous l’avons remarqué, la forme circulaire est compatible avec l’Univers tel qu’on l’observe tant que ses dimensions sont suffisamment grandes pour se boucler sur elles-mêmes au-delà de la portée de nos observations. Mais, pour les dimensions qui restent petites, un scénario plus réaliste indique qu’elles sont entortillées en un espace de Calabi-Yau. Bien sûr, la question clef est de savoir lequel. Comment cet espace particulier est-il déterminé ? Personne n'est encore en mesure de répondre à cette question. Mais, en combinant ces nouveautés cosmologiques avec les résultats sur le changement de topologie décrits dans le chapitre précédent, nous pouvons proposer un moyen d’y répondre. Grâce aux transitions conifold, nous savons maintenant que n’importe quelle forme de Calabi-Yau peut se transformer en n’importe quelle autre. On peut alors imaginer que, dans le tumulte et la chaleur des premiers instants qui ont suivi le big-bang, la partie de l’espace enroulée en Calabi-Yau, tout en restant petite, exécute une valse frénétique dans laquelle sa structure spatiale se déchire et se raccommode sans cesse, produisant rapidement une longue séquence de Calabi-Yau différents. Alors que l’Univers se refroidit et que trois de ses dimensions spatiales s’étendent, les transitions d’un Calabi-Yau à l’autre ralentissent, de sorte que les dimensions supplémentaires finissent par se figer en un Calabi-Yau dont, espérons-le, découlent les caractéristiques physiques observées du monde qui nous entoure. […]
>
>Gasperini et Veneziano ont proposé leur propre version de la cosmologie des cordes, dont certaines caractéristiques sont communes au scénario que l’on vient de décrire, tandis que d’autres en diffèrent considérablement. Comme Brandenberger et Vafa, ils s’appuient sur le fait que la théorie des cordes présente une longueur minimale pour éviter les infinis, en température et en densité d’énergie, des théories standard et inflationnaire. Mais, au lieu d’en conclure que l’Univers est né d'une coquille planckienne extrêmement chaude, Gasperini et Veneziano proposent qu’il puisse y avoir toute une préhistoire de l’Univers – qui commencerait bien avant ce que nous appelions jusqu’ici le temps zéro –, laquelle aurait produit cet embryon cosmologique de la taille de Planck.
>
>Dans ce scénario, qualifié de pré-big-bang, l’Univers part d’un état entièrement différent du big-bang. Les travaux de Gasperini et de Veneziano supposent que, plutôt que d’être incroyablement chaud et rigidement entortillé en un tout petit grain de l'espace, l’Univers a d’abord été froid et essentiellement infini. Les équations de la théorie des cordes indiquent alors –un peu comme l’époque d’inflation de Guth – qu’une instabilité a conduit tous les points de l’Univers à s’éloigner à toute vitesse les uns des autres. Gasperini et Veneziano ont montré que cela a contribué à courber progressivement l’espace, ce qui a entraîné une hausse énorme de la température et de la densité d'énergie. Après un certain temps, une région tridimensionnelle de taille millimétrique, à l’intérieur de cette vaste étendue, ressemblerait tout à fait au fragment dense et chaud de l’inflation de Guth. Avec l’expansion standard de la cosmologie ordinaire du big-bang, ce fragment pourrait alors rendre compte de la totalité de l’Univers qui nous est familier. De plus, comme l’époque pré-big-bang possède sa propre expansion inflationnaire, la solution de Guth au problème de l’horizon fait automatiquement partie de ce scénario cosmologique. Comme le dit Veneziano, « la théorie des cordes nous offre sa version de la cosmologie inflationnaire sur un plateau d’argent ». […]
>
>De manière analogue, certaines caractéristiques de l’Univers possèdent elles aussi une contingence historique – la raison pour laquelle cette étoile s’est formée ici, ou cette planète là-bas, procède d’une succession compliquée d’événements que nous pourrions, au moins en principe, réduire à quelques caractéristiques de ce qu’était l’Univers quand tout a commencé. Mais il est tout à fait envisageable que des aspects plus élémentaires de notre Univers – peut-être même les propriétés des particules fondamentales, matérielles et messagères – dépendent aussi directement de l’évolution historique (évolution qui résulte elle-même des conditions initiales).
>
>En fait, nous avons déjà mentionné une incarnation possible de cette idée dans le cadre de la théorie des cordes : pendant que se développait l’Univers primordial, torride, les dimensions supplémentaires ont pu passer d’une forme à une autre, pour finalement se fixer sur un espace de Calabi-Yau particulier, une fois que tout était suffisamment refroidi. Mais, l’issue de ce cheminement pourrait bien dépendre des détails de la façon dont tout cela a commencé. Or, connaissant l’influence du Calabi-Yau choisi sur les masses des particules et les propriétés des forces, nous voyons que l’évolution cosmologique et l’état de l’Univers à ses débuts peuvent avoir un impact considérable sur les physique observée aujourd’hui.
>
>Nous ne savons rien des conditions initiales de l’Univers, ni même quelles idées, quels concepts et quel langage seraient appropriés pour les décrire. Nous sommes convaincus que l’état initial extravagant – densité, énergie et température infinies – issu des modèles cosmologiques standard et inflationnaire constitue le signal que ces théories s’effondrent, plutôt qu’une description correcte des conditions physiques effectivement en vigueur. La théorie des cordes offre une légère amélioration en montrant comment éviter ces extrémités infinies ; toutefois, personne n’a la moindre idée sur la façon dont tout a vraiment commencé. En fait, notre ignorance s’étend plus loin encore : nous ne savons même pas si cela a un sens de se demander quelles étaient les conditions initiales ou si […] cette question restera à jamais hors d’atteinte de toute théorie. Certains physiciens, comme Hawking et James Hartle, de l'université de Californie, à Santa Barbara, ont courageusement essayé de faire ployer la question des conditions initiales sous le joug de la physique théorique, mais les résultats ne sont pas vraiment convaincants. Dans le contexte de la théorie des cordes/théorie M, notre compréhension de la cosmologie reste encore trop rudimentaire pour déterminer si notre candidate à la « théorie du tout » est vraiment à la hauteur de son nom et précise ses propres conditions initiales, les élevant ainsi au rang de loi physique. […]
>
>Néanmoins, même si on laisse de côté le problème des conditions initiales et de leur impact sur l’évolution cosmique, des hypothèses récentes, hautement spéculatives, suggèrent d’autres limitations potentielles au pouvoir explicatif de toute théorie ultime. Personne ne sait si ces idées sont viables et, pour l’instant, elles se développent en marge des principaux courants scientifiques. Elles jettent cependant un éclairage intéressant et inattendu sur un obstacle auquel pourrait se heurter toute théorie supposée finale.
>
>L’idée de base est la suivante. Imaginez que ce que l'on appelle l’Univers ne soit en fait qu’une minuscule partie d'une étendue cosmologique immensément plus grande. Imaginez qu’il ne soit que l’un des nombreux univers-îlots éparpillés à travers un gigantesque archipel cosmique. Cela peut vous sembler un peu tiré par les cheveux – et c’est peut-être le cas –, mais Andreï Linde a proposé un mécanisme qui pourrait produire un univers aussi gigantesque. Linde a découvert que le sursaut d’expansion inflationnaire pourrait ne pas avoir été un événement unique, isolé. Au contraire, avance-t-il, les conditions favorables à l'inflation auraient pu se répéter sur diverses régions dispersées dans tout le cosmos, lesquelles auraient alors subi leur propre expansion, se développant en de nouveaux univers, séparés. Et, dans chacun de ces univers, le processus continue, avec l’éruption de nouveaux univers dans les régions les plus reculées des anciens, engendrant ainsi un réseau interminable d’expansion cosmique. Les termes manquent, mais laissons-nous porter par le courant et baptisons du nom de multiunivers ce nouveau concept d’univers, très généralisé, dont on appellerait univers chacune de ses parties constituantes.
>
>Tout ce que l’on sait indique que la physique reste cohérente et uniforme à travers tout l’Univers. Mais cela pourrait ne pas être vrai des attributs physiques de ces autres univers tant que nous en sommes séparés, ou du moins suffisamment éloignés, pour que leur lumière n’ait pas eu le temps de nous atteindre. Nous pouvons alors imaginer que la physique varie d’un univers à l’autre. […]
>
>Si, à la lumière de cette dépendance si étroite de la vie aux détails de la physique, nous nous demandons par exemple pourquoi les forces et les particules ont les propriétés précises qu’on leur observe, une réponse possible serait la suivante : à travers l’ensemble du multiunivers, ces caractéristiques varient sur un vaste spectre ; leurs propriétés peuvent être différentes dans d’autres univers et elles le sont. La particularité de la combinaison de propriétés fondamentales que l’on observe dans notre Univers est assurément qu’elle permet à la vie de se développer. Et la vie, plus particulièrement la vie douée d'intelligence, est une condition sine qua non pour que l’on puisse se poser la question de la particularité des propriétés de notre Univers. Plus directement, les choses sont ce qu’elles sont dans notre Univers parce que, si elles étaient autrement, ne serions pas là pour le remarquer. […]
>
>L’enchaînement de ces arguments n’est que le dernier avatar en date d’une idée qui a une longue histoire, connue sous le nom de principe anthropique. Telle que nous venons de la présenter, elle fournit un point de vue diamétralement opposé au rêve d’une théorie unifiée, rigide et entièrement prédictive, dans laquelle les choses sont telles qu’on les observe car il n'aurait pas pu en être autrement, À l’opposé de la grâce poétique par laquelle tout s’ajusterait avec une inflexible élégance, le multiunivers et le principe anthropique dépeignent le tableau d’une collection baroque d’univers infiniment variés. Il nous sera très difficile, voire impossible, de savoir un jour si l’idée du multiunivers est exacte. Même s’il existe d’autres univers, peut-être ne pourrons-nous jamais entrer en contact avec aucun d’entre eux. Mais, en augmentant encore l’étendue de « ce qu’il y a tout là-bas » d’une manière qui amenuise la découverte que la Voie lactée n’est qu’une galaxie parmi d'autres, le concept de multiunivers attire au moins notre attention sur le fait que nous attendons peut-être un peu trop de notre théorie ultime.
>
>Nous devrions exiger de la théorie ultime qu’elle nous offre une description quantique cohérente de la matière et de toutes les forces. Nous devrions exiger de la théorie ultime qu’elle fournisse une cosmologie convaincante de notre Univers. Toutefois, si le modèle du multiunivers est correct et c’est un énorme « si » –, ce serait peut-être trop demander à notre théorie ultime que de nous expliquer également les propriétés détaillées des masses, des charges et des interactions.
>
>Soulignons que, même si l'on accepte les prémices spéculatives du multiunivers, la conclusion que cela compromet notre pouvoir prédictif n’est pas à toute épreuve. La raison, en quelques mots, est que, si nous débridons notre imagination jusqu’à envisager un multiunivers, nous devrions aussi relâcher nos rêveries théoriques afin de voir comment apprivoiser le caractère apparemment aléatoire de ce multiunivers. Dans une version relativement modérée, nous pourrions imaginer si le modèle du multiunivers était exact – qu’en déployant tout le pouvoir tentaculaire de notre théorie ultime cette « théorie ultime étendue » pourrait nous indiquer précisément quelle est la distribution des valeurs des paramètres fondamentaux parmi les univers constituants.
>
>Lee Smolin, de l’université d’État de Pennsylvanie, a proposé des idées encore plus radicales. En s’inspirant de la similarité entre les conditions du big-bang et le centre des trous noirs, caractérisées dans les deux cas par une densité de matière colossale, il a suggéré que chaque trou noir constitue la semence d'un univers nouveau, lequel ferait éruption par une explosion analogue à celle du big-bang, mais qui demeurerait à jamais dissimulé à nos yeux par l'horizon du trou noir. Ne se contentant pas de ne proposer qu’un autre mécanisme de création de multiunivers, Smolin a introduit un élément nouveau, une version cosmique de la mutation génétique, qui permet d’esquiver les limitations scientifiques associées au principe anthropique. Imaginez, avance-t-il, que, lorsqu’un univers émerge du cœur d'un trou noir, ses attributs physiques, comme la masse des particules ou des forces, soient proches de celles de l’univers parent, mais pas identiques. Puisque les trous noirs résultent de l’extinction des étoiles, et puisque la formation de ces mêmes étoiles dépend des valeurs précises des masses des composants élémentaires et des intensités de leurs interactions, la fécondité d’un univers donné – la progéniture de trous noirs qu’il pourra engendrer – dépend elle aussi de manière sensible de ces paramètres. De faibles variations dans les paramètres.
>
>Des univers descendants conduiront donc à en engendrer de plus optimaux, pour la production de trous noirs, que leurs univers géniteurs, dont la propre descendance en univers sera bien plus abondante. Après de nombreuses « générations », les descendants des univers optimisés pour produire des trous noirs seront alors si nombreux qu’ils représenteront l’essentiel de la population de ces multiunivers. Ainsi, plutôt que d’invoquer le principe anthropique, l’idée de Smolin fournit un mécanisme dynamique qui, en moyenne, canalise les paramètres de chaque génération successive d’univers vers des valeurs particulières, celles qui sont optimales pour la création des trous noirs.
>
>Cette approche propose une autre méthode, même dans le contexte du multiunivers, pour expliquer les paramètres fondamentaux de la matière et des interactions. Si la théorie de Smolin est juste, et si notre Univers constitue un exemple typique de multiunivers mature (ce sont là de grands « si », que l’on peut évidemment contester de bien des façons), alors, les paramètres fondamentaux observés pour les particules et leurs interactions devraient être optimaux pour la production de trous noirs. Ainsi, le moindre bricolage de ces paramètres de notre Univers devrait entraver leur formation. Les chercheurs ont commencé à étudier cette proposition ; il n’y a aujourd’hui aucun consensus quant à sa validité. Quand bien même l’hypothèse précise de Smolin serait inexacte, elle présente toutefois une forme nouvelle que pourrait prendre la théorie ultime. A première vue, la théorie ultime pourrait être moins rigide. Nous pourrions découvrir qu’elle permet de décrire une pléthore d’univers, dont la plupart n’auraient rien à voir avec le nôtre. De plus, nous pourrions imaginer que cette kyrielle d'univers puisse être physiquement concrétisée, donnant lieu à un multiunivers – lequel, à première vue, limiterait à jamais notre pouvoir prédictif. En fait, cette discussion illustre toutefois qu’une explication finale pourrait être accessible, à condition que l’on s’attache à comprendre non pas seulement les lois ultimes, mais aussi leurs implications pour l’évolution cosmologique à une échelle plus grande que prévu.



### Visions d’avenir [p. 411]

>Nous nous aventurons ici en terrain spéculatif, mais il se trouve que la théorie des cordes propose une réponse. Le graviton, plus petit paquet de l’interaction gravitationnelle, est une configuration de vibration particulière des cordes. Et tout comme un champ électromagnétique, tel que la lumière visible, est composé d’un nombre énorme de photons, un champ gravitationnel contient un nombre gigantesque de gravitons, et donc de cordes oscillant selon le mode vibratoire approprié. Par ailleurs, les champs gravitationnels sont codés dans la structure de l’espace-temps. Et nous sommes donc amenés à identifier la structure de l’espace-temps lui-même avec un nombre colossal de petites cordes exécutant toutes le même mode de vibration du graviton. Dans le langage spécialisé, une batterie si organisée, si énorme, de petites cordes vibrantes toutes identiques est connu sous le nom d’état cohérent. […]
>
>Nous pourrions nous demander s’il existe un précurseur « premier » à la structure de l’espace-temps, une configuration des cordes de la structure cosmique dans laquelle elles ne se seraient pas encore organisées selon la forme précise que nous identifions à l’espace-temps. Remarquez qu’il n’est pas vraiment approprié d’envisager cet état comme une masse de cordes vibrantes enchevêtrées qui devraient se coudre les unes aux autres pour former un tout organisé. En effet, du fait de notre mode de pensée, cela présuppose les deux notions d’espace et de temps : l’espace dans lequel la corde vibre et l’écoulement du temps qui nous permet de suivre ses changements de forme d’un instant à l’autre. Mais dans l’état originel, avant que les cordes qui façonnent la structure cosmique n’entrent dans cette danse oscillante, cohérente et ordonnée, il n’y a ni espace ni temps. Notre langage est trop fruste pour aborder ces idées puisque, en fait, la notion d’avant n’existe même pas. D’une certaine manière, c’est comme si les cordes étaient des « tessons » d’espace et de temps ; les notions conventionnelles de l’espace et du temps n’émergent que lorsqu’elles exécutent correctement ces vibrations sympathiques. […]
>
>Nous nous heurtons à une incompatibilité de paradigmes dès que l’on tente d’imaginer un univers qui existe sans invoquer le temps ni l’espace. Néanmoins, il est probable que nous devrons aborder ces idées et comprendre leurs implications avant de pouvoir apprécier entièrement la théorie des cordes. La raison en est que notre formulation actuelle de la théorie présuppose l’existence de l’espace et du temps au sein desquels les cordes (et les autres ingrédients découverts dans la théorie M) vibrent et se déplacent. […]
>
>Nous devons permettre à la théorie de créer sa propre scène spatio-temporelle à partir d’une configuration dénuée de temps, dénuée d’espace.
>
>On espère qu’avec ce point de départ vierge – peut-être une ère antérieure au big-bang ou au pré-big-bang (si l’on peut seulement user de termes temporels, en l’absence de tout autre cadre linguistique) – la théorie décrira un univers destiné à évoluer jusqu’à une forme d’où émergera un fond de vibrations cohérentes de cordes, conduisant aux notions conventionnelles de l’espace et du temps. Un tel modèle montrerait que l’espace, le temps et, par association, leur dimension, ne sont pas des éléments de définition essentiels à l’Univers. Mais, plutôt, ils apparaîtraient comme des notions commodes, issues d’un état originel plus basique, atavique.